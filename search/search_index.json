{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Decompilation Wiki","text":"<p>The Decompilation Wiki is a collection of categorized information on all things decompilation. From real-world applications to cutting-edge research papers, the Decompilation Wiki has it all! Join our Discord below for active community engagement. To get involved, see our contribution guide. The Decompilation Wiki is still early in development, so any contribution is appreciated! </p> <p></p>"},{"location":"#what-is-decompilation","title":"What Is Decompilation?","text":"<p>Interestingly, the term \"decompilation\" and its definition are still argued about by researchers. However, most people agree that decompilation is the reversal of compilation.  By that definition, decompilation is the process of turning low-level machine code into a higher-level representation.</p> <p>In many cases, this means turning machine code, like x86 assembly, into source code, like C. This methodology can also be applied to languages like Java, which create bytecode. The difficulty and accuracy of decompilation can vary per language target<sup>1</sup>.</p> <p>Decompilation has wide applications across cyber security, including:</p> <ul> <li>reverse engineering (the understanding of programs)</li> <li>vulnerability discovery (the understanding of program flaws)</li> <li>malware classification</li> <li>program repair</li> <li>and much more...</li> </ul>"},{"location":"#wiki-goals","title":"Wiki Goals?","text":"<p>This wiki has two main goals:</p> <ol> <li>Making decompilation knowledge more accessible to new-comers in the field</li> <li>Categorizing research and tooling to make future decompilation progress easier</li> </ol> <p>To accomplish the first goal, it is highly encouraged to link public code when adding a technique.  Additionally, we will store tutorials for self-rolling (to a degree) your own decompiler components.</p> <p>To accomplish the second goal, we will attempt to rapidly categorize new research and tools in the area. These categorizations may not be agreed upon at first, however, we will update them as the community hits consensus.  In this way, we can quickly attempt to taxonomize the area of decompilation while iterating on it.</p>"},{"location":"#who-made-this","title":"Who Made This?","text":"<p>The Decompilation Wiki was started by Zion Leonahenahe Basque, but is sustained by the contributions of the decompilation community. Both closed and open-source developers are welcome! </p> <p>The wiki is highly inspired by the following sources:</p> <ul> <li>Program-Transformation.org: a wiki on program transformations, including some decompilation.</li> <li>CTF Wiki: a wiki for Capture the Flag, inspiring this layout and design.</li> <li>\"30 Years into Scientific Binary Decompilation\", Dr. Ruoyu (Fish) Wang: a source of information on decompilers.</li> </ul> <p>Additionally, the wiki is due in large part to the support and advisement of Zion's PhD committee: Dr. Ruoyu (Fish) Wang, Dr. Yan Shoshitaishvili, Dr. Adam Doup\u00e9, and Dr. Christina Cifuentes. </p>"},{"location":"#decompilation-paper-list","title":"Decompilation Paper List","text":"<p>Across this wiki you will find many papers cited from across the scientific community.  For easy accessibiliy, you can find all cited papers here: Comprehensive Decompilation List.</p> <ol> <li> <p>Yakdan, Khaled, et al. \"Helping johnny to analyze malware: A usability-optimized decompiler and malware analysis user study.\" 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016.\u00a0\u21a9</p> </li> </ol>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for wanting to contribute to the Decompilation Wiki!  Below are some simple tips to make requests for changes easier to review for the community.  Currently, the wiki is still in early development, so any PR is welcome. </p> <p>If there are ever questions on good contributions, opening issues and discussion on Discord are the best mediums for feedback.</p>"},{"location":"contributing/#finding-places-to-help","title":"Finding Places to Help","text":"<p>Since the Decompilation Wiki aims to categorize and house a wide-ranging field, help is always needed to add and keep up with new content. If you are looking to contribute, but are not sure how the GitHub Issues listing is a great place to start. Any issue is a candidate for contribution, but ones marked as <code>help wanted</code> are likely the easiest to complete for newcomers. </p> <p>Additionally, the wiki has a series of decompilation-related listings that constantly need unknown items added to it.  Some include:</p> <ul> <li>full decompilers</li> <li>tools </li> <li>blogs </li> <li>talks</li> </ul> <p>If you are aware of any unlisted items in these, it is highly appreciated for you to make a PR to add them. </p>"},{"location":"contributing/#update-guide","title":"Update Guide","text":"<p>To make a change, open a PR on the GitHub repo after forking and making a branch. There are two types of changes you might make while contributing to the wiki:</p> <ol> <li>Adding a listing to a new decompiler/tool/blog/...</li> <li>Updating a section regarding fundamental or applied research</li> </ol> <p>The best way to make PRs for these types of changes are different and found below.</p>"},{"location":"contributing/#tool-listing-change","title":"Tool Listing Change","text":"<p>When making changes to this area try to include three things: 1. A link to any high-level description of it (papers are ok). 2. Tags relevant to the tool (see sections like Community Blogs) 3. Tool source link if open-source </p> <p>PRs in this category require no justification and will very likely be merged after a simple review of the content. </p>"},{"location":"contributing/#research-area-update","title":"Research Area Update","text":"<p>Changes to research areas require a little more than tool changes. If the change is a simple typo, no justification is needed and the PR will be accepted.</p> <p>However, if the change is more content-based, then it will require more explanations.  When making changes here, it will be important to either include a citation or a well-thought-out argument against current wording/posturing. Any links to community discussions are always appreciated, such as Reddit threads, Twitter threads, or blogs. </p> <p>Additionally, any PRs that require larger discussion will likely be tied to a Discussion post on GitHub. </p>"},{"location":"contributing/#developing-locally","title":"Developing Locally","text":"<p>The Decompilation Wiki is made with mkdocs. The best way to make changes to the wiki is to get a local version running.</p> <p>Simply install mkdocs and build the site: <pre><code>pip install mkdocs\nmkdocs serve\n</code></pre></p> <p>The site should be launched on http://127.0.0.1:8000/. </p> <p>Thank you for your efforts to improve the Decompilation Wiki!</p>"},{"location":"applications/overview/","title":"Decompiler Applications","text":"<p>Across the internet, there are many ways people have used decompilers in the wild. In this section, you can find a collection of some of those use cases.</p> <p>As an example, some decompilation uses include: </p> <ul> <li>Program Reversing: used to understand how a program works and how to interact with it.</li> <li>Program Reconstruction: used to completely (or partially) recompile the targeted binary</li> <li>Automated Program Repair: used for patching faulty programs</li> <li>Manual &amp; Automated Vulnerability Discovery: used for finding vulnerabilities</li> </ul> <p>For links to full decompilers, see the Decompilers section. </p>"},{"location":"applications/program-reconstruction/","title":"Program Reconstruction","text":"<p>When source code is unavailable for a compiled program, users may want to recover the source code, so they can make edits to it and recompile it.  In the video game scene, this can be useful for modding. This can also be useful for porting a program to a new platform. </p>"},{"location":"applications/program-reconstruction/#video-games","title":"Video Games","text":"<p>Reverse engineers whom also love playing video games often reverse their favorite games.  In some cases, they go as far as attempting to recompile the entire project.  However, to recompile the project, they first need to recover compilable code.  In these cases, practitioners often use a decompiler to first get pseudo-C, then modify it to make it compilable. </p> <p>The end goal of these projects is to recover program source that will recompile to a byte-match of the original binary.  Most projects include a percent completion of the estimated program recompilation.</p> <p>Many of the games in this list were collected from GitHub projects, individuals, or popular blog posts <sup>1</sup>.</p> Game Original Platform Completed Paper Mario: Sticker Star 3DS Super Mario 3D Land 3DS The Legend of Zelda: Ocarina of Time 3D 3DS Ambermoon Amiga \u2705 The Settlers I DOS \u2705 Tokyo Bus Guide Dreamcast Donkey Kong '94 GB \u2705 Kirby's Dream Land GB \u2705 Metroid II: Return of Samus GB Pok\u00e9mon Red and Blue GB \u2705 Pok\u00e9mon Yellow GB \u2705 Super Mario Land GB \u2705 Super Mario Land 3: Wario Land GB \u2705 Banjo-Kazooie: Grunty's Revenge GBA Breath of Fire GBA Fire Emblem: The Binding Blade GBA Fire Emblem: The Sacred Stones GBA Golden Sun GBA Harvest Moon: Friends of Mineral Town GBA Kirby &amp; The Amazing Mirror GBA Metroid - Zero Mission GBA Pok\u00e9mon Emerald GBA Pok\u00e9mon FireRed and LeafGreen GBA Pok\u00e9mon Mystery Dungeon: Red Rescue Team GBA Pok\u00e9mon Pinball: Ruby and Sapphire GBA Pok\u00e9mon Ruby and Sapphire GBA Sonic Advance 2 GBA Summon Night Swordcraft Story 3 GBA Super Mario Advance 2: Super Mario World GBA The Legend of Zelda: Minish Cap GBA \u2705 Links Awakening DX GBC Pok\u00e9mon Crystal GBC \u2705 Pok\u00e9mon Gold and Silver GBC \u2705 Pok\u00e9mon Pinball GBC \u2705 The Legend of Zelda: Link's Awakening DX HD GBC \u2705 Wario Land 3 GBC \u2705 Animal Crossing GameCube Animal Forest e+ GameCube Chibi-Robo: PIA GameCube Doshin the Giant GameCube Harvest Moon: A Wonderful Life GameCube Homeland GameCube Kirby Air Ride GameCube Luigi's Mansion GameCube Mario Kart: Double Dash!! GameCube Mario Party 4 GameCube \u2705 Mario Party 5 GameCube Mario Party 6 GameCube Mario Party 7 GameCube Mario Superstar Baseball GameCube Metroid Prime 1 GameCube Metroid Prime 2 GameCube Naruto: Gekit\u014d Ninja Taisen! 4 GameCube Need for Speed: Most Wanted GameCube Need for Speed: Underground GameCube Paper Mario: The Thousand-Year Door GameCube Pikmin 1 GameCube Pikmin 2 GameCube Ratatouille GameCube Rocket: Robot on Wheels GameCube Skies of Arcadia Legends GameCube Sonic Adventure DX GameCube Sonic Riders GameCube SpongeBob SquarePants: Battle for Bikini Bottom GameCube Star Fox Adventures GameCube Summoner: A Goddess Reborn GameCube Super Mario Strikers GameCube Super Mario Sunshine GameCube Super Smash Bros. Melee GameCube The Incredibles GameCube The Legend of Zelda: The Wind Waker GameCube The Legend of Zelda: Twilight Princess GameCube The SpongeBob SquarePants Movie GameCube Ty the Tasmanian Tiger GameCube Phantasy Star I MasterSystem Phantasy Star II MegaDrive Phantasy Star III MegaDrive Phantasy Star IV MegaDrive Ristar MegaDrive Streets of Rage 2 MegaDrive \u2705 AeroGauge N64 Aidyn Chronicles: The First Mage N64 Animal Forest N64 Banjo-Kazooie N64 \u2705 Banjo-Tooie N64 Blast Corps N64 Body Harvest N64 Bomberman 64 N64 Bomberman 64: The Second Attack! N64 Bomberman Hero N64 Castlevania 64 N64 Conker's Bad Fur Day N64 Chameleon Twist 1 N64 Chameleon Twist 2 N64 Dark Rift N64 Diddy Kong Racing N64 Dinosaur Planet N64 Donkey Kong 64 N64 Doom 64 N64 \u2705 Doraemon: Nobita to Mittsu no Seireiseki N64 Dr. Mario 64 N64 Duke Nukem 64 N64 Duke Nukem: Zero Hour N64 \u2705 F-Zero X N64 F-Zero X Expansion Kit N64 Gauntlet Legends N64 Gex 64: Enter the Gecko N64 Glover N64 Goldeneye 007 N64 Harvest Moon 64 N64 Jet Force Gemini N64 Kirby 64: The Crystal Shards N64 Lego Racers N64 Mario Golf N64 Mario Kart 64 N64 \u2705 Mario Party 1 N64 Mario Party 2 N64 Mario Party 3 N64 Mario Tennis N64 Mischief Makers N64 Mystical Ninja Starring Goemon N64 Neon Genesis Evangelion 64 N64 Onegai Monsters N64 Paper Mario N64 \u2705 Perfect Dark N64 \u2705 Pokemon Puzzle League N64 Pokemon Snap N64 Pokemon Stadium N64 Pokemon Stadium 2 N64 Quest 64 N64 Shadowgate 64 N64 Space Station Silicon Valley N64 Star Fox 64 N64 Star Wars: Shadows of the Empire N64 Super Mario 64 N64 \u2705 Super Smash Bros. N64 Superman 64 N64 The Legend of Zelda: Majora's Mask N64 \u2705 The Legend of Zelda: Ocarina of Time N64 \u2705 The New Tetris N64 Virtual Pool 64 N64 Virtual Pro Wrestling 2 N64 Wave Race 64 N64 Yoshi's Story N64 Castlevania: Order of Ecclesia NDS Dragon Quest IX NDS Mario &amp; Luigi - Partners in Time NDS Mario Party DS NDS Pok\u00e9mon Diamond and Pearl NDS Pok\u00e9mon HeartGold and SoulSilver NDS Rhythm Heaven NDS The Legend of Zelda: Phantom Hourglass NDS The Legend of Zelda: Spirit Tracks NDS Touhou Project 1 NEC PC-9800 \u2705 Touhou Project 2 NEC PC-9800 Touhou Project 3 NEC PC-9800 Touhou Project 4 NEC PC-9800 Touhou Project 5 NEC PC-9800 Zelda II: The Adventure of Link NES \u2705 Carmageddon PC Deus Ex: Human Revolution - Director's Cut PC Lego Island PC \u2705 Sonic Mania PC Spider-Man (Neversoft) PC Castlevania: Symphony of the Night PS1 Crash Bandicoot PS1 Crash Bandicoot 2: Cortex Strikes Back PS1 Crash Team Racing PS1 Croc: Legend of the Gobbos PS1 Doom PS1 Driver 2 PS1 \u2705 Final Fantasy VII PS1 Legacy of Kain: Soul Reaver PS1 Legend of Dragoon PS1 \u2705 Legend of Legaia PS1 MediEvil 1 PS1 Metal Gear Solid PS1 R4: Ridge Racer Type 4 PS1 Shin Megami Tensei PS1 Silent Hill PS1 Spyro the Dragon PS1 Tomb Raider 1 PS1 \u2705 Tomb Raider 1-5 PS1 Tomba! PS1 Vagrant Story PS1 Vandal Hearts PS1 Xenogears PS1 Dark Cloud PS2 Fatal Frame 1 PS2 Fatal Frame 2: Crimson Butterfly PS2 Ico PS2 Jak II PS2 \u2705 Jak and Daxter: The Precursor Legacy PS2 \u2705 Kingdom Hearts PS2 Klonoa 2: Lunatea's Veil PS2 Metal Gear Solid 2: Sons of Liberty PS2 PaRappa the Rapper 2 PS2 Resident Evil - Code: Veronica X PS2 Sly Cooper and the Thievius Raccoonus PS2 Street Fighter III: 3<sup>rd</sup> Strike PS2 \u2705 Twisted Metal: Black PS2 Xenosaga Episode 1: Der Wille zur Macht PS2 Yakuza PS2 Demon's Crest SNES Donkey Kong Country 1 SNES Donkey Kong Country 2 SNES Donkey Kong Country 3 SNES Earthbound / Mother 2 SNES Final Fantasy IV SNES Final Fantasy VI SNES Goof Troop SNES Super Ghouls 'n Ghosts SNES Super Mario RPG SNES Super Mario World SNES Super Mario World 2: Yoshi's Island SNES \u2705 Super Metroid SNES \u2705 Super Punch-Out!! SNES The Legend of Zelda: A Link to the Past SNES \u2705 Minecraft: Nintendo Switch Edition Switch Super Mario 3D World + Bowser's Fury Switch Super Mario Odyssey Switch The Legend of Zelda: Breath of the Wild Switch Inazuma Eleven Strikers Wii Kirby's Epic Yarn Wii Mario Kart Wii Wii Mario Party 8 Wii Mario Party 9 Wii Pokemon Battle Revolution Wii Pokepark Wii: Pikachu's Adventure Wii Super Mario Galaxy Wii Super Mario Galaxy 2 Wii Super Paper Mario Wii Super Smash Bros. Brawl Wii The Legend of Zelda: Skyward Sword Wii Wii Sports Wii Xenoblade Chronicles Wii New Super Mario Bros. U Wii U Halo: Combat Evolved Xbox Minecraft: Xbox 360 Edition Xbox 360 Minecraft: Pocket Edition (2011) iOS \u2705 <ol> <li> <p>https://www.resetera.com/threads/decompilation-projects-ot-free-next-gen-update-for-your-favorite-classics-jak-ii-pc-port-out-in-beta.682687/ \u21a9</p> </li> </ol>"},{"location":"applied-research/code-similarity/","title":"Code Similarity","text":""},{"location":"applied-research/code-similarity/#introduction","title":"Introduction","text":"<p>In cases such as malware identification, the ability to estimate code similarity among binaries is critical<sup>1</sup>. Research in this area generally looks at ways to improve the reliability of similarity detection among binaries. </p> <p>There is little work in the direct use of decompilation for code similarity, however, the general work in the binary analysis is frequent.  These works are included here since they often touch on or improve fundamental components in decompilation. </p> <p>The most direct research in this area has utilized Ghidra decompilation to identify inlined functions in decompilation<sup>2</sup>. </p>"},{"location":"applied-research/code-similarity/#related-works","title":"Related Works","text":"<p>Many works have progressed towards binary-based code similarity that do not explicitly use decompilation <sup>1</sup><sup>3</sup><sup>4</sup><sup>5</sup><sup>6</sup>. Most of these works have improved code similarity techniques indirectly by improving it for their specific uses cases.  These uses have included malware identification<sup>1</sup>, duplicated bug hunting<sup>3</sup><sup>4</sup>, and code reuse<sup>5</sup>.</p> <p>Recent work has suggested that machine learning has made significant strides in this area<sup>6</sup>.</p> <ol> <li> <p>Hu, Xin, Tzi-cker Chiueh, and Kang G. Shin. \"Large-scale malware indexing using function-call graphs.\" Proceedings of the 16<sup>th</sup> ACM conference on Computer and communications security. 2009.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Ahmed, Toufique, Premkumar Devanbu, and Anand Ashok Sawant. \"Finding Inlined Functions in Optimized Binaries.\" arXiv preprint arXiv:2103.05221 (2021).\u00a0\u21a9</p> </li> <li> <p>Feng, Qian, et al. \"Scalable graph-based bug search for firmware images.\" Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Eschweiler, Sebastian, Khaled Yakdan, and Elmar Gerhards-Padilla. \"Discovre: Efficient cross-architecture identification of bugs in binary code.\" Ndss. Vol. 52. 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Mirzaei, Omid, et al. \"Scrutinizer: Detecting code reuse in malware via decompilation and machine learning.\" Detection of Intrusions and Malware, and Vulnerability Assessment: 18<sup>th</sup> International Conference, DIMVA 2021, Virtual Event, July 14\u201316, 2021, Proceedings 18. Springer International Publishing, 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>Marcelli, Andrea, et al. \"How machine learning is solving the binary function similarity problem.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"applied-research/overview/","title":"Applied Research Overview","text":"<p>Decompiler research that does not neatly fit into one of the fundamental areas is defined here as applied research. Research in this area contributes to a specific use-case of decompilation that may not necessarily improve base decompilation.</p> <p>As an example, most researchers would agree that variable name prediction in stripped binaries is an important research area<sup>1</sup>.  However, as it stands, variable name prediction does not improve any fundamental research area (except neural decompilation). As such, we consider it an applied research area, with that target being human-comprehensible decompilation. </p> <p>This section is ever-growing as new research areas are explored in decompilation.  Currently, the following areas exist:</p> <ul> <li>Symbol Recovery: recovering the names or high-level symbols that are associated with a function or variable</li> <li>Code Similarity: measuring how similar (for various uses) some binary is to another</li> <li>Vulnerability Discovery: tuning decompilation to be better used for vulnerability discovery</li> </ul>"},{"location":"applied-research/overview/#other-research","title":"Other Research","text":"<p>Some research areas don't have enough work to define a label for them. The following works are listed here:</p> <ul> <li>Byte-exact recompilable decompilation<sup>4</sup></li> <li>Patchable decompilation<sup>2</sup></li> <li>Verifiable decompilation<sup>3</sup></li> <li>Higher abstraction support<sup>5</sup><sup>6</sup><sup>7</sup></li> </ul> <ol> <li> <p>Pal, Kuntal Kumar, et al. \"\"Len or index or count, anything but v1\": Predicting Variable Names in Decompilation Output with Transfer Learning.\" 2024 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 2024.\u00a0\u21a9</p> </li> <li> <p>Reiter, Pemma, et al. \"Automatically mitigating vulnerabilities in x86 binary programs via partially recompilable decompilation.\" arXiv preprint arXiv:2202.12336 (2022).\u00a0\u21a9</p> </li> <li> <p>Verbeek, Freek, Pierre Olivier, and Binoy Ravindran. \"Sound C Code Decompilation for a subset of x86-64 Binaries.\" Software Engineering and Formal Methods: 18<sup>th</sup> International Conference, SEFM 2020, Amsterdam, The Netherlands, September 14\u201318, 2020, Proceedings 18. Springer International Publishing, 2020.\u00a0\u21a9</p> </li> <li> <p>Schulte, Eric, et al. \"Evolving exact decompilation.\" Workshop on Binary Analysis Research (BAR). 2018.\u00a0\u21a9</p> </li> <li> <p>Fokin, Alexander, et al. \"SmartDec: approaching C++ decompilation.\" 2011 18<sup>th</sup> Working Conference on Reverse Engineering. IEEE, 2011.\u00a0\u21a9</p> </li> <li> <p>Wu, Ruoyu, et al. \"{DnD}: A {Cross-Architecture} deep neural network decompiler.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Liu, Zhibo, et al. \"Decompiling x86 deep neural network executables.\" 32<sup>nd</sup> USENIX Security Symposium (USENIX Security 23). 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"applied-research/symbol-recovery/","title":"Symbol Recovery","text":""},{"location":"applied-research/symbol-recovery/#introduction","title":"Introduction","text":"<p>A symbol, in the context of binaries, is a name associated with an object. In most cases, this is either function names or variable names.  It is often useful for reverse engineering to have the original symbols to more quickly understand the purpose of an object. </p>"},{"location":"applied-research/symbol-recovery/#symbol-recovery-example","title":"Symbol Recovery Example","text":"<p>Below is a snippet of a C program: <pre><code>int mode;\nchar* name;\nlong long timezone;\n</code></pre></p> <p>After compiling and stripping, a common developer practice, the binary will be decompiled to something like: <pre><code>int v1;\nchar* v2;\nlong long v3;\n</code></pre></p> <p>Assuming the types are recovered perfectly (hard), it is still hard to understand what these variables do. </p>"},{"location":"applied-research/symbol-recovery/#previous-work","title":"Previous Work","text":"<p>Research in this area has been concerned with the recovery of both variable names<sup>1</sup><sup>2</sup><sup>4</sup><sup>5</sup><sup>6</sup><sup>7</sup> and function names<sup>3</sup><sup>5</sup>.  Approaches have varied between using neural networks<sup>2</sup><sup>3</sup><sup>6</sup><sup>7</sup>, machine translation<sup>4</sup>, probabilistic methods<sup>5</sup>, and BERT-based language models<sup>1</sup>. In many cases, the bottleneck of this work has been dataset generation<sup>1</sup>.</p> <ol> <li> <p>Pal, Kuntal Kumar, et al. \"\"Len or index or count, anything but v1\": Predicting Variable Names in Decompilation Output with Transfer Learning.\" 2024 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Dramko, Luke, et al. \"DIRE and its data: Neural decompiled variable renamings with respect to software class.\" ACM Transactions on Software Engineering and Methodology 32.2 (2023): 1-34.\u00a0\u21a9\u21a9</p> </li> <li> <p>Artuso, Fiorella, et al. \"Function naming in stripped binaries using neural networks.\" arXiv preprint arXiv:1912.07946 (2019).\u00a0\u21a9\u21a9</p> </li> <li> <p>Jaffe, Alan, et al. \"Meaningful variable names for decompiled code: A machine translation approach.\" Proceedings of the 26<sup>th</sup> Conference on Program Comprehension. 2018.\u00a0\u21a9\u21a9</p> </li> <li> <p>He, Jingxuan, et al. \"Debin: Predicting debug information in stripped binaries.\" Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. 2018.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>DIRE: A Neural Approach to Decompiled Identifier Naming\u00a0\u21a9\u21a9</p> </li> <li> <p>Chen, Qibin, et al. \"Augmenting decompiler output with learned variable names and types.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"applied-research/vulnerability-discovery/","title":"Vulnerability Discovery","text":"<p>In many uses of decompilation, humans, or machines, aim to understand if a program is safe. To verify if this program is safe, they attempt to do the opposite: find vulnerabilities in the program. Some decompilers, and their associated research, have attempted to tune their decompilers to be better at this task<sup>1</sup>. There has also been work at evaluating decompilers by how well they perform with source tools<sup>2</sup>. </p> <p>Most research in this area has focused on static analysis<sup>1</sup><sup>2</sup><sup>3</sup> and symbolic execution<sup>4</sup> applied to decompilation. Since these tasks have often been researched with source, an application to binaries has been achieved through decompilation. </p> <ol> <li> <p>Botacin, Marcus, et al. \"Revenge is a dish served cold: Debug-oriented malware decompilation and reassembly.\" Proceedings of the 3<sup>rd</sup> Reversing and Offensive-oriented Trends Symposium. 2019.\u00a0\u21a9\u21a9</p> </li> <li> <p>Mantovani, Alessandro, et al. \"The Convergence of Source Code and Binary Vulnerability Discovery--A Case Study.\" Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security. 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>Park, Jihee, et al. \"Static Analysis of JNI Programs via Binary Decompilation.\" IEEE Transactions on Software Engineering (2023).\u00a0\u21a9</p> </li> <li> <p>Han, HyungSeok, et al. \"QueryX: Symbolic Query on Decompiled Code for Finding Bugs in COTS Binaries.\" 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"decompilers/dec-history-1/","title":"History of Decompilation (1960-2007)","text":""},{"location":"decompilers/dec-history-1/#maintainers-note","title":"Maintainers Note","text":"<p>The following text is a clone, with small fixes, of the popular Program-Transformation Wiki, which went offline in 2025. All credits to the following text on this page go to the original creators of the Program-Transformation Wiki. The maintainers of the Decompilation Wiki do not claim credit for it, but instead preserve it here for future researchers in decompilation. </p>"},{"location":"decompilers/dec-history-1/#why-decompilation","title":"Why Decompilation?","text":""},{"location":"decompilers/dec-history-1/#why-not-just-disassemble","title":"Why not just disassemble?","text":"<p>Consider the Java world, where there are simple disassemblers and sophisticated decompilers that often work well and with little user intervention. Would you use a Java disassembler to try to understand some Java bytecode program? Most likely not. If there is a good decompiler available, you don't need to see the individual instructions. If and when a good decompiler for executable programs becomes available, it will be a better choice than a disassembler in most circumstances.</p>"},{"location":"decompilers/dec-history-1/#applications","title":"Applications","text":"<p>There are many reasons why you might want to decompile a program. Here are some of them, based on this diagram:</p> <p></p> <p>If the output is considered...</p> <ul> <li>A comprehension aid. You don't have the resources to create compilable code. Even so, you can do these things:<ul> <li>You can perform code checking of various kinds:<ul> <li>Find bugs. You focus on the area of the program that you are interested in. looking for bugs.</li> <li>Find vulnerabilities. You scan the whole program, looking for code that could be exploited.</li> <li>Find malware. Again you scan the whole program, this time looking for code that has already been exploited. It may be easier and quicker to find malware in 1000 lines of C than in 10,000 lines of assembler.</li> <li>Verification. You want to make sure that the binary code that you have corresponds to the source code that you also have. This is important in safety critical applications.</li> <li>Comparison. Suppose you are accused of violating a software patent, or suspect that someone has violated your patent. You can show how different or similar two programs are by comparing decompiled source code. To some extent, a decompiler can make two different implementations of the same program reveal their similarities. As an example of this, using different optimisations with the same compiler somtimes results in similar decompiled code.</li> </ul> </li> <li>Learn Algorithm. To the extent that is allowable by law, you might want to discover how a particular feature of a piece of software is implemented. It's a little bit like browsing patents online; you can't use what you see directly in a commercial setting, unless you have an arrangement with the author.</li> <li>Interoperability. It may be legal and necessary to reverse engineer some binary code for the purposes of interoperability. There are the famous cases of Sega vs Accolade and Atari v Nintendo, where small games companies successfully argued their right to reverse engineer some existing games, so that they could manufacture competing games for that platform. I don't know if they used decompilation for this (probably not), but it's a good example of iwhere decompilation might make a difficult job much easier.</li> </ul> </li> <li>If you have compilable code, then you can do more:<ul> <li>With little user input:<ul> <li>Optimise for platform. Example: you have an old program written for the 80286 processor, but you have a Pentium 4. You can recompile the code with optimisations appropriate for your actual hardware.</li> <li>Port cross platform (where the same libraries are available). For example, you have an Intel Windows version of a program, but you have an Alpha (running Windows NT). You can recompile the decompiled output with an Alpha compiler. Or recompile it with Winelib (to provide Windows libraries) and run the \"Windows\" program natively on your PPC laptop under Linux. Or compile it under Darwine on your Mac running OS/X. If the vendor doesn't provide one, a 64-bit driver (required for 64-bit Windows XP) might be rewritten from the decompilation of a 32-bit driver. Decompiling or disassembling an existing driver binary may be the only way to find interoperability information, e.g. to write a Linux driver.</li> </ul> </li> <li>If you put enough effort into the decompilation to produce maintainable code, you can also:<ul> <li>Fix bugs (without the tediom of patching the binary file).</li> <li>You can also add new features that were not in the original program.</li> <li>You can recover lost or unusable source code. It is estimated that 5% of all software in the world has at least some source files missing. You can use the decompiled code to replace the missing source file(s). You can also maintain code that was written in assembler, or in a language for which the compilers no longer exist.</li> </ul> </li> </ul> </li> </ul> <p>Some more decompilation applications that don't fit very well into the above classification:</p> <ul> <li>Defeating copy protection, where the company who wrote the software (that you paid for) is out of business, and you can't transfer it to a new machine. It does happen!</li> <li>Support for programs that ship without debugging information, often linking with third party code, and one day it just stops working. Recompiling with debugging support turned on may or may not show the fault. People experience this problem in the field all the time, and at present the tools they have to work with are inadequate. Decompilation may be able to ease the support burden, even for companies that have the source code (except perhaps to some third party code).</li> </ul>"},{"location":"decompilers/dec-history-1/#software-freedoms","title":"Software Freedoms","text":"<p>On a SlashDot articled called \"BitKeeper Love Triangle: McVoy, Linus and Tridge\" 11/Apr/2005), it was posted:</p> <p>\"The purpose of copyrights is to advance science and useful arts, not to reward authors.</p> <p>If rewarding authors for that purpose is required, then they will be rewarded.</p> <p>Copyrights on binaries however, reward authors while stifling the progress of science and useful arts.</p> <p>It encourages people to create secretly-operating software that helps them get revenue but does not inspire new works, does not enter the public domain and does not help anyone else in the long run.</p> <p>So what if publishing your binary was almost as useful as publishing source code? Decompilation is not yet at a stage where this is true, but in 5-10 years, it may well be. Decompilation could then become a tool for software freedom.</p> <p>Here is another quote, this time from \"Breaking into Locked Rooms to Access Computer Source Code: Does the DMCA Violate a Constitutional Mandate When Technological Barriers of Access Are Applied to Software?\" by Rod Dixon of Virginia Journal of Law and Technology:</p> <p>\"27. For non-software literary works, once the work is published, the ideas contained in the work become apparent and conspicuous. In this manner, the work's basic ideas may provide a basis for further development of additional works by any member of the public with access to the work. This is a critical function of the public domain because it serves the primary purpose of copyright. ... The enrichment of the public domain of ideas increases the public's access to works, which further enriches the public domain. Software, however, presents a slight conundrum that disrupts the flow of the recursive enrichment of the public domain served by copyright law.\"</p> <p>Again, decompilation may (if the laws are written correctly) allow the ideas contained in programs not released with source code to become available to the public domain, as they were always intended to. The novel ideas can be patented (unless you believe that all software patents are bad); this does at least disclose the ideas (they are freely readable by all) and can freely be used by all once the patent expires.</p>"},{"location":"decompilers/dec-history-1/#timeline-overview","title":"Timeline Overview","text":""},{"location":"decompilers/dec-history-1/#part-1","title":"Part 1","text":"Years Work 1960, 1962 D-Neliac decompiler 1963\u20131967 The Lockheed Neliac decompiler 1966 W. Sassaman 1967 Autocoder to Cobol translator 1972\u20131976 The Inverse Compiler 1973 C. R. Hollander PhD thesis 1973 B. C. Housel PhD thesis 1974 The Piler System 1974 F. L. Friedman PhD thesis 1974 Ultrasystems 1974 V. Schneider and G. Winiger 1977\u20131979 L. Peter Deutsch 1977, 1981, 1988 Decompilation of Polish code 1978 G. L. Hopwood PhD thesis 1978 D. A. Workman"},{"location":"decompilers/dec-history-1/#part-2","title":"Part 2","text":"Years Work 1981 Zebra 1982 Decompilation of DML programs 1982, 1984 Forth Decompiler 1984 Dataflex Decompilers 1985 Software transport system 1988 J. Reuter's Decomp VAX BSD decompiler 1989 FermaT 1990 Austin Code Works\u2019 exe2c DOS decompiler 1991 PLM-90 decompiler 1991 O\u2019Gorman PhD thesis 1991\u20131994 Decompiler compiler 1991\u20131993 8086 C Decompiling System 1993 Alpha AXP Migration Tools 1993 Source/PROM Comparator 1994 The dcc decompiler 1995 DECLER Decompiler 1997\u20132001 University of Queensland Binary Translator 1999 A. Mycroft's Type Reconstruction for Decompilation"},{"location":"decompilers/dec-history-1/#part-3","title":"Part 3","text":"Years Work 2000 University of London\u2019s Asm21toc reverse compiler 2001 Proof-Directed De-compilation of Low-Level Code 2001 Computer Security Analysis through Decompilation and High-Level Debugging 2001 Type Propagation in IDA Pro Disassembler 2001 DisC, by Satish Kumar 2002 ndcc decompiler circa 2002 The Anatomizer Decompiler 2002 Analysis of Virtual Method Invocation for Binary Translation 2002 Boomerang 2002 Desquirr 2004 Analyzing Memory Accesses in x86 Executables 2004 R. Falke\u2019s Type Analysis for Decompilers"},{"location":"decompilers/dec-history-1/#history-1960-1979","title":"History: 1960-1979","text":"<p>Decompilers have been written for a variety of applications since the development of the first compilers. The very first decompiler was written by Joel Donnelly in 1960 at the Naval Electronic Labs to decompile machine code to Neliac on a Remington Rand Univac M-460 Countess computer. The project was supervised by Professor Maurice Halstead, who worked on decompilation during the 1960s and 70s, and published techniques which form the basis for today's decompilers. It is for this reason that we dedicate this page to the memory of Prof Maurice Halstead and award him the title of Father of Decompilation.</p> <p>Throughout the last decades, different uses have been given to decompilers. In the 1960s, decompilers were used to aid in the program conversion process from second to third generation computers; in this way, manpower would not be spent in the time-consuming task of rewriting programs for the third generation machines. During the 70s and 80s, decompilers were used for the portability of programs, documentation, debugging, re-creation of lost source code, and the modification of existing binaries. In the 90s, decompilers have become a reverse engineering tool capable of helping the user with such tasks as checking software for the existence of malicious code, checking that a compiler generates the right code, translation of binary programs from one machine to another, and understading of the implementation of a particular library function.</p> <p>The following descriptions illustrate the best-known decompilers and/or research performed into decompiler topics by individual researchers or companies. Most of these descriptions first appeared in Cristina Cifuentes' PhD thesis Reverse Compilation Techniques. They are reproduced herein with permission from the author.</p>"},{"location":"decompilers/dec-history-1/#d-neliac-decompiler-1960","title":"D-Neliac decompiler, 1960","text":"<p>As reported by Halstead in [ Hals62 ], the Donnelly-Neliac (D-Neliac) decompiler was produced by J.K.Donnelly and H.Englander at the Navy Electronics Laboratory (NEL) in 1960. Neliac is an Algol-type language developed at the NEL in 1955. The D-Neliac decompiler produced Neliac code from machine code programs; different versions were written for the Remington Rand Univac M-460 Countess computer and the Control Data Corporation 1604 computer. See also NeliacDecompiler.</p> <p>D-Neliac proved useful for converting non-Neliac compiled programs into Neliac, and for detecting logic errors in the original high-level program. This decompiler proved the feasibility of writing decompilers.</p>"},{"location":"decompilers/dec-history-1/#the-lockheed-neliac-decompiler-1963-7","title":"The Lockheed Neliac Decompiler, 1963-7","text":"<p>The Lockheed decompiler, also known as the IBM 7094 to Univac 1108 Neliac Decompiler helped in the migration of scientific applications from the IBM 7094 to the newer Univac 1108 at Lockheed Missiles and Space. Binary card images were translated to Univac 1108 Neliac, as well as assembly code (if available). Binary code was produced from Fortran applications. As reported in [Hals70].</p> <p>Halstead analyzed the implementation effort required to raise the percentage of correctly decompiled instructions half way to 100%, and found that it was approximately equal to the effort already spent [ Hals70 ]. This was because decompilers from that time handled straightforward cases, but the harder cases were left for the programmer to consider. In order to handle more cases, more time was required to code these special cases into the decompiler, and this time was proportionately greater than the time required to code simple cases.</p>"},{"location":"decompilers/dec-history-1/#wsassaman-1966","title":"W.Sassaman, 1966","text":"<p>Sassaman developed a decompiler at TRW Inc., to aid in the conversion process of programs from 2<sup>nd</sup> to 3<sup>rd</sup> generation computers. This decompiler took as input symbolic assembler programs for the IBM 7000 series and produced Fortran programs. Binary code was not chosen as input language because the information in the symbolic assembler was more useful. Fortran was a standard language in the 1960s and ran on both 2<sup>nd</sup> and 3<sup>rd</sup> generation computers. Engineering applications which involved algebraic algorithms were the type of programs decompiled. The user was required to define rules for the recognition of subroutines. The decompiler was 90% accurate, and some manual intervention was required [ Sass66 ].</p> <p>This decompiler makes use of assembler input programs rather than pure binary code. Assembler programs contain useful information in the form of names, macros, data and instructions, which are not available in binary or executable programs, and therefore eliminate the problem of separating data from instructions in the parsing phase of a decompiler.</p>"},{"location":"decompilers/dec-history-1/#autocoder-to-cobol-conversion-aid-program-1967","title":"Autocoder to Cobol Conversion Aid Program, 1967","text":"<p>Housel reported on a set of commercial decompilers developed by IBM to translate Autocoder programs, which were business data processing oriented, to Cobol. The translation was a one-to-one mapping and therefore manual optimization was required. The size of the final programs occupied 2.1 times the core storage of the original program [ Hous73 ].</p> <p>This decompiler is really a translation tool of one language to another. No attempt is made to analyze the program and reduce the number of instructions generated. Inefficient code was produced in general.</p>"},{"location":"decompilers/dec-history-1/#the-inverse-compiler-1972-6","title":"The Inverse Compiler, 1972-6","text":"<p>The Inverse compiler, also known as the Sperry*Univac 494 to 1100 inverse compiler, was developed at Univac to aid in the migration to newer machines, including business applications (i.e. decompilation to COBOL). This decompiler was based on the Lockheed Neliac decompiler.</p>"},{"location":"decompilers/dec-history-1/#crhollander-1973","title":"C.R.Hollander, 1973","text":"<p>Hollander's PhD dissertation [ Holl73 ] describes a decompiler designed around a formal syntax-oriented metalanguage, and consisting of 5 cooperating sequential processes; initializer, scanner, parser, constructor, and generator; each implemented as an interpreter of sets of metarules. The decompiler was a metasystem that defined its operations by implementing interpreters.</p> <p>The initializer loads the program and converts it into an internal representation. The scanner interacts with the initializer when finding the fields of an instruction, and interacts with the parser when matching source code templates against instructions. The parser establishes the correspondence between syntactic phrases in the source language and their semantic equivalents in the target language. Finally, the constructor and generator generate code for the final program.</p> <p>An experimental decompiler was implemented to translate a subset of IBM's System/360 assembler into an Algol-like target language. This decompiler was written in Algol-W, a compiler developed at Stanford University, and worked correctly on the 10 programs it was tested against.</p> <p>This work presents a novel approach to decompilation, by means of a formal syntax-oriented metalanguage, but its main drawback is precisely this methodology, which is equivalent to a pattern-matching operation of assembler instructions into high-level instructions. This limits the amount of assembler instructions that can be decompiled, as instructions that belong to a pattern need to be in a particular order to be recognized; intermediate instructions, different control flow patterns, or optimized code is not allowed. In order for syntax-oriented decompilers to work, the set of all possible patterns would need to be enumerated for each high-level instruction of each different compiler. Another approach would be to write a decompiler for a specific compiler, and make use of the specifications of that compiler; this approach is only possible if the compiler writer is willing to reveal the specifications of his compiler. It appears that Hollander's decompiler worked because the compiler specifications for the Algol-W compiler that he was using were known, as this compiler was written at the University where he was doing this research. The set of assembler instructions generated for a particular Algol-W instruction were known in this case.</p>"},{"location":"decompilers/dec-history-1/#bchousel-1973","title":"B.C.Housel, 1973","text":"<p>Housel's PhD dissertation [ Hous73 ] describes a clear approach to decompilation by borrowing concepts from compiler, graph, and optimization theory. His decompiler involves 3 major phases: partial assembly, analyzer, and code generation.</p> <p>The partial assembly phase separates data from instructions, builds a control flow graph, and generates an intermediate representation of the program. The analyzer analyzes the program in order to detect program loops and eliminate unnecessary intermediate instructions. Finally, the code generator optimizes the translation of arithmetic expressions, and generates code for the target language.</p> <p>An experimental decompiler was written for Knuth's MIX assembler (MIXAL), producing PL/1 code for the IBM 370 machines. 6 programs were tested, 88% of the generated statements were correct, and the remaining 12% required manual intervention [ Hous73b HH74 ].</p> <p>Housel made a real attempt at a general decompiler design, although his experimental decompiler was time constrained (completed in about 5 person-months). He describes a series of mappings (transformations) to a \"Final Abstract Representation\" and back again [ HH74 ]. The experimental decompiler was extended in Friedman's work.</p> <p>This decompiler proved that by using known compiler and graph methods, a decompiler could be written that produced good high-level code. The use of an intermediate representation made the analysis completely machine independent. The main objection to this methodology is the choice of source language, MIX assembler, not only for the greater amount of information available in these programs, but for being a simplified non-real-life assembler language.</p>"},{"location":"decompilers/dec-history-1/#the-piler-system-1974","title":"The Piler System, 1974","text":"<p>Barbe's Piler system attempts to be a general decompiler that translates a large class of source--target language pairs to help in the automatic translation of computer programs. The Piler system was composed of three phases: interpretation, analysis, and conversion. In this way, different interpreters could be written for different source machine languages, and different converters could be written for different target high-level languages, making it simple to write decompilers for different source--target language pairs. Other uses for this decompiler included documentation, debugging aid, and evaluation of the code generated by a compiler.</p> <p>During interpretation, the source machine program was loaded into memory, parsed and converted into a 3-address microform representation. This meant that each machine instruction required one or more microform instructions. The analyzer determined the logical structure of the program by means of data flow analysis, and modified the microform representation to an intermediate representation. A flowchart of the program after this analysis was made available to users, and they could even modify the flowchart, if there were any errors, on behalf of the decompiler. Finally, the converter generated code for the target high-level language [ Barb74 ].</p> <p>Although the Piler system attempted to be a general decompiler, only an interpreter for machine language of the GE/Honeywell 600 computer was written, and skeletal converters for Univac 1108's Fortran and Cobol were developed. The main effort of this project concentrated on the analyzer. See also PilerSystem.</p> <p>The Piler system was a first attempt at a general decompiler for a large class of source and target languages. Its main problem was to attempt to be general enough with the use of a microform representation, which was even lower-level than an assembler-type representation.</p>"},{"location":"decompilers/dec-history-1/#flfriedman-1974","title":"F.L.Friedman, 1974","text":"<p>Friedman's PhD dissertation describes a decompiler used for the transfer of mini-computer operating systems within the same architectural class [ Frie74 ]. Four main phases are described: pre-processor, decompiler, code generator, and compiler.</p> <p>The pre-processor converts assembler code into a standard form (descriptive assembler language). The decompiler takes the standard assembler form, analyses it, and decompiles it into an internal representation, from which FRECL code is then generated by the code generator. Finally, a FRECL compiler compiles this program into machine code for another machine. FRECL is a high-level language for program transport and development; it was developed by Friedman, who also wrote a compiler for it. The decompiler used in this project was an adaptation of Housel's decompiler [ Hous73 ].</p> <p>Two experiments were performed; the first one involved the transport of a small but self-contained portion of the IBM 1130 Disk Monitor System to Microdata 1600/21; up to 33% manual intervention was required on the input assembler programs. Overall, the amount of effort required to prepare the code for input to the transport system was too great to be completed in a reasonable amount of time; therefore, a second experiment was conducted. The second experiment decompiled Microdata 1621 operating system programs into FRECL and compiled them back again into Microdata 1621 machine code. Some of the resultant programs were re-inserted into the operating system and tested. On average, only 2% of the input assembler instructions required manual intervention, but the final machine program had a 194% increase in the number of machine instructions. See also [ FS73 ].</p> <p>This dissertation is a first attempt at decompiling operating system code, and it illustrates the difficulties faced by the decompiler when decompiling machine-dependent code. Input programs to this transport system require a large amount of effort to be presented in the format required by the system, and the final produced programs appear to be inefficient; both in the size of the program and the time to execute many more machine instructions.</p>"},{"location":"decompilers/dec-history-1/#ultrasystems-1974","title":"Ultrasystems, 1974","text":"<p>Hopwood reported on a decompilation project at Ultrasystems, Inc., in which he was a consultant for the design of the system [ Hopw78 ]. This decompiler was to be used as a documentation tool for the Trident submarine fire control software system. It took as input Trident assembler programs, and produced programs in the Trident High-Level Language (THLL) that was being developed at this company. Four main stages were distinguished: normalization, analysis, expression condensation, and code generation.</p> <p>The input assembler programs were normalized so that data areas were distinguished with pseudo-instructions. An intermediate representation was generated, and the data analyzed. Arithmetic and logical expressions were built during a process of expression condensation, and finally, the output high-level language program was generated by matching control structures to those available in THLL.</p> <p>This project attempts to document assembler programs by converting them into high-level language. The fact is, given the time constraints of the project, the expression condensation phase was not coded, and therefore the output programs were hard to read, as several instructions were required for a single expression.</p>"},{"location":"decompilers/dec-history-1/#vschneider-and-gwiniger-1974","title":"V.Schneider and G.Winiger, 1974","text":"<p>Schneider and Winiger presented a notation for specifying the compilation and decompilation of high-level languages. By defining a context-free grammar for the compilation process (i.e. describe all possible 2-address object code produced from expressions and assignments), the paper shows how this grammar can be inverted to decompile the object code into the original source program [ Schn74 ]. Even more, an ambiguous compilation grammar will produce optimal object code, and will generate an unambiguous decompilation grammar. A case study showed that the object code produced by the Algol 60 constructs could not be decompiled deterministically. This work was part of a future decompiler, but further references in the literature about this work were not found.</p> <p>This work presents, in a different way, a syntax-oriented decompiler [ Holl73 ]; that is, a decompiler that uses pattern matching of a series of object instructions to reconstruct the original source program. In this case, the compilation grammar needs to be known in order to invert the grammar and generate a decompilation grammar. Note that no optimization is possible if it is not defined as part of the compilation grammar.</p>"},{"location":"decompilers/dec-history-1/#l-peter-deutsch-1977-1979","title":"L Peter Deutsch 1977-1979","text":"<p>L. Peter Deutsch, using technology derived from his Ph.D thesis (\"An Interactive Program Verifier\", U.C. Berkeley, 1973), built a very small-scale instruction-set-retargetable decompiler (RMICRO) in Lisp at Xerox PARC. It was able to decompile binary code in 4 instruction sets (two microcodes, one minicomputer, and one bytecoded) into a C-like language, using Baker's algorithm (Journal of the ACM, January 1977) to convert flowgraphs into if/while constructs. It never worked very well, and was not eveloped further. Peter is known for work on Lisp systems in the '70s, and later work on Ghostscript and just-in-time compilation for Smalltalk.</p>"},{"location":"decompilers/dec-history-1/#decompilation-of-polish-code-1977-1981-1988","title":"Decompilation of Polish code, 1977, 1981, 1988","text":"<p>Two papers in the area of decompilation of Polish code into Basic code are found in the literature. The problem arises in connection with highly interactive systems, where a fast response is required to every input from the user. The user's program is kept in an intermediate form, and then ``decompiled'' each time a command is issued. An algorithm for the translation of reverse Polish notation to expressions is given in [ Balb79 ].</p> <p>The second paper presents the process of decompilation as a two step problem: the need to convert machine code to Polish representation, and the conversion of Polish code to source form. The paper concentrates on the second step of the decompilation problem, but yet claims to be decompiling Polish code to Basic code by means of a context-free grammar for Polish notation and a left-to-right or right-to-left parsing scheme [ Bert81 ].</p> <p>This technique was recently used in a decompiler that converted reverse Polish code into spreadsheet expressions [ May88 ]. In this case, the programmers of a product that included a spreadsheet-like component wanted to speed up the product by storing user's expressions in a compiled form, reverse Polish notation in this case, and decompile these expressions whenever the user wanted to see or modify them. Parentheses were left as part of the reverse Polish notation to reconstruct the exact same expression the user had input to the system.</p> <p>The use of the word decompilation in this sense is a misuse of the term. All that is being presented in these papers is a method for re-constructing or deparsing the original expression (written in Basic or Spreadsheet expressions) given an intermediate Polish representation of a program. In the case of the Polish to Basic translators, no explanation is given as to how to arrive at such an intermediate representation given a machine program.</p>"},{"location":"decompilers/dec-history-1/#glhopwood-1978","title":"G.L.Hopwood, 1978","text":"<p>Hopwood's PhD dissertation [ Hopw78 ] describes a 7-step decompiler designed for the purposes of transferability and documentation. It is stated that the decompilation process can be aided by manual intervention or other external information.</p> <p>The input program to the decompiler is formatted by a preprocessor, then loaded into memory, and a control flow graph of the program is built. The nodes of this graph represent one instruction. After constructing the graph, control patterns are recognized, and instructions that generate a goto statement are eliminated by the use of either node splitting or the introduction of synthetic variables. The source program is then translated into an intermediate machine independent code, and analysis of variable usage is performed on this representation in order to find expressions and eliminate unnecessary variables by a method of forward substitution. Finally, code is generated for each intermediate instruction, functions are implemented to represent operations not supported by the target language, and comments are provided. Manual intervention was required to prepare the input data, provide additional information that the decompiler needed during the translation process, and to make modifications to the target program.</p> <p>An experimental decompiler was written for the Varian Data machines 620/i. It decompiled assembler into MOL620, a machine-oriented language developed at University of California at Irvine by M.D.Hopwood and the author. The decompiler was tested with a large debugger program, Isadora, which was written in assembler. The generated decompiled program was manually modified to recompile it into machine code, as there were calls to interrupt service routines, self-modifying code, and extra registers used for subroutine calls. The final program was better documented than the original assembler program.</p> <p>The main drawbacks of this research are the granularity of the control flow graph and the use of registers in the final target program. In the former case, Hopwood chose to build control flow graphs that had one node per instruction; this means that the size of the control flow graph is quite large for large programs, and there is no benefit gained as opposed to using nodes that are basic blocks (i.e. the size of the nodes is dependent on the number of changes of flow of control). In the latter case, the MOL620 language allows for the use of machine registers, and sample code illustrated in Hopwood's dissertation shows that registers were used as part of expressions and arguments to subroutine calls. The concept of registers is not a high-level concept available in high-level languages, and it should not be used if wanting to generate high-level code.</p>"},{"location":"decompilers/dec-history-1/#daworkman-1978","title":"D.A.Workman, 1978","text":"<p>This work describes the use of decompilation in the design of a high-level language suitable for real time training device systems, in particular the F4 trainer aircraft [ Work78 ]. The operating system of the F4 was written in assembler, and it was therefore the input language to this decompiler. The output language was not determined as this project was to design one, thus code generation was not implemented.</p> <p>Two phases of the decompiler were implemented: the first phase, which mapped the assembler to an intermediate language and gathered statistics about the source program, and the second phase, which generated a control flow graph of basic blocks, classified the instructions according to their probable type, and analyzed the flow of control in order to determine high-level control structures. The results indicated the need of a high-level language that handled bit strings, supported looping and conditional control structures, and did not require dynamic data structures or recursion.</p> <p>This work presents a novel use of decompilation techniques, although the input language was not machine code but assembler. A simple data analysis was done by classifying instructions, but did not attempt to analyze them completely as there was no need to generate high-level code. The analysis of the control flow is complete and considers 8 different categories of loops and 2-way conditional statements.</p>"},{"location":"decompilers/dec-history-1/#history-1980-1999","title":"History: 1980-1999","text":""},{"location":"decompilers/dec-history-1/#zebra-1981","title":"Zebra, 1981","text":"<p>The Zebra prototype was developed at the Naval Underwater Systems Centre in an attempt to achieve portability of assembler programs. Zebra took as input a subset of the ULTRA/32 assembler, called AN/UYK-7, and produced assembler for the PDP11/70. The project was described by D.L.Brinkley in [ Brin81 ].</p> <p>The Zebra decompiler was composed of 3 passes: a lexical and flow analysis pass, which parsed the program and performed control flow analysis in the graph of basic blocks. The second pass was concerned with the translation of the program to an intermediate form, and the third pass simplified the intermediate representation by eliminating extraneous loads and stores, in much the same way described by Housel [ Hous73 , Hous73b ]. It was concluded that it was hard to capture the semantics of the program and that decompilationwas economically impractical, but it could aid in the transportation process.</p> <p>This project made use of known technology to develop a decompiler of assembler programs. No new concepts were introduced by this research, but it raised the point that decompilation is to be used as a tool to aid in the solution of a problem, but not as tool that will give all solutions to the problem, given that a 100% correct decompiler cannot be built.</p>"},{"location":"decompilers/dec-history-1/#decompilation-of-dml-programs-1982","title":"Decompilation of DML programs, 1982","text":"<p>A decompiler of database code was designed to convert a subset of Codasyl DML programs, written with procedural operations, into a relational system with a nonprocedural query specification. An Access Path Model is introduced to interpret the semantic accesses performed by the program. In order to determine how FIND operations implement semantic accesses, a global data flow reaching analysis is performed on the control flow graph, and operations are matched to templates. The final graph structures are remapped into a relational structure. This method depends on the logical order of the objects and a standard ordering of the DML statements [ Katz82 ].</p> <p>Another decompiler of database code was proposed to decompile well-coded application programs into a proposed semantic representation is described in [ Dors82 ]. This work was induced by changes in the use requirements of a Database Management System (DBMS), where application programs were written in Cobol-DML. A decompiler of Cobol-DML programs was written to analyse and convert application programs into a model and schema-independent representation. This representation was later modified or restructured to account for database changes. Language templates were used to match against key instructions of a Cobol-DML programs.</p> <p>In the context of databases, decompilation is viewed as the process of grouping a sequence of statements which represent a query into another (nonprocedural) specification. Data flow analysis is required, but all other stages of a decompiler are not implemented for this type of application.</p>"},{"location":"decompilers/dec-history-1/#forth-decompiler-1982-1984","title":"Forth Decompiler, 1982, 1984","text":"<p>A recursive Forth decompiler is a tool that scans through a compiled dictionary entry and decompiles words into primitives and addresses [ Dudl82 ]. Such a decompiler is considered one of the most useful tools in the Forth toolbox [ Hill84 ]. The decompiler implements a recursive descent parser so that decompiled words can be decompiled in a recursive fashion.</p> <p>These works present a deparsing tool rather than a decompiler. The tool recursively scans through a dictionary table and returns the primitives or addresses associated with a given word.</p>"},{"location":"decompilers/dec-history-1/#dataflex-decompilers-1984","title":"Dataflex Decompilers, 1984","text":"<p>DataFlex is a macro-based language. Some macros include over 80 DataFlex commands in one macro command. The Database Managers company Dataflex Decompilers have the capability of recovering the highest-level macro command instead of the low-level commands that compose such a macro. The techniques used in this decompiler include pattern matching and the recovery of control structures such as if's and loops. The generated code is functionally equivalent to the original source and is guaranteed to be recompilable without changes.</p>"},{"location":"decompilers/dec-history-1/#software-transport-system-1985","title":"Software Transport System, 1985","text":"<p>C.W.Yoo describes an automatic Software Transport System (STS) that moves assembler code from one machine to another. The process involves the decompilation of an assembler program for machine m1 to a high-level language, and the compilation of this program in a machine m2 to assembler. An experimental decompiler was developed on the Intel 8080 architecture; it took as input assembler programs and produced PL/M programs. The recompiled PL/M programs were up to 23% more efficient than their assembler counterpart. An experimental STS was developed to develop a C cross-compiler for the Z-80 processor. The project encountered problems in the lack of data type in the STS [ Woo85 ].</p> <p>The STS took as input an assembler program for machine m1 and an assembler grammar for machine m2, and produced an assembler program for machine m2. The input grammar was parsed and produced tables used by the abstract syntax tree parser to parse the input assembler program and generate an abstract syntax tree (AST) of the program. This AST was the input to the decompiler, which then performed control and data flow analyses, in much the same way described by Hollander [ Holl73 ], Friedman [ Frie74 ], and Barbe [ Barb74 ], and finally generated high-level code. The high-level language was then compiled for machine m2.</p> <p>This work does not present any new research into the decompilation area, but it does present a novel approach to the transportation of assembler programs by means of a grammar describing the assembler instructions of the target architecture.</p>"},{"location":"decompilers/dec-history-1/#decomp-1988","title":"Decomp, 1988","text":"<p>See DecompDecompiler for information about a decompiler for the Vax BSD 4.2 which took as input object files, and produced C-like programs.</p>"},{"location":"decompilers/dec-history-1/#fermat-1989-to-present","title":"FermaT, 1989 to present","text":"<p>Martin Ward's PhD thesis [ War89 ] is about formal, provable program transformations. He has written a program transformation engine called FermaT which facilitates forward and reverse engineering from assembly language to specifications and back again. The technology is marketed through the company SoftwareMigrations. Real-world decompilation of assembly language programs (such as IBM-370 assembler) to C [ War99 ] and COBOL have been performed, and recently from 80186 assembly language to C [ SML03 ].</p>"},{"location":"decompilers/dec-history-1/#exe2c-1990","title":"exe2c, 1990","text":"<p>The Austin Code Works sponsored the development of the exe2c decompiler, targetted at the PC compatible family of computers running the DOS operating system [ Aust91 ]. The project was announced in April 1990 [ Guth90 ], tested by about 20 people, and it was decided that it needed some more work to decompile in C. A year later, the project reached a beta operational level [ Guth91a ], but was never finished [ Guth91b ]. I (Cristina Cifuentes) was a beta tester of this release.</p> <p>exe2c is a multipass decompiler that consists of 3 programs: e2a, a2aparse, and e2c. e2a is the disassembler. It converts executable files to assembler, and produces a commented assembler listing as well. e2aparse is the assembler to C front-end processor, which analyzes the assembler file produced by e2a and generates .cod and .glb files. Finally, the e2c program translates the files prepared by a2aparse and generates pseudo-C. An integrated environment, envmnu, is also provided.</p> <p>Programs decompiled by exe2c make use of a header file that defines registers, types and macros. The output C programs are hard to understand because they rely on registers and condition codes (represented by Boolean variables). Normally, one machine instruction is decompiled into one or more C instructions that perform the required operation on registers, and set up condition codes if required by the instruction. Expressions and arguments to subroutines are not determined, and a local stack is used for the final C programs. It is obvious from this output code that a data flow analysis was not implemented in exe2c. This decompiler has implemented a control flow analysis stage; looping and conditional constructs are available. The choice of control constructs is generally adequate. Case tables are not detected correctly, though. The number and type of procedures decompiled shows that all library routines, and compiler start-up code and runtime support routines found in the program are decompiled. The nature of these routines is normally low-level, as they are normally written in assembler. These routines are hard to decompile as, in most cases, there is no high-level counterpart (unless it is low-level type C code).</p> <p>This decompiler is a first effort in many years to decompile executable files. The results show that a data flow analysis and heuristics are required to produce better C code. Also, a mechanism to skip all extraneous code introduced by the compiler and to detect library subroutines would be beneficial.</p>"},{"location":"decompilers/dec-history-1/#plm-80-decompiler-1991","title":"PLM-80 Decompiler, 1991","text":"<p>The Information Technology Division of the Australian Department of Defence researched into decompilation for defence applications, such as maintenance of obsolete code, production of scientific and technical intelligence, and assessment of systems for hazards to safety or security. This work was described by S.T. Hood in [ Hood91 ].</p> <p>Techniques for the construction of decompilers using definite-clause grammars, an extension of context-free grammars, in a Prolog environment are described. A Prolog database is used to store the initial assembler code and the recognised syntactic structures of the grammar. A prototype decompiler for Intel 8085 assembler programs compiled by a PLM-80 compiler was written in Prolog. The decompiler produced target programs in Small-C, a subset of the C language. The definite-clause grammar given in this report was capable of recognizing if-then control structures, and while loops, as well as static (global) and automatic (local) variables of simple types (i.e. character, integers, and longs). A graphical user interface was written to display the assembler and pseudo-C programs, and to enable the user to assign variable names, and comments. This interface also asked the user for the entry point to the main program, and allowed him to select the control construct to be recognized.</p> <p>The analysis performed by this decompiler is limited to the recognition of control structures and simple data types. No analysis on the use of registers is done or mentioned. Automatic variables are represented by an indexed variable that represents the stack. The graphical interface helps the user document the decompiled program by means of comments and meaningful variable names. This analysis does not support optimized code.</p>"},{"location":"decompilers/dec-history-1/#j-ogorman-phd-thesis-1991","title":"J. O'Gorman PhD thesis, 1991","text":"<p>The Systematic Decompilation thesis by John O'Gorman [ OGor91 ], University of Limerick, describes a pattern matching technique used for decompiling VAX binaries into Pascal source code. The technique requires the availability of the compiler used, performs a coverage of constructs available in the language, and creates small test programs that use the constructs, in order to derive the patterns of machine code used for each high-level construct. When decompiling a Pascal executable, the patterns are matched to determine which Pascal construct to recreated. Unoptimized code is used.</p> <p>The thesis is available for downloading (ftp) in postscript format: ftp://www.csis.ul.ie/techrpts/ul-91-12.ps.</p>"},{"location":"decompilers/dec-history-1/#decompiler-compiler-1991-1994","title":"Decompiler compiler, 1991-1994","text":"<p>A decompiler compiler is a tool that takes as input a compiler specification and the corresponding portions of object code, and returns the code for a decompiler; i.e. it is an automatic way of generating decompilers, much in the same way that yacc is used to generate compilers [ Bowe91a, Bowe91b, Breu94 ].</p> <p>Two approaches are described to generate such a decompiler compiler: a logic and a functional programming approach. The former approach makes use of the bidirectionality of logic programming languages such as Prolog, and runs the specification of the compiler backwards to obtain a decompiler [Bowe91a, Bowe91b, Bowe93b]. In theory this is correct, but in practice this approach is limited to the implementation of the Prolog interpreter, and therefore problems of strictness and reversibility are encountered [ Breu92, Breu93 ]. The latter approach is based on the logic approach but makes use of lazy functional programming languages like Haskell, to generate a more efficient decompiler [ aBowe91a, Bowe91b, Bowe93b ]. Even if a non-lazy functional language is to be used, laziness can be simulated in the form of objects rather than lists.</p> <p>The decompiler produced by a decompiler compiler will take as input object code and return a list of source codes that can be compiled to the given object code. In order to achieve this, an enumeration of all possible source codes would be required, given a description of an arbitrary inherited attribute grammar. It is proved that such an enumeration is equivalent to the Halting Problem [ Breu92, Breu93 ], and is therefore non-computable. Even further, there is no computable method which takes an attribute grammar description and decides whether or not the compiled code will give a terminating enumeration for a given value of the attribute [ Breu92, Breu93 ], so it is not straightforward which grammars can be used. Therefore, the class of grammars acceptable to this method needs to be restricted to those that produce a complete enumeration, such as non left-recursive grammars.</p> <p>An implementation of this method was firstly done for a subset of an Occam-like language using a functional programming language. The decompiler grammar was an inherited attribute grammar which took the intended object code as an argument [ Breu92, Breu93 ]. A Prolog decompiler was also described based on the compiler specification. This decompiler applied the clauses of the compiler in a selective and ordered way, so that the problem of non-termination would not be met, and only a subset of the source code programs would be returned (rather than an infinite list) [ Bowe91c, Bowe93 ]. Recently, this method made use of an imperative programming language, C++, due to the inefficiencies of the functional and logic approach. In this prototype, C++ object's were used as lazy lists, and a set of library functions was written to implement the operators of the intermediate representation used [ Breu94 ]. Problems with optimized code have been detected.</p> <p>As illustrated by this research, decompiler compilers can be constructed automatically if the set of compiler specifications and object code produced for each clause of the specification is known. In general, this is not the case as compiler writers do not disclose their compiler specifications. Only customized compilers and decompilers can be built by this method. It is also noted that optimizations produced by the optimization stage of a compiler are not handled by this method, and that real executable programs cannot be decompiled by the decompilers generated by the method described. The problem of separating instructions from data is not addressed, nor is the problem of determining the data types of variables used in the executable program. In conclusion, decompiler compilers can be generated automatically if the object code produced by a compiler is known, but the generated decompilers cannot decompile arbitrary executable programs.</p>"},{"location":"decompilers/dec-history-1/#8086-c-decompiling-system-1991-1993","title":"8086 C Decompiling System, 1991-1993","text":"<p>This decompiler takes as input executable files from a DOS environment and produces C programs. The input files need to be compiled with Microsoft C version 5.0 in the small memory model [ Fuan93 ]. Five phases were described: recognition of library functions, symbolic execution, recognition of data types, program transformation, and C code generation. The recognition of library functions and intermediate language was further described in [ Fuan91, Hung91 ].</p> <p>The recognition of library functions for Microsoft C was done to eliminate subroutines that were part of a library, and therefore produce C code for only the user routines. A table of C library functions is built-into the decompiling system. For each library function, its name, characteristic code (sequence of instructions that distinguish this function from any other function), number of instructions in the characteristic code, and method to recognize the function were stored. This was done manually by the decompiler writer. The symbolic execution translated machine instructions to intermediate instructions, and represented each instruction in terms of its symbolic contents. The recognition of data types is done by a set of rules for the collection of information on different data types and analysis rules to determine the data type in use. The program transformation transforms storage calculation into address expressions, e.g. array addressing. Finally, the C code generator transforms the program structure by finding control structures, and generates C code.</p> <p>8086C seems to be based on a Unix/68000 decompiler called 68000C [ Zong88 ]. See also DECLER.</p> <p>This decompiling system makes use of library function recognition to generate more readable C programs. The method of library recognition is hand-crafted, and therefore inefficient if other versions of the compiler, other memory models, or other compilers were used to compile the original programs. The recognition of data types is a first attempt to recognize types of arrays, pointers and structures, but not much detail is given in the paper. No description is given as to how an address expression is reached in the intermediate code, and no examples are given to show the quality of the final C programs.</p>"},{"location":"decompilers/dec-history-1/#alpha-axp-migration-tools-1993","title":"Alpha AXP Migration Tools, 1993","text":"<p>When Digital Equipment Corporation designed the Alpha AXP architecture, the AXP team got involved in a project to run existing VAX and MIPS code on the new Alpha AXP computers. They opted for a binary translator which would convert a sequence of instructions of the old architecture into a sequence of instructions of the new architecture. The process needed to be fully automatic and to cater for code created or modified during execution. Two parts to the migration process were defined: a binary translation, and a runtime environment [ Site93 ].</p> <p>The binary translation phase took binary programs and translated them into AXP opcodes. It made use of decompilation techniques to understand the underlying meaning of the machine instructions. Condition code usage analysis was performed as these conditions do not exist on the Alpha architecture. The code was also analyzed to determine function return values and find bugs (e.g. uninitialized variables). MIPS has standard library routines which are embedded in the binary program. In this case, a pattern matching algorithm was used to detect routines that were library routines, such routines were not analysed but replaced by their name. Idioms were also found and replaced by an optimal instruction sequence. Finally, code was generated in the form of AXP opcodes. The new binary file had both, the new code and the old code.</p> <p>The runtime environment executes the translated code and acts as a bridge between the new and old operating systems (e.g. different calling standards, exception handling). It had a built-in interpreter of old code to run old code not discovered or nonexistent at translation time. This was possible because the old code was also saved as part of the new binary file.</p> <p>Two binary translators were written: VEST, to translate from the OpenVMS VAX system to the OpenVMS AXP system, and mx, to translate ULTRIX MIPS images to DEC OSF/1 AXP images. The runtime environments for these translators were TIE and mxr respectively.</p> <p>This project illustrates the use of decompilation techniques in a modern translation system. It proved successful for a large class of binary programs. Some of the programs that could not be translated were programs that were technically infeasible to translate, such as programs that use privileged opcodes, or run with superuser privileges.</p>"},{"location":"decompilers/dec-history-1/#sourceprom-comparator-1993","title":"Source/PROM Comparator, 1993","text":"<p>A tool to demonstrate the equivalence of source code and PROM contents was developed at the Nuclear Electric plc, UK, to verify the correct translation of PL/M-86 programs into PROM programs executed by safety critical computer controlled systems [ Pave93 ].</p> <p>Three stages are identified: the reconstitution of object code files from the PROM files, the disassembly of object code to an assembler-like form with help from a name-table built up from the source code, and decompilation of assembler programs and comparison with the original source code. In the decompiling stage, it was noted that it was necessary to eliminate intermediate jumps, registers and stack operations, identify procedure arguments, resolve indexes of structures, arrays and pointers, and convert the expresssions to a normal form. In order to compare the original program and the decompiled program, an intermediate language was used. The source program was translated to this language with the use of a commercial product, and the output of the decompilation stage was written in the same language. The project proved to be a practical way of verifying the correctness of translated code, and to demonstrate that the tools used to create the programs (compiler, linker, optimizer) behave reliably for the particular safety system analyzed.</p> <p>This project describes a use of decompilation techniques, to help demonstrate the equivalence of high-level and low-level code in a safety-critical system. The decompilation stage performs much of the analysis, with help from a symbol table constructed from the original source program. The task is simplified by the knowledge of the compiler used to compile the high-level programs.</p>"},{"location":"decompilers/dec-history-1/#cristina-cifuentes-phd-thesis-reverse-compilation-techniques-1994","title":"Cristina Cifuentes' PhD Thesis \"Reverse Compilation Techniques\", 1994","text":"<p>Considered by some to be the definitive work on general decompilation from binary files. You can download the thesis from Cristina Cifuentes' page, as a compressed postscript file (474K). This work draws heavily on standard forward engineering techniques such as data flow analysis, applied to decompilation. Similarly, graph techniques are used to restructure the generated code into standard loops, conditional statements, and other high level constructs. Type recovery is limited to built-in types (no arrays or structures). Cristina demonstrated her techniques in a research prototype called dcc.</p> <p>After her PhD, Cristina worked for some years on Binary Translation, for example see the UQBT page. Cristina is currently associate advisor for Mike Van Emmerik's current PhD research, also on decompilation.</p>"},{"location":"decompilers/dec-history-1/#decler-decompiler-1995","title":"DECLER Decompiler, 1995","text":"<p>DECLER [ DRM95 ], [ Zong96 ], [ CL00 ] is a five stage decompiler, based on 8086C and 68000C. The stages are disassembler, library function recogniser, symbolic executer, \"AB transformer\", and \"C transformer\". The AB transformer appears to be a formal transformation system that recovers types as a side effect. The C transformer is the structuring back end.</p>"},{"location":"decompilers/dec-history-1/#university-of-queensland-binary-translator-1997-2001","title":"University of Queensland Binary Translator, 1997-2001","text":"<p>The University of Queensland Binary Translator (UQBT), 1997-2001 . This Binary Translator uses a standard C compiler as the \"back end\"; in other words, it emits C source code. However, this is not the same as a decompiler, where the goal is human readable high level code. As a result, UQBT can't be used in any of the applications that come under the heading of \"comprehension aid\" or \"maintainable code\". Work on UQBT was not completed, however it was capable of producing low level source code for moderate sized programs, such as the smaller SPEC benchmarks [ CVE00, CVEU+99 ].</p>"},{"location":"decompilers/dec-history-1/#a-mycrofts-type-reconstruction-for-decompilation-1999","title":"A. Mycroft's Type Reconstruction for Decompilation, 1999","text":"<p>One of the hardest problems to solve in decompilation is that of recovering high-level data types from machine code in a correct way. Such types include structures, arrays and more. In this work, Alan Mycroft presents a system for infering high-level types from assembler-based (RTL) code [ Mycr99 ]. Alan's type inference system is based on Milner's work for ML. The paper presents a type system, the constraints on types and worked-through examples that include structures and arrays as part of their output. Experimental results for the system are not available.</p> <p>This is the best type system that I am aware of that currently deals with recoverying high-level data types in a machine-independent way, as it is based on RTLs and makes no unreasonable assumptions on the shape of the RTLs. Implementation results are needed in order to determine how feasible this system is in real practice.</p>"},{"location":"decompilers/dec-history-1/#history-2000-2007","title":"History: 2000-2007","text":""},{"location":"decompilers/dec-history-1/#university-of-londons-asm21toc-reverse-compiler-2000","title":"University of London's Asm21toc reverse compiler, 2000.","text":"<p>This assembly language decompiler for Digital Signal Processing (DSP) code was written in a compiler-compiler called rdp [ JSW00 ]. The authors note that DSP is one of the last areas where assembly language is still commonly used. As usual, the decompilation from assembly language is considerably easier than from executable code; in fact the authors \"doubt the usefulness\" of decompiling from binary files.</p>"},{"location":"decompilers/dec-history-1/#proof-directed-de-compilation-of-low-level-code-2001","title":"Proof-Directed De-compilation of Low-Level Code, 2001.","text":"<p>Katsumata and Ohori published a paper [ KO01 ] on decompilation based on proof theoretical methods. The input is Jasmin, essentially Java assembly language. The output is an ML-like simply typed functional language. Their example shows an iterative implementation of the factorial function transformed into two functions (an equivalent recursive implementation). Their approach is to treat each instruction as a constructive proof representing its computation. While not immediately applicable to a general decompiler, their work may have application where proof of correctness (usually of a compilation) is required.</p> <p>In [ Myc01 ], Mycroft compares his Type-Based decompilation with this work. Structuring to loops and conditionals is not attempted by either system. He concludes that the two systems produce very similar results in the areas where they overlap, but that they have different strengths and weaknesses.</p>"},{"location":"decompilers/dec-history-1/#computer-security-analysis-through-decompilation-and-high-level-debugging-2001","title":"Computer Security Analysis through Decompilation and High-Level Debugging, 2001.","text":"<p>Cifuentes et al suggested dynamic decompilation as a way to provide a powerful tool for security work. The main idea is that the security analyst is only interested in one small piece of code at one time, and so high level code could be generated \"on the fly\". One problem with traditional (static) decompilation is that it is difficult to determine the range of possible values of variables; by contrast, a dynamic decompiler can provide at least one value (the current value) with no effort [ CWVE01 ].</p>"},{"location":"decompilers/dec-history-1/#type-propagation-in-ida-pro-disassembler-2001","title":"Type Propagation in IDA Pro Disassembler, 2001.","text":"<p>Guilfanov describes the type propagation system in the popular disassembler IDA Pro Ida Pro. The types of parameters to library calls are captured from system header files. The parameter types for commonly used libraries are saved in files called type libraries. Assignments to parameter locations are annotated with comments with the name and type of the parameter. This type information is propagated to other parts of the disassembly, including all known callers. At present, no attempt is made to find the types for other variables not associated with the parameters of any library calls [ Gui01 ].</p>"},{"location":"decompilers/dec-history-1/#disc-by-satish-kumar-2001","title":"DisC, by Satish Kumar, 2001.","text":"<p>This decompiler is designed to read only programs written in Turbo C version 2.0 or 2.01; it is an example of a compiler specific decompiler. There is no significant advantage to this approach, since general techniques are not much more difficult to implement. It is an interesting observation that since most aspects of decompilation are ultimately pattern matching in some sense, the difference between pattern matching decompilers and general ones is essentially one of the generality of the patterns. http://www.debugmode.com/dcompile/disc.htm.</p>"},{"location":"decompilers/dec-history-1/#ndcc-decompiler-2002","title":"ndcc decompiler, 2002.","text":"<p>Andr\ufffd Janz modified the dcc decompiler to read 32-bit Windows Portable Executable (PE) files. The intent was to use the modified decompiler to analyse malware. The author states that a rewrite would be needed to fully implement the 80386 instruction set. Even so, reasonable results were obtained, while retaining dcc's severe limitations [ Jan02 ].</p>"},{"location":"decompilers/dec-history-1/#the-anatomizer-decompiler-circa-2002","title":"The Anatomizer Decompiler, circa 2002.","text":"<p>K. Morisada released a decompiler for Windows 32-bit programs, which appears to be comparable in capability to the REC compiler for that platform. See also AnatomizerDecompiler and http://jdi.at.infoseek.co.jp.</p>"},{"location":"decompilers/dec-history-1/#analysis-of-virtual-method-invocation-for-binary-translation-2002","title":"Analysis of Virtual Method Invocation for Binary Translation, 2002.","text":"<p>Tr\ufffdger and Cifuentes show a method of analysing indirect call instructions. If such a call implements a virtual method call and is correctly identified, various important aspects of the call are extracted. The technique as presented is limited to one basic block; as a result, it fails for some less common cases. [ TC02 ].</p>"},{"location":"decompilers/dec-history-1/#boomerang-2002","title":"Boomerang, 2002.","text":"<p>This is an open source decompiler, with several front ends (two are well developed) and a C back end. It uses an internal representation based on the Static Single Assignment form, and pioneers dataflow-based type analysis. At the time of writing, it is still limited to quite small (toy) binary programs. http://boomerang.sourceforge.net.</p>"},{"location":"decompilers/dec-history-1/#desquirr-2002","title":"Desquirr, 2002.","text":"<p>This is an IDA Pro Plugin, written by David Eriksson as part of his Master's thesis. It decompiles one function at a time to the IDA output window. While not intended to be a serious decompiler, it illustrates what can be done with the help of a powerful disassembler and about 5000 lines of C++ code. Because a disassembler does not carry semantics for machine instructions, each supported processor requires a module to decode instruction semantics and addressing modes. The X86 and ARM processors are supported. Conditionals and loops are emitted as gotos, there is some simple switch analysis, and some recovery of parameters and returns is implemented. http://desquirr.sourceforge.net/desquirr.</p>"},{"location":"decompilers/dec-history-1/#analyzing-memory-accesses-in-x86-executables-2004","title":"Analyzing Memory Accesses in x86 Executables, 2004.","text":"<p>Balakrishnan and Reps from the University of Wisconsin have developed a framework for analysing binary programs that they call Codesurfer/x86. The aim is to produce intermediate representations that are similar to those that can be created for a program written in a high-level language. The binary is first disassembled with the IDA Pro disassembler. A plug-in for IDA Pro called Connector, provided by Grammatech Inc, interfaces to the rest of the tool. Value-set Analysis (VSA) is used to produce an intermediate representation, which is presented in a source code analysis tool called CodeSurfer (sold separately by Grammatech Inc for C/C++ source code analysis) [ BR04, RBLT05 ]. Codesurfer/x86 may be commercially available soon.</p>"},{"location":"decompilers/dec-history-1/#r-falkes-type-analysis-for-decompilers-2004","title":"R. Falke's Type Analysis for Decompilers, 2004","text":"<p>Raimar Falke, in his Diploma Thesis [ Fal04 ] (German) for the Technical University of Dresden, implemented an adaption of Mycroft's type constraint theory in a decompiler called YaDeC. He extended it to handle arrays. To keep the project manageable, he used objdump for the front end, ignored floating point types, assumed only stack parameters, and so on. An English translation of the paper's summary can be found in FalkeDiplomaSummary.</p>"},{"location":"decompilers/dec-history-1/#andromeda-decompiler-2004-2005","title":"Andromeda Decompiler, 2004-2005.","text":"<p>Andrey Shulga wrote a decompiler for Windows x86 and C. The decompiler itself was never released, however a GUI program for manipulating the IR generated by the compiler is available at the author's web site [ Shu04 ]. Only an x86 front end is written at present, and only a C/C++ back end, although the decompiler is claimed to be capable of other front and back ends. The output for the provided demonstration IR is extremely impressive, but it is not clear whether the IR is largely automatically generated by the decompiler, or hand edited. The web page has been inactive since May 2005.</p>"},{"location":"decompilers/dec-history-1/#hex-rays-decompiler-plugin-2007","title":"Hex Rays Decompiler Plugin, 2007.","text":"<p>Ilfak Guilfanov, author of the IDA Pro disassembler, released a commercial decompiler plugin for IDA Pro [ Gui07a, Gui07b ]. This plugin adds a decompiler view to the other views available in the interactive disassembler. One function is shown at a time; most functions decompile in a fraction of a second to a quite C-like output. The author stresses that output is not designed for re-compilation, only for more rapid comprehension of what the function is doing. The output includes compound conditionals (with || and &amp;&amp;), loops (for, while, break, etc), and function parameters and returns. There is also an API which gives access to the decompiler's IR, allowing custom analyses if desired. At this stage, only the x86 architecture is supported. The decompiler relies on the disassembler (and possibly manual intervention) to separate code from data and to identify functions.</p>"},{"location":"decompilers/dec-history-1/#static-single-assignment-for-decompilation-2007","title":"Static Single Assignment for Decompilation, 2007.","text":"<p>Mike Van Emmerik completed this PhD theses at the University of Queensland in October 2007 [ VE07 ]. The main theme is how the SSA form enables various aspects of decompilation to be more readily analysed, although this leads to other topics such as type analysis and the analysis of indirect jumps and calls. An industry case study is discussed. Although considerable progress is made, many problems still remain, particularly related to alias analysis. There is a chapter of results, using the Boomerang open source decompiler. A new algorithm for finding preserved locations in procedures in the presence of recursion is given. There is a comprehensive glossary of terms, history, comparison with Java decompilers, a survey of compiler infrastructures that may be suitable for decompilation, and the problems faced by decompilers are summarised in terms of separation problems (code from data, pointers from constants, and original from offset pointers).</p>"},{"location":"decompilers/dec-history-1/#references","title":"References","text":"<p>Hals62</p> <p>M.H. Halstead. Machine-independent computer programming, Chapter 11, pages 143-150. Spartan Books, 1962.</p> <p>Hals67</p> <p>M.H. Halstead. Machine independence and third generation computers. In Proceedings SJCC (Sprint Joint Computer Conference), pages 587-592, 1967.</p> <p>Hals70</p> <p>M.H. Halstead. Using the computer for program conversion. Datamation, pages 125-129, May 1970.</p> <p>Sass66</p> <p>W.A. Sassaman. A computer program to translate machine language into Fortran. In Proceedings SJCC, pages 235-239, 1966.</p> <p>Hous73</p> <p>B.C. Housel. A Study of Decompiling Machine Languages into High-Level Machine Independent Languages. PhD dissertation, Purdue University, Computer Science, August 1973.</p> <p>Hous73b</p> <p>B.C. Housel and M.H. Halstead. A methodology for machine language decompilation. Technical Report RJ 1316 (#20557), Purdue University, Department of Computer Science, December 1973. Also published as [ HH74 ].</p> <p>Holl73</p> <p>C.R. Hollander. Decompilation of Object Programs. PhD dissertation, Stanford University, Computer Science, January 1973.</p> <p>FS73</p> <p>F.L. Friedman and V.B.Schneider. A Systems Implementation Language. ACM SIGPLAN Notices 8(9), pages 60-63, September 1973.</p> <p>HH74</p> <p>B.C. Housel and M.H. Halstead. A methodology for machine language decompilation. In Proceedings of the 27<sup>th</sup> ACM Annual Conference, ACM Press, pages 254-260, 1974.</p> <p>Barb74</p> <p>P. Barbe. The Piler system of computer program translation. Technical report PLR-020, Probe Consultants Inc., September 1974. Prepared for the Office of Naval Research, distributed by National Technical Information Service, USA. ADA000294. Contract N00014-67-C-0472.</p> <p>Frie74</p> <p>F.L. Friedman. Decompilation and the Transfer of Mini-Computer Operating Systems. PhD dissertation, Purdue University, Computer Science, August 1974.</p> <p>Schn74</p> <p>V. Schneider and G. Winiger. Translation grammars for compilation and decompilation. BIT, 14:78-86, 1974.</p> <p>Bake77</p> <p>B.S. Baker. An algorithm for structuring flowgraphs. Journal of the ACM, 24(1):98-120, January 1977.</p> <p>Hopw78</p> <p>G.L. Hopwood. Decompilation. PhD dissertation, University of California, Irvine, Computer Science, 1978.</p> <p>Work78</p> <p>D.A. Workman. Language design using decompilation. Technical report, University of Central Florida, December 1978.</p> <p>Balb79</p> <p>D. Balbinot and L. Petrone. Decompilation of Polish code in Basic. Rivista di Informatica, 9(4):329-335, October 1979.</p> <p>Bert81</p> <p>M.N. Bert and L. Petrone. Decompiling context-free languages from their Polish-like representations. pages 35-57, 1981.</p> <p>May88</p> <p>W. May. A simple decompiler. Dr.Dobb's Journal, pages 50-52, June 1988.</p> <p>Cifu94</p> <p>C. Cifuentes. Reverse Compilation Techniques. PhD Dissertation. Queensland University of Technology, Department of Computing Science, 1994. Also as compressed postscript.</p> <p>Holl73</p> <p>C.R. Hollander. Decompilation of Object Programs. PhD dissertation, Stanford University, Computer Science, January 1973.</p> <p>Hous73</p> <p>B.C. Housel. A Study of Decompiling Machine Languages into High-Level Machine Independent Languages. PhD dissertation, Purdue University, Computer Science, August 1973.</p> <p>Hous73b</p> <p>B.C. Housel and M.H. Halstead. A methodology for machine language decompilation. Technical Report RJ 1316 (#20557), Purdue University, Department of Computer Science, December 1973.</p> <p>Barb74</p> <p>P. Barbe. The Piler system of computer program translation. Technical report, Probe Consultants Inc., September 1974.</p> <p>Frie74</p> <p>F.L. Friedman. Decompilation and the Transfer of Mini-Computer Operating Systems. PhD dissertation, Purdue University, Computer Science, August 1974.</p> <p>Bake77</p> <p>B.S. Baker. An algorithm for structuring flowgraphs. Journal of the ACM, 24(1):98-120, January 1977.</p> <p>Brin81</p> <p>D.L. Brinkley. Intercomputer transportation of assembly language software through decompilation. Technical report, Naval Underwater Systems Center, October 1981.</p> <p>Bert81</p> <p>M.N. Bert and L. Petrone. Decompiling context-free languages from their Polish-like representations. pages 35-57, 1981.</p> <p>Katz82</p> <p>R.H. Katz and E. Wong. Decompiling CODASYL DML into relational queries. ACM Transactions on Database Systems, 7(1):1-23, March 1982.</p> <p>Dors82</p> <p>L.M. Dorsey and S.Y. Su. The decompilation of COBOL-DML programs for the purpose of program conversion. In Proceedings of COMPSAC 82. IEEE Computer Society's Sixth International Computer Software and Applications Conference, pages 495-504, Chicago, USA, November 1982. IEEE.</p> <p>Dudl82</p> <p>R. Dudley. A recursive decompiler. FORTH Dimensions, 4(2):28, Jul-Aug 1982.</p> <p>Hill84</p> <p>N.L. Hills and D. Moines. Revisited: Recursive decompiler. FORTH Dimensions, 5(6):16-18, Mar-Apr 1984.</p> <p>Woo85</p> <p>C.W. Yoo. An approach to the transportation of computer software. Information Processing Letters, 21:153-157, September 1985.</p> <p>May88</p> <p>W. May. A simple decompiler. Dr.Dobb's Journal, pages 50-52, June 1988.</p> <p>Reut88</p> <p>J. Reuter. Formerly available from ftp://ftp.cs.washington.edu/pub/decomp.tar.Z. Public domain software, 1988. Now downloadable from the DecompDecompiler page.</p> <p>Zong88</p> <p>Liu Zongtian and Zhu Yifen, The Application of the Symbolic Execution to the 68000 C Anti-compiler, Chinese Journal of Computers, 11(10):633-637, 1988.</p> <p>War89</p> <p>M. Ward, \"Proving Program Refinements and Transformations\", Oxford University, PhD Thesis, 1989.</p> <p>Guth90</p> <p>S. Guthery. exe2c. News item in comp.compilers USENET newsgroup, 30 Apr 1990.</p> <p>Guth91a</p> <p>S. Guthery. exe2c. News item in comp.compilers USENET newsgroup, 23 Apr 1991.</p> <p>Reut91</p> <p>J. Reuter. Private communication. Email, 1991.</p> <p>Aust91</p> <p>Austin Code Works. exe2c. Beta release, 1991. Email: info@acw.com.</p> <p>Bowe91a</p> <p>J.P. Bowen, P.T. Breuer, and K.C. Lano. The REDO project: Final report. Technical Report PRG-TR-23-91, Oxford University Computing Laboratory, 11 Keble Road, Oxford OX1 3QD, December 1991.</p> <p>Bowe91b</p> <p>J. Bowen and P. Breuer. Decompilation techniques. Internal to ESPRIT REDO project no. 2487 2487-TN-PRG-1065 Version 1.2, Oxford University Computing Laboratory, 11 Keble Road, Oxford OX1 3QD, March 1991.</p> <p>Bowe91c</p> <p>J. Bowen. From programs to object code using logic and logic programming. In R. Giegerich and S.L. Graham, editors, Code Generation - Concepts, Tools, Techniques, Workshops in Computing, pages 173-192, Dagstuhl, German</p> <p>Hood91</p> <p>S.T. Hood. Decompiling with definite clause grammars. Technical Report ERL-0571-RR, Electronics Research Laboratory, DSTO Australia, PO Box 1600, Salisbury, South Australia 5108, September 1991.</p> <p>Hung91</p> <p>L. Hungmong, L. Zongtian, and Z. Yifen. Design and implementation of the intermediate language in a PC decompiler system. Mini-Micro Systems, 12(2):23-28,46, 1991.</p> <p>Fuan91</p> <p>C. Fuan and L. Zongtian. C function recognition technique and its implementation in 8086 C decompiling system. Mini-Micro Systems, 12(11):33-40,47, 1991.</p> <p>Guth91b</p> <p>S. Guthery. Private communication. Austin Code Works, 11100 Leafwood Lane, Austin, TX 78750-3587, 14 Dec 1991.</p> <p>OGor91</p> <p>J. O'Gorman. Systematic Decompilation. PhD Thesis. Technical Report UL-CSIS-91-12, University of Limerick, 1991. URL: ftp://www.csis.ul.ie/techrpts/ul-91-12.ps</p> <p>Breu92</p> <p>P.T. Breuer and J.P. Bowen. Decompilation: The enumeration of types and grammars. Technical Report PRG-TR-11-92, Oxford University Computing Laboratory, 11 Keble Road, Oxford OX1 3QD, 1992.</p> <p>Bowe93</p> <p>J. Bowen. From programs to object code and back again using logic programming: Compilation and decompilation. Journal of Software Maintenance: Research and Practice. 5(4):205-234, 1993.</p> <p>Breu93</p> <p>P.T. Breuer and J.P. Bowen. Decompilation: the enumeration of types and grammars. Transaction of Programming Languages and Systems, 1993.</p> <p>Bowe93b</p> <p>J. Bowen, P. Breuer, and K. Lano. A compendium of formal techniques for software maintenance. Software Engineering Journal, pages 253-262, September 1993.</p> <p>Pave93</p> <p>D.J. Pavey and L.A. Winsborrow. Demonstrating equivalence of source code and PROM contents. The Computer Language, 36(7):654-667, 1993.</p> <p>Fuan93</p> <p>C. Fuan, L. Zongtian, and L. Li. Design and implementation techniques of the 8086 C decompiling system. Mini-Micro Systems, 14(4):10-18,31, 1993. Chinese language.</p> <p>Breu94</p> <p>P.T. Breuer and J.P. Bowen. Generating decompilers. Information and Software Technology Journal, 1994.</p> <p>Site93</p> <p>R.L. Sites, A. Chernoff, M.B. Kirk, M.P. Marks, and S.G. Robinson. Binary translation. Communications of the ACM, 36(2):69-81, February 1993.</p> <p>Cifu94</p> <p>C. Cifuentes. Reverse Compilation Techniques. PhD Dissertation. Queensland University of Technology, Department of Computing Science, 1994.</p> <p>DRM95</p> <p>DECLER User Guide and Reference Manual. Microcomputer Institute, Hefei University of Technology, 1995.3.</p> <p>Zong96</p> <p>Liu Zongtian. DECLER: the C Lanuage Decompilation System. Microelectronics and Computer, 17(5):1-3.</p> <p>Mycr99</p> <p>A. Mycroft. Type-Based Decompilation. Proceedings of ESOP'99, LNCS 1576, Springer-Verlag, 1999.</p> <p>War99</p> <p>M. P. Ward. Assembler to C Migration using the FermaT Transformation System. Proceedings of ACSM'99, pp67-76.</p> <p>CVEU+99</p> <p>C. Cifuentes, M. Van Emmerik, D. Ung, D. Simon, and T. Waddington. Preliminary experiences with the use of the UQBT binary translation framework. In Proc. Workshop on Binary Translation, Newport Beach, pages 12-22. Technical Committee on Technical Architecture Newsletter, IEEE CS-Press, Dec 1999.</p> <p>CL00</p> <p>Kaiming Chen, Zongtian Liu, Recognition and Recovery of Switch Structure in Decompilation System, Mini-Micro Systems, 21(12):1279-1281, 2001. Chinese language.</p> <p>CVE00</p> <p>C. Cifuentes and M. Van Emmerik. UQBT: Adaptable Binary Translation at low cost. Computer 33(3) pages 60-66, March 2000.</p> <p>SML03</p> <p>M. P. Ward. Pigs from Sausages? Reengineering from Assembler to C via FermaT Transformation. White paper, http://www.smltd.com/migration-t.pdf, April 2003. Published in Science of Computer Programming Special Issue: Transformations Everywhere 52(1-3):213-255, 2004.</p> <p>JSW00</p> <p>A. Johnstone, E. Scott, and T. Womack. Reverse Compilation for Digital Signal Processors: a Working Example. In Proceedings of the Hawaii International Conference on System Sciences. IEEE-CS Press, January 2000.</p> <p>KO01</p> <p>S. Katumata and A. Ohori. Proof-directed de-compilation of low-level code. In European Symposium on Programming, volume 2028 of Lecture Notes in Computer Science, pages 352-366. Springer-Verlag 2001.</p> <p>Myc01</p> <p>A. Mycroft. Comparing type-based and proof-directed decompilation. In Proceedings of the Working Conference on Reverse Engineering, pages 362-367, Stuttgart Germany. IEEE-CS Press 2001.</p> <p>CWVE01</p> <p>C. Cifuences, T. Waddington, and M. Van Emmerik. Computer Security Analysis through Decompilation and High Level Debugging. In Proceedings of the Working Conference on Reverse Engineering, pages 375-380, Stuttgart Germany. IEEE-CS Press 2001.</p> <p>Gui01</p> <p>I. Guilfanov. A simple type system for program reengineering. In Proceedings of the Working Conference on Reverse Engineering, pages 357-361, Stuttgart Germany. IEEE-CS Press 2001.</p> <p>Jan02</p> <p>Andr\ufffd Janz. Experimente mit einem Decompiler im Hinblick auf die forensische Informatik, 2002. http://agn-www.informatik.uni-hamburg.de/papers/doc/diparb_andre_janz.pdf .</p> <p>TC02</p> <p>J. Tr\ufffdger and C. Cifuentes. Analysis of virtual method invocation for binary translation. In Proceedings of the Working Conference on Reverse Engineering, Richmond, Virginia; pages 65-74. IEEE-CS Press, 2002.</p> <p>BR04</p> <p>G. Balakrishnan and T. Reps. Analyzing Memory Accesses in x86 Executables. In Proceedings of Compiler Construction (LNCS 2985) pages 5-23. Springer-Verlag April 2004.</p> <p>Fal04</p> <p>R. Falke. Entwicklung eines Typeanalysesystem f\ufffd\ufffd\ufffdr einen Decompiler (Development of a type analysis system for a decompiler), 2004. Diploma thesis, German language. http://risimo.net/diplom.ps . No longer available, however archived here (archive.org, 491kB).</p> <p>Shu04</p> <p>Andrey Shulga. Andromeda Decompiler web page, 2004. http://shulgaaa.at.tut.by .</p> <p>RBLT05</p> <p>T. Reps, G. Balakrishnan, J. Lim and T. Teilelbaum. A Next-Generation Platform for Analysing Executables. To appear in Proc. 3<sup>rd</sup> Asian Symposium on Programming Languages and Systems, Springer-Verlag 2005.</p> <p>Gui07a</p> <p>I. Guilfanov. Blog: Decompilation Gets Real, April 2007. http://hexblog.com/2007/04/decompilation_gets_real.html .</p> <p>Gui07b</p> <p>I. Guilfanov. Hex-rays home page, 2007. http://www.hex-rays.com .</p> <p>VE07</p> <p>M. Van Emmerik. Static Single Assignment for Decompilation. PhD thesis, University of Queensland, 2007. http://vanemmerikfamily.com/mike/master.pdf or http://vanemmerikfamily.com/mike/master.ps.gz .</p>"},{"location":"decompilers/directory/","title":"Decompiler Directory","text":"<p>Over the years, many decompilers have been made by both hackers and academics alike.  In this directory, you will find a listing of all known (at least to the wiki authors) decompilers. </p> <p>Each decompiler should also be listed with some minimal facts about their differences. </p>"},{"location":"decompilers/directory/#legend","title":"Legend","text":"<ul> <li>\ud83c\udf10: open-source</li> <li>\ud83d\udc80: inactive (2 years without activity)</li> <li>0\ufe0f\u20e3: binary decompiler (compiled languages)</li> <li>\ud83d\udd0c: implemented as a plugin to another decompiler</li> </ul>"},{"location":"decompilers/directory/#alphabetical-order","title":"Alphabetical Order","text":"Decompiler Attributes Intermediate Language Release Date angr decompiler 0\ufe0f\u20e3, \ud83c\udf10 VEX, P-Code, AIL 2018 Binary Ninja 0\ufe0f\u20e3 BNIL 2015 Boomerang 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80 ? 2008 CFR \ud83c\udf10 ? 2012 Cutter 0\ufe0f\u20e3, \ud83c\udf10 ? 2017 dewolf 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udd0c BNIL 2021 DREAM 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80, \ud83d\udd0c Micro Code 2015 Fernflower \ud83c\udf10 ? 2009 Ghidra 0\ufe0f\u20e3, \ud83c\udf10 P-Code 2019 Hopper 0\ufe0f\u20e3 ? 2012 IDA Pro (HexRays) 0\ufe0f\u20e3 Micro Code 2007 ILSpy \ud83c\udf10 ? 2007 JEB 0\ufe0f\u20e3 ? 2014 Procyon \ud83c\udf10, \ud83d\udc80 ? 2012 REC Studio 0\ufe0f\u20e3, \ud83d\udc80 ? 1997 Reko 0\ufe0f\u20e3, \ud83c\udf10 ? 2007 Relyze 0\ufe0f\u20e3, \ud83d\udc80 ? 2015 RetDec 0\ufe0f\u20e3, \ud83c\udf10 LLVM IR 2017 rev.ng 0\ufe0f\u20e3, \ud83c\udf10 TCG, LLVM IR 2017 r2dec 0\ufe0f\u20e3, \ud83c\udf10 ? 2019 Snowman 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80 ? 2015 fcd 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80 ? 2015"},{"location":"decompilers/father-of-dec/","title":"The Father of Decompilation (1960-2007)","text":""},{"location":"decompilers/father-of-dec/#maintainers-note","title":"Maintainers Note","text":"<p>The following text is a clone, with small fixes, of the popular Program-Transformation Wiki, which went offline in 2025. All credits to the following text on this page go to the original creators of the Program-Transformation Wiki. The maintainers of the Decompilation Wiki do not claim credit for it, but instead preserve it here for future researchers in decompilation. </p>"},{"location":"decompilers/father-of-dec/#about","title":"About","text":"<p>Maurice Halstead, referred to fondly as Maury, received a BA degree in physics-meteorology from the University of California, Berkeley in 1940, an MS degree in aerological engineering from the Naval Academy Postgraduate School, Annapolis, MD in 1943, and a PhD degree in physical science from Johns Hopkins University, Baltimore, MD in 1951. His resume describes his professional career.</p> <p>\"Maury was a true experimental scientist, pioneer and visionary in the field of computer software. At a time when software developers were still arguing about the viability of higher level languages, Maury committed part of the resources of a Navy development project to create the first self-compiling compiler (Neliac) which was then used to complete the project on time and within budget. At another time, with a team of 30 people working on the Global Weather conversion project apparently in schedule trouble, he took the gamble to cut the project team staff in half (rather than increasing the staff). The project was completed on time due to releasing the productive members' time to productive work rather than assisting junior members with their problems.</p> <p>\"Maury's background in physics, meteorology and the physical sciences provided him with insights and interests that ultimately led to the development of software metrics which he initially called 'Software Thermodynamics.'</p> <p>\"As a boss, Maury could be the epitome of a father figure: he was supportive, interested, and always made a person feel like the ideas that he fed to them were their own (at least this was the case for me). He could also be an aggressive politician and created enemies as well as friends as a player in the development of computing capabilities of the U.S. Navy.\"</p> <p>The following sections describe three decompilation projects for a variety of machines. The aim of these decompiler projects was to aid in the migration of software to newer/successor machines, hence eliminating part of the time consuming task of rewriting code for the new machines. \"A primary requirement was to be able to decompile from binary (or more specifically using binary card images as input).\" </p>"},{"location":"decompilers/father-of-dec/#the-d-neliac-decompiler-1960-u-s-navy-electronic-labs","title":"The D-Neliac Decompiler (1960, U. S. Navy Electronic Labs)","text":"<p>The Neliac language was developed at the U. S. Navy Electronic Labs (NEL) in 1958 to support a Navy systems project. The compiler was developed to reduce the increasing costs of these systems which previously used assembler/machine language. Prior to the first decompiler, the first Neliac compiler was written in assembler by Maury and Lieutenant John White, and perhaps others. The first self-compiling Neliac compiler was written by Ensign A. E. Lemay. Neliac was a dialect of Algol 58. Neliac supported low-level code as well as high-level language constructs. These first compilers for the language were developed in 1958-9 under the direction of Maury Halstead.</p> <p>In 1960, Maury directed a project on decompilation mainly to show the usefulness of the Neliac language as a Universal Computer Oriented Language, as well as a problem-oriented language. Joel Donnelly and Herman Englander implemented the D-Neliac decompiler for the Univac M-460 Countess Computer while on Maury's staff. \"Each time Joel would bring an 'impossibility' to Maury, they would sit down and figure a way around it.\" As Herm noted, \"the difficult we do immediately -- the impossible takes a little longer.\" The D-Neliac decompiler was an operational decompiler that decompiled Univac M-460 binary code into Neliac source code. There was also a CDC 1604 version of D-Neliac.</p> <p>In the summer of 1962, Bill joined NEL and worked for Maury developing an earth atmosphere simulation program. After returning to NMSU for one more year of graduate studies, Bill returned to NEL in 1963. </p>"},{"location":"decompilers/father-of-dec/#the-ibm-7094-to-univac-1108-neliac-decompiler-1963-7-lockheed-missiles-and-space","title":"The IBM 7094 to Univac 1108 Neliac Decompiler (1963-7, Lockheed Missiles and Space)","text":"<p>With Maury's move to Lockheed in the fall of 1963, and with the introduction of newer and faster machines, the IBM 360 and the Univac 1108, Lockheed planned to move its scientific applications to the Univac 1108 and its business applications to the IBM 360. The conversion to 1108 was done via decompilation to Neliac of assembler programs and LIFT/SIFT of Fortran programs. The conversion to IBM 360 was by manual conversion and 360 emulation of IBM 7094/1401/1410 programs.</p> <p>\"The first version of the IBM 7094 to Neliac Decompiler was from IBM 7094 binary card images to IBM 7094 Neliac. Then, after the Univac 1107-8 compiler was bootstrapped from the IBM 7094 version, the decompiler was modified to translate to Univac 1108 Neliac. Since the resulting programs were running on a different system and instruction set, we did not have the luxury of inserting crutch code (i.e., machine code), even though Neliac allowed it. What we did do, however, was to modify the Neliac compiler, which we then called the Neliac \"recompiler\", to compile in ways to assist the correct operation of the decompiled code. One of these was the order of evaluation of arithmetic. That was one of the first tasks I accomplished after enhancing the Neliac compiler to include compilation of algebraic expressions and exponentiation. The original compiler allowed only very simple arithmetic. The recompiler used this simpler, and predictable, order of evaluation. This allowed such idioms as the conversion of integer to floating point to work as they had on the IBM 7094 (where the conversion was performed by a load of the integer, ORing the result with a floating point exponent, followed by a floating point add of zero). Of course, maintaining the order of arithmetic is sometimes important, and that was the primary emphasis of the recompiler.\"</p> <p>Original team members of the advanced software development group working for Maury on the Neliac compiler and decompiler projects included: Bill Caudle, Lambert Lui and Gordon Pelton on the decompiler; Bob Stelloh and Alan Howard on the compiler. Al Fuhrman developed the Neliac compressor/reformatter. Others were involved in the later stages of the group.</p> <p>The Lockheed decompiler was used in several demonstrations that included Phillips Petroleum, Ontario Hydro and the US Air Force. \"One of the more humorous decompilations I ever did was the decompilation of a Monte Carlo application that included a random number generator. The converted random number generator was required to produce the same sequence of random numbers, and the Monte Carlo application was required to generate exactly the same results on the same inputs.\"</p> <p>The Lockheed decompiler was used on a sizable project with Lockheed as the subcontractor to Univac. This contract was won after Bill put on a demonstration to the Air Force in Washington DC. Lambert Lui supported Bill in this effort, and Maury attended the actual demo (to assure our success, ashtrays were placed in the computer room for the Air Force Colonel. The team was warned not to ask the Colonel to extinguish his cigar.) The other vendors in the competition were IBM (using strictly conversion) and CDC (using code simulation).</p> <p>Starting in the fall of 1967 and completed in about May 1968, the US Air Force Global Weather Center's weather forecasting programs (125 programs written in assembler and FORTRAN) were converted to run on the Univac 1108 at the Offutt AFB in Omaha. Bill recalls having encountered a bug in one of the decompiled programs -- the largest assembler program in the set. This program was 24K lines of code and 6K lines of data (corresponding approximately to a 180K byte program). Using a set of input data picked by the Air Force as part of an acceptance test, the program generated by the decompiler would crash. After exhaustive manual checking of the decompiled code, a parallel run of the original program on the IBM 7094 was requested. The result? The parallel run showed that the original program running against the same test data would crash too -- the bug was in the original program, and was faithfully reproduced in the decompiled program.</p> <p>\"We were very successful in decompiling programs that had resulted from FORTRAN programs. These would run with little manual change. The IBM 7094 to Neliac Decompiler recognized FORTRAN calling sequences and replaced them with a more compact version. For pure assembler programs, the decompiler was estimated to remove 80-90% of the effort over manual conversion. At that time, we estimated the cost of conversion to be less than U$1.00/instruction decompiled. This was substantially less than the estimate at that time for manual conversion of assembler programs.\"</p>"},{"location":"decompilers/father-of-dec/#attempts-at-marketing-decompilation-services-1969-72","title":"Attempts at Marketing Decompilation Services (1969-72)","text":"<p>Bill and Maury were part of a company in Lafayette, IN, formed in 1968-69 called Teknatronic Applications, Inc. (TAI). Company members included three full-time staff. A group of professors (Maury Halstead, Jay Nunamaker, Samuel Conte and Andrew Winston all of Purdue University, Daniel Teichroew at University of Michigan and Glenn Graves of UCLA) were board members and acted as TAI consultants. As one of the business ventures at TAI, Maury had purchased a license from Lockheed to market the IBM 7094 to Neliac decompiler as a conversion service. Marketing was attempted through the Univac Information Systems Division (ISD) in Chicago. A demo at the Chicago ISD was made. (As an amusing aside, someone came in with a photocopy of 4 cards of binary code and asked Bill to decompile this code. The decompiled Neliac code showed that the cards were those of a simple numeric routine, whereas they had been thought to be a complicated system interface.) This brief attempt to market the decompilation service was a failure in that it produced no business for TAI.</p> <p>When the primary investors in TAI failed to produce sustaining capital for the TAI, it was abandoned. In 1970, the company staff and university associates established another company called Combinatorics (Also in Lafayette, IN). In 1973, Combinatorics was bought by Mathematica, Inc., of Princeton, NJ.</p> <p>Throughout this period, Maury supervised a PhD thesis on decompilation at Purdue University, that of Barry Housel (awarded 1973). At the same time, this was the period in which Maury began his formal investigation of the complexity of programs, resulting in what are now known as the \"Halstead Metrics\". </p>"},{"location":"decompilers/father-of-dec/#the-sperryunivac-494-to-1100-inverse-compiler-1972-6","title":"The Sperry*Univac 494 to 1100 Inverse Compiler (1972-6)","text":"<p>The success of the Global Weather project with Univac in 1968 resulted in a contract in 1972 between Combinatorics and Univac. Maury and Bill were contracted to lead the effort for the Univac 494 to Series 1100 decompiler, which Univac preferred to call \"Inverse Compiler\". Maury was a consultant to Bill on a one day/month basis. Bill was the technical leader of the project for the two years of the contract with Univac. During this contract, a feasibility study, functional specification, design and initial implementation of the decompiler were performed.</p> <p>The Sperry*Univac Inverse Compiler was made up of the following modules: input processing, basic block generation, local data usage analysis, control and data flow analysis, data mapping, reporting, intermediate form preparation, level transformations, and target language production. The model for decompilation is distinguished from prior models by the extensive use of analysis techniques in an attempt to derive data structures. The latter was in recognition of the fact that data conversion is of primary importance. Additional details on the analysis techniques are given in the previously unpublished paper On the Inverse of Compiling by Bill Caudle, 1980.</p> <p>The minimum input to the inverse compiler was the binary code in the absolute or relocatable load modules. Additional input in the form of the associated assembly and map listings for the program (produced by the ASM or SPURT assemblers) were optional; these listings assisted in the conversion process. The reporting module flagged missing libraries and parts of the program that would be hard to decompile. Using the input processor, the user could provide additional information to completely define the instruction blocks, data blocks, and subroutine definitions. Any information that the decompiler obtained by analysis could be modified or enhanced by the conversion specialist. However, no changes were allowed to be made to the binary programs, which remained unmodified. The enhanced input files were then input to the decompiler which produced COBOL code. The optimizer was optionally used to improve the readability of the generated COBOL code. The 1100 series sales memo from 1976 announces the 494 to 1100 inverse compiler.</p> <p>The following figures were reported in internal documents:</p> <ul> <li>Figure of merit: 67 to 87%. The figure of merit is the removed cost of resources required to successfully convert a program with the decompiler in ratio with the cost to convert the program manually.</li> <li>Code expansion: 1.79 (inhouse evaluation: 1.6).</li> <li>Translation cost: $0.94 per line of assembler.</li> <li>Decompiler cost: the cost of a decompiler with a high figure of merit was similar to the cost of a modern high-level language compiler.</li> <li>Computer cost: the above per line costs included computer cost at approximately U$160/hour.</li> </ul> <p>Subsequent to the 2 year contract, Bill went to work with Sperry*Univac in Roseville, MN, and continued on the decompilation project for another 2 years. About 10 people worked on the decompilation project in Roseville, with as many at times in Salt Lake City, UT. Work on decompilation in Roseville was in the area of loop analysis, interval analysis, global data analysis, and dictionary techniques. The Salt Lake City group provided input processing, conversion to internal forms, level transformations, and conversion to target language. Effectively, this separated the project into that related to decompiler techniques and that more directly related to compiler techniques.</p> <p>In 1975, there was a version of the decompiler that generated PLUS (a PL/1-like language for systems programming) code. Univac did a benchmark test on decompilation in Zurich using the PLUS version. This was the first conversion done outside their own environment. Bill led the team in Zurich that translated 8 programs of 5600 lines of assembly code. These programs were picked from the most difficult type of programs to be converted and resulted in many improvements in the process. Part of the results of this field test were the recommendation to use COBOL as the target language.</p> <p>As a result of this field effort, in late 1975/76 a version of the inverse compiler was developed that generated COBOL code. Both PLUS and COBOL target languages were generated from the same internal representation used in the decompiler. </p>"},{"location":"decompilers/tools/","title":"Tools","text":"<p>The community continues to extend decompilers outside of fundamental improvements in the form of plugins and tools. Here you can find a listing of tools and plugins used by the community. </p>"},{"location":"decompilers/tools/#generic-tools","title":"Generic Tools","text":"<p>Tools that work in most popular decompilers.</p> <ul> <li>BinDiff: A decompiler-based diffing tool for binaries.</li> <li>BinSync: A Git-based collaboration framework for decompilers. Supports IDA, Ghidra, Binja, and angr decompiler.</li> <li>DogBolt: A web-based tool for comparing popular decompiler's decompilation.</li> <li>RevSync: A synchronization tool for decompilers. Supports IDA &amp; Binja. </li> </ul>"},{"location":"decompilers/decompiler/angr/","title":"angr decompiler","text":""},{"location":"decompilers/decompiler/binary_ninja/","title":"Binary Ninja","text":""},{"location":"decompilers/decompiler/ghidra/","title":"Ghidra","text":""},{"location":"decompilers/decompiler/ida_pro/","title":"IDA Pro (HexRays)","text":""},{"location":"fundamentals/evaluation/","title":"Quality Evaluation","text":""},{"location":"fundamentals/evaluation/#introduction","title":"Introduction","text":"<p>The best way to measure the quality of overall decompilation is still an open problem in decompilation research. However, there have been a variety of methods for evaluating the quality of individual fundamental components in decompilation.</p>"},{"location":"fundamentals/evaluation/#control-flow-structuring-metrics","title":"Control Flow Structuring Metrics","text":"<p>Early evaluations in control flow structuring utilized metrics from software engineering.  As such, these metrics were not compared to their original source, but instead used standalone.  This was useful because these metrics could be computed in real-world scenarios of decompilation use and reduced in runtime.  The following early metrics were used:</p> <ul> <li>Gotos<sup>1</sup><sup>2</sup><sup>3</sup><sup>4</sup><sup>5</sup></li> <li>Full-function Recompilability<sup>2</sup></li> <li>Lines of Code (LoC)<sup>4</sup></li> <li>McCabe Cyclomatic Complexity (MCC)<sup>5</sup></li> </ul> <p>In all but the case of function recompilability, the decompiler would want to optimize for reducing all of these metrics. However, recent work has argued against the pure reduction of these metrics<sup>5</sup>.  The SAILR paper<sup>5</sup>, argues that these metrics should be measured relative to its source (like machine translation). The following metrics were used as compared to source:</p> <ul> <li>Gotos</li> <li>Boolean Expression</li> <li>Function Calls</li> <li>Graph Edit Distance (GED)</li> </ul> <p>In each metric, the score closest to the source is the best structured decompilation.  Since these metrics can only be used with knowledge of source, they can only be used when developing a decompiler (not optimized in runtime).</p>"},{"location":"fundamentals/evaluation/#variable-typing-metrics","title":"Variable Typing Metrics","text":"<p>Unlike control flow structuring metrics, most evaluations for variable typing have focused on comparing choices to the source<sup>6</sup><sup>7</sup><sup>8</sup><sup>9</sup>. Generally, these evaluations would flow like the following:</p> <ol> <li>Extract the ground-truth types from the source (usually with DWARF info)</li> <li>Guess variables and types from decompilation</li> <li>See how often (or how close) each chosen variable location and type was to source</li> </ol> <p>Some evaluations have also measured how conservative they were at approximating types<sup>7</sup>.</p>"},{"location":"fundamentals/evaluation/#human-evaluation","title":"Human Evaluation","text":"<p>Some work has utilized human evaluations to understand the quality of decompilation<sup>10</sup><sup>11</sup>.  These works have focused on qualitative metrics, such as users' perception of the code's complexity<sup>10</sup>.  Additionally, the Decomperson<sup>12</sup> paper studied how humans might make decompilation themselves when given assembly.</p> <p>Since how humans rate decompilation is related to how they reverse engineer, some work has explored how humans reverse and exploit binaries. These studies have mainly focused on how hackers use tooling and techniques for reverse engineering<sup>15</sup><sup>16</sup>. </p>"},{"location":"fundamentals/evaluation/#other-general-metrics","title":"Other General Metrics","text":"<p>Some work has constructed a taxonomy of decompiler-to-source errors<sup>13</sup>. That work categorized and identified many errors that occur when decompiling a binary. </p> <p>Other work has identified general methods by which we may test the correctness of decompilation<sup>14</sup>. Relatedly, some structuring work has attempted to measure this through recompiliability<sup>2</sup>. </p>"},{"location":"fundamentals/evaluation/#dataset-generation","title":"Dataset Generation","text":"<p>The question of \"what dataset?\" to use has also been an issue in decompilation. Additionally, compiled binaries can sometimes be hard to access. Some related work has explored ways to generate bigger binary datasets for the evaluation of binary tools like decompilers<sup>17</sup>.</p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Noonan, Matt, Alexey Loginov, and David Cok. \"Polymorphic type inference for machine code.\" Proceedings of the 37<sup>th</sup> ACM SIGPLAN Conference on Programming Language Design and Implementation. 2016.\u00a0\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011)\u00a0\u21a9\u21a9</p> </li> <li> <p>Zhang, Zhuo, et al. \"Osprey: Recovery of variable and data structure via probabilistic analysis for stripped binary.\" 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Chen, Qibin, et al. \"Augmenting decompiler output with learned variable names and types.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"Helping johnny to analyze malware: A usability-optimized decompiler and malware analysis user study.\" 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Enders, Steffen, et al. \"dewolf: Improving Decompilation by leveraging User Surveys.\" arXiv preprint arXiv:2205.06719 (2022).\u00a0\u21a9</p> </li> <li> <p>Decomperson: How Humans Decompile and What We Can Learn From It\u00a0\u21a9</p> </li> <li> <p>Dramko, Luke, et al. \"A Taxonomy of C Decompiler Fidelity Issues.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9</p> </li> <li> <p>How far we have come: Testing decompilation correctness of C decompilers\u00a0\u21a9</p> </li> <li> <p>Nosco, Timothy, et al. \"The industrial age of hacking.\" 29<sup>th</sup> USENIX Security Symposium (USENIX Security 20). 2020.\u00a0\u21a9</p> </li> <li> <p>Mantovani, Alessandro, et al. \"{RE-Mind}: a First Look Inside the Mind of a Reverse Engineer.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Singhal, Vidush, et al. \"Cornucopia: A Framework for Feedback Guided Generation of Binaries.\" Proceedings of the 37<sup>th</sup> IEEE/ACM International Conference on Automated Software Engineering. 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/neural-decompilation/","title":"Neural Decompilation","text":""},{"location":"fundamentals/neural-decompilation/#introduction","title":"Introduction","text":"<p>This wiki defines a neural decompiler as the following:</p> <ul> <li>Any decompiler that wholly replaces one or many fundamental decompiler components with a machine-learning model.</li> </ul> <p>As such, decompilation produced by a neural decompiler is neural decompilation. Approaches in this area are promising because they often offer a more generic solution to decompilation.  Instead of a decompiler developer having to program a component, the component is learned from binaries and their corresponding source<sup>1</sup>. </p>"},{"location":"fundamentals/neural-decompilation/#previous-works","title":"Previous Works","text":"<p>The earliest academic work in this area is the 2018 work \"Using Recurrent Neural Networks for Decompilation\"<sup>1</sup>.  The work utilized a Recurrent Neural Network (RNN) to replace all components downstream from CFG recovery, omitting the lifting phase to train on x86 assembly.  Subsequent work also focused on x86, while changing their method for decompilation to Neural Machine Translation<sup>2</sup><sup>3</sup><sup>4</sup><sup>5</sup><sup>7</sup> and a more refined RNN<sup>6</sup>. The most reliable of these works, Coda, claims an average of \"82% program recovery accuracy on unseen binary sample(s)\"<sup>6</sup>. The notable Beyond-The-C, also introduces early work to turn binary code into multiple languages other than C <sup>7</sup>.</p> <p>With the recent popularization of Large Language Models (LLM), like ChatGPT, early work has utilized LLMs for end-to-end decompilation<sup>8</sup>.  Other related work in the area has used LLMs to improve the structured output of other decompilers<sup>9</sup>. </p> <ol> <li> <p>Katz, Deborah S., Jason Ruchti, and Eric Schulte. \"Using recurrent neural networks for decompilation.\" 2018 IEEE 25<sup>th</sup> International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2018.\u00a0\u21a9\u21a9</p> </li> <li> <p>Katz, Omer, et al. \"Towards neural decompilation.\" arXiv preprint arXiv:1905.08325 (2019).\u00a0\u21a9</p> </li> <li> <p>Liang, Ruigang, et al. \"Neutron: an attention-based neural decompiler.\" Cybersecurity 4 (2021): 1-13.\u00a0\u21a9</p> </li> <li> <p>Liang, Ruigang, et al. \"Semantics-recovering decompilation through neural machine translation.\" arXiv preprint arXiv:2112.15491 (2021).\u00a0\u21a9</p> </li> <li> <p>Cao, Ying, et al. \"Boosting neural networks to decompile optimized binaries.\" Proceedings of the 38<sup>th</sup> Annual Computer Security Applications Conference. 2022.\u00a0\u21a9</p> </li> <li> <p>Fu, Cheng, et al. \"Coda: An end-to-end neural program decompiler.\" Advances in Neural Information Processing Systems 32 (2019).\u00a0\u21a9\u21a9</p> </li> <li> <p>Hosseini, Iman, and Brendan Dolan-Gavitt. \"Beyond the C: Retargetable decompilation using neural machine translation.\" arXiv preprint arXiv:2212.08950 (2022).\u00a0\u21a9\u21a9</p> </li> <li> <p>Tan, Hanzhuo, et al. \"LLM4Decompile: Decompiling Binary Code with Large Language Models.\" arXiv preprint arXiv:2403.05286 (2024).\u00a0\u21a9</p> </li> <li> <p>Hu, Peiwei, Ruigang Liang, and Kai Chen. \"DeGPT: Optimizing Decompiler Output with LLM.\"\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/overview/","title":"Overview","text":""},{"location":"fundamentals/overview/#introduction","title":"Introduction","text":"<p>Modern decompilation is built on many techniques from both binary analysis and classic compilation algorithms.  Fundamental research in this area focuses on improving base decompilation across all languages.</p> <p>Some academic work has defined fundamental decompilation research to include three areas<sup>1</sup>, this wiki includes an extra fourth one:</p> <ol> <li>Control Flow Graph Recovery: the extraction of (lifted) directed graphs indicating code execution</li> <li>Type Recovery: the typing and discovery of variables in the program</li> <li>Control Flow Structuring: the conversion of a CFG to a linear code-like output</li> <li>Quality Evaluation: the measurement of overall decompilation quality</li> </ol> <p>Each of these fundamental areas affects the quality of one another in some way. For instance, improvements to type recovery can directly improve the results of control flow structuring<sup>4</sup>. With this in mind, we included the fourth area, quality evaluation in decompilation, as fundamental. We include this area because it influences the methodologies for the previous three areas and few works have standardized on metrics<sup>1</sup><sup>2</sup><sup>3</sup>.</p> <p>Other works, such as function name recovery can be found in the Applied Research section.</p>"},{"location":"fundamentals/overview/#generic-decompilation-pipeline","title":"Generic Decompilation Pipeline","text":"<p>Modern decompilers are comprised of fundamental components that can be directly mapped to each fundamental research area. Most decompilers follow a pipeline flow that resembles the following<sup>3</sup>:</p> <p></p> <p>The evaluation component is optional but has been used in the past for on-the-fly decision-making in other components like control flow structuring<sup>5</sup>. Each component can be implemented at various levels, such as the optional lifting phase in control flow recovery. </p> <p>In the case of neural decompilation, most components are replaced by the machine learning model. </p> <ol> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy SAILR! There is no need to DREAM of C: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"Helping johnny to analyze malware: A usability-optimized decompiler and malware analysis user study.\" 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016.\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011).\u00a0\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/disassembly/","title":"Disassembling","text":""},{"location":"fundamentals/cfg-recovery/disassembly/#introduction","title":"Introduction","text":"<p>Work in this area dates back to the 90s and can generally be broken up into two disassemblying techniques<sup>1</sup>:</p> <ol> <li>Linear Sweep<sup>2</sup>: starts disassembling from the first byte in the program and linearly continues</li> <li>Recursive Traversal<sup>3</sup>: disassembles by following control flow as it is discovered</li> </ol> <p>Linear sweep is considered to be more prone to errors in programs that embed data in the middle of their code[1].  Additionally, all disassembling techniques confront the main problem of Instruction boundary identification (IBI)<sup>4</sup>, which dictates when an instruction starts and ends.</p>"},{"location":"fundamentals/cfg-recovery/disassembly/#previous-works","title":"Previous Works","text":"<p>Relevant work for decompilation focused on improving both the accuracy and scalability of disassembling programs<sup>2</sup><sup>6</sup>.  Some of these works have improved these methods for work on deobfuscated binaries<sup>3</sup> as well as benign ones. Recent work in the area has focused on reassemblable disassembly<sup>4</sup><sup>5</sup>. This area is promising for decompilation as it makes re-compilable decompilation more achievable <sup>5</sup>. </p> <p>There has also been work to measure the accuracy of these disassembling frameworks by comparing their results to that of an instrumented compiler<sup>7</sup><sup>8</sup>. </p> <ol> <li> <p>Kruegel, Christopher, et al. \"Static disassembly of obfuscated binaries.\" USENIX security Symposium. Vol. 13. 2004.\u00a0\u21a9</p> </li> <li> <p>Free Software Foundation. GNU Binary Utilities, Mar 2002. https://www.gnu.org/software/binutils/manual/.\u00a0\u21a9\u21a9</p> </li> <li> <p>C. Cifuentes and K. Gough. Decompilation of Binary Programs. Software Practice &amp; Experience, 25(7):811-829, July 1995.\u00a0\u21a9\u21a9</p> </li> <li> <p>Flores-Montoya, Antonio, and Eric Schulte. \"Datalog disassembly.\" 29<sup>th</sup> USENIX Security Symposium (USENIX Security 20). 2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi, Aravind Machiry, John Grosen, Paul Grosen, Christopher Kruegel, and Giovanni Vigna. Ramblr: Making reassembly great again. In NDSS, 2017\u00a0\u21a9\u21a9</p> </li> <li> <p>Andriesse, Dennis, et al. \"An {In-Depth} Analysis of Disassembly on {Full-Scale} x86/x64 Binaries.\" 25<sup>th</sup> USENIX security symposium (USENIX security 16). 2016.\u00a0\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Ground truth for binary disassembly is not easy.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Sok: All you ever wanted to know about x86/x64 binary disassembly but were afraid to ask.\" 2021 IEEE symposium on security and privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/function-recovery/","title":"Function Identification","text":"<p>Function identification is used to identify the instruction boundaries of a function in the binary. For decompilation purposes, this serves as crucial information for the flow of code in the program.</p> <p>There have generally been two approaches:</p> <ol> <li>Pattern based (with weights)<sup>1</sup><sup>2</sup></li> <li>Machine-learning based<sup>3</sup></li> </ol> <p>Modern tools often utilize pattern-based techniques. </p> <ol> <li> <p>Bao, Tiffany, et al. \"{BYTEWEIGHT}: Learning to recognize functions in binary code.\" 23<sup>rd</sup> USENIX Security Symposium (USENIX Security 14). 2014.\u00a0\u21a9</p> </li> <li> <p>Andriesse, Dennis, Asia Slowinska, and Herbert Bos. \"Compiler-agnostic function detection in binaries.\" 2017 IEEE European symposium on security and privacy (EuroS&amp;P). IEEE, 2017.\u00a0\u21a9</p> </li> <li> <p>Shin, Eui Chul Richard, Dawn Song, and Reza Moazzezi. \"Recognizing functions in binaries with neural networks.\" 24<sup>th</sup> USENIX security symposium (USENIX Security 15). 2015.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/jump-resolving/","title":"Jump Resolving","text":"<p>After recovering a CFG from a binary, there may be calls that have an unknown target.  Indirect jump resolving is the process of discovering some, or all, of the targets those jumps may go to.  A basic implementation of this is Switch statements resolving, since they are often compiled into indirect jumps <sup>1</sup>.</p> <p>The work in this area has mostly focused on ways to reduce the total set of analyzed pointers while doing pointer analysis<sup>2</sup><sup>3</sup>. Additionally, these works have used program knowledge to further reduce constant-reducible pointers, like those found in class initialization in C++<sup>2</sup>. </p>"},{"location":"fundamentals/cfg-recovery/jump-resolving/#indirect-jump-example","title":"Indirect Jump Example","text":"<p>A simple indirect jump looks like the following in x86: <pre><code>jmp [rax];\n</code></pre></p> <p>Since pointer analysis is considered hard, it may not be trivial to find the value of <code>rax</code>.</p> <ol> <li> <p>Brumley, David, et al. \"Native x86 decompilation using {Semantics-Preserving} structural analysis and iterative {Control-Flow} structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9</p> </li> <li> <p>Kim, Sun Hyoung, et al. \"Refining Indirect Call Targets at the Binary Level.\" NDSS. 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kim, Sun Hyoung, et al. \"Binpointer: towards precise, sound, and scalable binary-level pointer analysis.\" Proceedings of the 31<sup>st</sup> ACM SIGPLAN International Conference on Compiler Construction. 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/lifting/","title":"Program Lifting","text":""},{"location":"fundamentals/cfg-recovery/lifting/#introduction","title":"Introduction","text":"<p>In program lifting, disassembly is converted into an intermediate language (IL)<sup>1</sup>, which is also referred to as an intermediate representation (IR). Converting disassembly to an IL allows decompiler developers to make optimizations on the IL level which apply to multiple architectures. </p> <p>There exist many ILs for analysis of programs, but some of the most notable ones for decompilation are tied to binary analysis. Most binary analysis platforms that have created or used an IL often follow the same techniques but mostly differ in their later use of ILs<sup>1</sup><sup>2</sup><sup>3</sup><sup>4</sup>.  One such use is recompilable decompilation, which can be made easier by lifting to compiled ILs like LLVM-IR<sup>5</sup><sup>6</sup>. Another use-case has been the verification of decompilation correctness<sup>8</sup>.</p> <p>Similar to static analysis, most ILs used in decompilation support some form of static single assignment (SSA) since it simplifies some analyses<sup>7</sup>.</p>"},{"location":"fundamentals/cfg-recovery/lifting/#example-lifted-program","title":"Example Lifted Program","text":"<p>Below is some example x86 assembly of a simple C program: <pre><code>0000000000001129 &lt;main&gt;:\n    1129:   f3 0f 1e fa             endbr64\n    112d:   55                      push   rbp\n    112e:   48 89 e5                mov    rbp,rsp\n    1131:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi\n    1134:   83 7d ec 02             cmp    DWORD PTR [rbp-0x14],0x2\n    1138:   7e 09                   jle    1143 &lt;main+0x1a&gt;\n    113a:   c7 45 fc 00 00 00 00    mov    DWORD PTR [rbp-0x4],0x0\n    1141:   eb 07                   jmp    114a &lt;main+0x21&gt;\n    1143:   c7 45 fc 01 00 00 00    mov    DWORD PTR [rbp-0x4],0x1\n    114a:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]\n    114d:   5d                      pop    rbp\n    114e:   c3                      ret \n</code></pre></p> <p>It can be lifted to an IL like VEX, the IL used in the angr decompiler: <pre><code>00 | ------ IMark(0x401129, 4, 0) ------\n01 | PUT(rip) = 0x000000000040112d\n02 | ------ IMark(0x40112d, 1, 0) ------\n03 | t0 = GET:I64(rbp)\n04 | t8 = GET:I64(rsp)\n05 | t7 = Sub64(t8,0x0000000000000008)\n06 | PUT(rsp) = t7\n07 | STle(t7) = t0\n08 | ------ IMark(0x40112e, 3, 0) ------\n09 | PUT(rbp) = t7\n10 | PUT(rip) = 0x0000000000401131\n11 | ------ IMark(0x401131, 3, 0) ------\n12 | t10 = Add64(t7,0xffffffffffffffec)\n13 | t13 = GET:I64(rdi)\n14 | t25 = 64to32(t13)\n15 | t12 = t25\n16 | STle(t10) = t12\n17 | PUT(rip) = 0x0000000000401134\n18 | ------ IMark(0x401134, 4, 0) ------\n19 | t14 = Add64(t7,0xffffffffffffffec)\n20 | t5 = LDle:I32(t14)\n21 | PUT(cc_op) = 0x0000000000000007\n22 | t26 = 32Uto64(t5)\n23 | t16 = t26\n24 | PUT(cc_dep1) = t16\n25 | PUT(cc_dep2) = 0x0000000000000002\n26 | PUT(rip) = 0x0000000000401138\n27 | ------ IMark(0x401138, 2, 0) ------\n28 | t29 = 64to32(t16)\n29 | t30 = 64to32(0x0000000000000002)\n30 | t28 = CmpLE32S(t29,t30)\n31 | t27 = 1Uto64(t28)\n32 | t23 = t27\n33 | t31 = 64to1(t23)\n34 | t18 = t31\n35 | if (t18) { PUT(rip) = 0x401143; Ijk_Boring }\nNEXT: PUT(rip) = 0x000000000040113a; Ijk_Boring\n...\n</code></pre></p> <p>In the case of VEX, each <code>t</code> variable is only ever assigned once, making this SSA form.  You will also notice how verbose every instruction has become.  Even simple <code>mov</code> instructions, which assign a value to a register, have much more information now.  The lifted VEX above has been cut for brevity. </p>"},{"location":"fundamentals/cfg-recovery/lifting/#intermediate-languages","title":"Intermediate Languages","text":"Name Project Notes BIL BAP Includes another IL called Core Theory BNIL Binary Ninja Includes 7 different ILs internally such as Low Level IL, Medium Level, SSA forms, and others Boogie Boogie GitHub Repository Cas Amoco Chunk IR Egalito DBA BINSEC BINSEC, GitHub Repository ESIL Radare Documentation Falcon IL Falcon FalkerIL Falker* Not well-documented; created because existing ILs didn't meet specific purposes GDSL GDSL GTIRB GTIRB GitHub Repository JEB IR JEB LowUIR B2R2 Miasm IR Miasm Blog Post Microcode Hex-Rays Hex Blog; not the same as Insight Microcode! Microcode Insight Website; not the same as Hex-Rays Microcode! P-Code GHIDRA Sleigh Documentation RDIL RedASM REIL BinNavi RREIL Bindead Snowman IR Snowman GitHub Repository SSL Jakstab TSL CodeSonar Unnamed EiNSTeiN- VEX Valgrind Angr Docs, FOSDEM Slides Vine BitBlaze Based on VEX VTIL VTIL Designed for binary translation; initially used for defeating VMProtect"},{"location":"fundamentals/cfg-recovery/lifting/#llvm-ir-based-lifting","title":"LLVM IR-based Lifting","text":"<p>One of the most popular and widely known IRs for program analysis is LLVM IR <sup>9</sup>, which is used in LLVM-based projects like the Clang compiler. With this in mind, many decompiler/binary analysis platforms have attempted to adopt LLVM IR as their main IR for analysis in the past.  However, it is still unknown how effective these approaches are since they have not achieved wide-scale adoption in either industry or academia. </p> Name Project Notes allin allin bin2llvm S2E Dagger Dagger fcd fcd Fracture Fracture libbeauty libbeauty Has been removed from GitHub, reference URL linked to instead mctoll mctoll Rellume binopt remill McSema reopt reopt RetDec RetDec revng revng First lifted to QEMU TCG, then LLVM IR <ol> <li> <p>Song, Dawn, et al. \"BitBlaze: A new approach to computer security via binary analysis.\" Information Systems Security: 4<sup>th</sup> International Conference, ICISS 2008, Hyderabad, India, December 16-20, 2008. Proceedings 4. Springer Berlin Heidelberg, 2008.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kinder, Johannes, and Helmut Veith. \"Jakstab: a static analysis platform for binaries: tool paper.\" Computer Aided Verification: 20<sup>th</sup> International Conference, CAV 2008 Princeton, NJ, USA, July 7-14, 2008 Proceedings 20. Springer Berlin Heidelberg, 2008.\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"BAP: A binary analysis platform.\" Computer Aided Verification: 23<sup>rd</sup> International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23. Springer Berlin Heidelberg, 2011.\u00a0\u21a9</p> </li> <li> <p>Wang, Fish, and Yan Shoshitaishvili. \"Angr-the next generation of binary analysis.\" 2017 IEEE Cybersecurity Development (SecDev). IEEE, 2017.\u00a0\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9</p> </li> <li> <p>Revng. \u201cRevng/Revng: Revng: The Core Repository of the Rev.Ng Project.\u201d GitHub, github.com/revng/revng. Accessed 27 Apr. 2024.\u00a0\u21a9</p> </li> <li> <p>Van Emmerik, Michael James. Static single assignment for decompilation. University of Queensland, 2007.\u00a0\u21a9</p> </li> <li> <p>Engel, Daniel, Freek Verbeek, and Binoy Ravindran. \"BIRD: A Binary Intermediate Representation for Formally Verified Decompilation of X86-64 Binaries.\" International Conference on Tests and Proofs. Cham: Springer Nature Switzerland, 2023.\u00a0\u21a9</p> </li> <li> <p>https://llvm.org/docs/LangRef.html \u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/overview/","title":"Overview","text":""},{"location":"fundamentals/cfg-recovery/overview/#introduction","title":"Introduction","text":"<p>A control flow graph (CFG) is a graph that describes the \"flow\" of execution in a program<sup>1</sup>.  As such, CFG recovery, in binary analysis, is the recovery of such a graph from binary code.  Since decompilation directly relies on this CFG, the recovery of it is considered fundamental to decompilation. </p> <p>This recovery can be broken up into multiple phases, some being optional depending on the decompilation target:</p> <ol> <li>Disassembling: the conversion of binary code to its mnemonic instructions and operands </li> <li>Program Lifting: the conversion disassembly to an intermediate language (IL) for better abstraction </li> <li>Function Recognition: the discovery of boundaries defining a function</li> <li>Indirect Jump Resolving: the resolution of jumps that have no constant target(s). </li> </ol> <p>The second phase, program lifting, is only required if the decompiler aims to be architecture agnostic. Most decompilers will use an IL to make their later analyses more widely applicable. </p> <p>Methods for evaluating improvements in this field are also of note but have had very limited work. The most recent of these works has focused on instrumenting and comparing to the compile-generated CFG<sup>2</sup><sup>3</sup>. </p> <p>There has been little work in replacing CFG recovery algorithms with a machine-learning model<sup>4</sup>. Related work in binary analysis has looked at how these recovered CFGs may be instrumentable<sup>5</sup>.</p>"},{"location":"fundamentals/cfg-recovery/overview/#graph-recovery-example","title":"Graph Recovery Example","text":"<p>An example C program is shown below: <pre><code>int main(int argc) {\n    int ret_code;\n    if(argc &gt; 2) {\n        ret_code = 0;\n    }\n    else {\n        ret_code = 1;\n    }\n    return ret_code;\n}\n</code></pre></p> <p>It is compiled on a x86-64 Linux machine with GCC, without optimizations: <pre><code>gcc example.c -o example &amp;&amp; objdump -D -M intel example | grep \"&lt;main&gt;:\" -A 12\n</code></pre></p> <p>After disassembling with objdump, which includes some function identification, we get the following disassembly: <pre><code>0000000000001129 &lt;main&gt;:\n    1129:   f3 0f 1e fa             endbr64\n    112d:   55                      push   rbp\n    112e:   48 89 e5                mov    rbp,rsp\n    1131:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi\n    1134:   83 7d ec 02             cmp    DWORD PTR [rbp-0x14],0x2\n    1138:   7e 09                   jle    1143 &lt;main+0x1a&gt;\n    113a:   c7 45 fc 00 00 00 00    mov    DWORD PTR [rbp-0x4],0x0\n    1141:   eb 07                   jmp    114a &lt;main+0x21&gt;\n    1143:   c7 45 fc 01 00 00 00    mov    DWORD PTR [rbp-0x4],0x1\n    114a:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]\n    114d:   5d                      pop    rbp\n    114e:   c3                      ret\n</code></pre></p> <p>In this example, the instructions at <code>0x1138</code> and <code>0x114a</code> cause the control flow to branch, resulting in the recovered graph:</p> <p></p> <p>Notice the implied edges coming from the assembly that looked linear (<code>0x114a</code>). The structure of the graph will also look the same if lifted to an IL.</p> <ol> <li> <p>Allen, Frances E. \"Control flow analysis.\" ACM Sigplan Notices 5.7 (1970): 1-19.\u00a0\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Ground truth for binary disassembly is not easy.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Sok: All you ever wanted to know about x86/x64 binary disassembly but were afraid to ask.\" 2021 IEEE symposium on security and privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Yu, Shih-Yuan, et al. \"Cfg2vec: Hierarchical graph neural network for cross-architectural software reverse engineering.\" 2023 IEEE/ACM 45<sup>th</sup> International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 2023.\u00a0\u21a9</p> </li> <li> <p>Di Bartolomeo, Luca, Hossein Moghaddas, and Mathias Payer. \"{ARMore}: Pushing Love Back Into Binaries.\" 32<sup>nd</sup> USENIX Security Symposium (USENIX Security 23). 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/gotoless/","title":"Gotoless Structuring","text":""},{"location":"fundamentals/structuring/gotoless/#introduction","title":"Introduction","text":"<p>Gotoless structuring is a type of structuring that ignores some compiler patterns to make code that contains no gotos<sup>1</sup>. According to this work, these gotos are signs of unstructured code which is bad for readability<sup>1</sup><sup>2</sup>. </p> <p>The most famous of these works, and the founder of the area, is the DREAM decompiler<sup>1</sup>. The DREAM decompiler used reaching conditions on statements to condense and reduce decompilation. </p> <p>The followup to this work, which is a hybrid of gotoless and schema-based methods, is the rev.ng<sup>2</sup> decompiler.  They used a method called \"Combing\", which duplicated nodes in the original graph to get rid of unstructured regions. </p> <ol> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/loop-reduction/","title":"Loop Reduction","text":""},{"location":"fundamentals/structuring/loop-reduction/#introduction","title":"Introduction","text":"<p>Loop reduction is a technique performed on CFGs in order to identify the basic blocks that are contained within a loop. Decompilers inherited this from compilers, where the technique is used to enable optimisations such as loop unrolling. There are several challenges involved in the identification of loops that make this a very interesting topic.</p> <p>Surprisingly, there are quite a few different algorithms used in decompilers for the identification of loops:</p> <ol> <li>Interval Analysis<sup>1</sup>.</li> <li>Dominator-Based<sup>2</sup>.</li> <li>Havlak-Tarjan<sup>3</sup>.</li> </ol>"},{"location":"fundamentals/structuring/loop-reduction/#interval-analysis","title":"Interval Analysis","text":"<p>This method was introduced to decompilation by Cristina Cifuentes<sup>4</sup>. It served an important step in the ability to identify the order of nested loops. It works in the following way:</p> <ol> <li> <p>Identify the set of intervals (maximal single entry subgraphs) in a graph.     <ol> <li>A node is added to an interval if all its immediate predecessors are in the interval.</li> <li>A new interval is created if the previous condition does not hold for a node.</li> </ol></p> </li> <li> <p>Collapse the nodes into a new graph.</p> </li> <li> <p>Repeat until you have one node. Once this condition is met you will have a derived sequence of graphs<sup>5</sup>.</p> <p></p> </li> </ol> <p>Now the loops have been collected in their relative nesting. To compute the loops in a graph you first constrain yourself to one interval at a time and ignore external edges. In the case of the interval containing 5, 6 and 7 we ignore the edge 7 -&gt; 4 for it to be dealt with in the interval containing 4 and 5-7.</p> <p>Each interval head is potentially the head of a loop, and can be queried by determining if there is a back-edge to the interval head. Once a loop head is found a path of nodes within the loop from head to the node where the back-edge is found must be followed. I believe the details for this are conveyed well in Cristina's thesis<sup>4</sup>, therefore they have been omitted from this article.</p> <p>Overall this technique is good, however it is unable to deal with irreducible graphs properly<sup>7</sup> an important quality of better algorithms. An irreducible graph is one where it forms a shape of the following figure, our goal with the following algorithms is to deal with them in such a way that they do not inhibit our ability to detect reducible loops.</p> <p></p>"},{"location":"fundamentals/structuring/loop-reduction/#dominator-based-loop-reduction","title":"Dominator-Based Loop Reduction","text":"<p>A node n is dominated by another node d if all paths to n contain d. We therefore call d a dominator. To utilise this definition to find loops we will cover the Sreedhar-Gao-Lee (SGL) algorithm<sup>2</sup>.</p> <p>The dominators found in a control flow graph can be simply represented in what is called a dominator tree. This is constructed by setting a parent's child nodes to those which it immediately dominates. In the following figure the CFG is represented on the left with its corresponding dominator tree on the right.</p> <p></p> <p>In the SGL algorithm they combine both the CFG and the dominator tree into one graph named a DJ graph. The DJ graph is named due to each node having a set of edges for the nodes that it immediately dominates 'D' and for the edges in the flow graph 'J'. We can represent our previous diagram in such a manner using red lines for 'D' edges and black lines for 'J' edges.</p> <p></p> <p>Such a representation is rare in practice as it adds complexity, therefore my recommendation is to not be funky and keep the CFG and dominator tree separate.</p> <p>The following steps outline the process of this algorithm:</p> <ol> <li> <p>Perform a depth-first search and identify back-edges.     <ol> <li>A back edge occurs if a node w receives an edge from a node v, where v is considered an ancestor of w.</li> <li>An ancestor can be identified if 2 properties hold: the preorder number of w is less than or equal to v, and the preorder number of the last descendent of w is greater than or equal to that of v i.e. there exists a path from w to v.</li> </ol></p> </li> <li> <p>Traverse the graph from the deepest level up to the top. Visit all nodes that have that corresponding level in the CFG.     <ol> <li>Check the predecessors of that node, and if one or more back-edges are dominated by the node then we can classify this node as the head of a reducible loop.</li> <li>If however we find a back-edge which the node does not dominate we classify this node as the head of an irreducible loop.</li> </ol></p> </li> <li> <p>In the case of a irreducible loop we mark it and deal with it in step 5.</p> </li> <li> <p>In the case of a reducible loop we start at the node where the back-edge is coming from, adding nodes to the loop body until we reach a node which has already been added to the loop body.     <ol> <li>If a node has already been assigned to a loop/s, it's least nested loop header is assigned to the loop body instead. The Union-Find data structure can be used for this purpose (covered in the next section).</li> <li>Once all nodes have been added to the loop, they are collapsed.</li> </ol></p> </li> <li> <p>Before we move to the next level, if we have identified any irreducible loops we take the subgraph containing all the nodes at our current level and greater*. We calculate any strongly connected components and reduce them into one node<sup>6</sup>.     <ol> <li>A strongly connected component is a subgraph where every node is reachable from every other node.</li> </ol></p> </li> </ol> <p>* The subgraph I'm referring to is the one where we have already collapsed the reducible loops to one node.</p>"},{"location":"fundamentals/structuring/loop-reduction/#havlak-tarjan","title":"Havlak-Tarjan","text":"<p>In 1997 Havlak presented another way<sup>3</sup>, this was a variation on Tarjan's interval analysis that enabled the collapsing of irreducible loops. Similar to the previous algorithm we will use a Union-Find data structure. </p> <p>A Union-Find can be employed on disjoint sets i.e. sets with no elements in common. Union combines two sets and Find gets the set of an element. This can be done by using a tree structure and instantiating each node as its own set.</p> <p>For our purposes we would use union to add another loop/node to an outer loop. Find would be used to get the outer most loop header. In the following visualisation we present the union operation on the 2 loops (4, (5, 6), 7) from the first figure.</p> <p></p> <p>On the left if we call Find(6) it returns 5, on the right calling Find(6) returns 4. This satisfies our need to obtain the outermost loop header of a node. A further optimisation can be employed on this, by changing 6's parent to be 4 just before we return from Find.</p> <p>The following steps outline the process of this algorithm:</p> <ol> <li> <p>Perform a DFS and assign the preorder number of each nodes, and the preorder number of its last descendant. </p> </li> <li> <p>Separate each nodes incoming edges into two classifications: back edge and other edge.      <ol> <li>A back edge occurs if a node w receives an edge from a node v, where v is considered an ancestor of w.</li> <li>An ancestor can be identified if 2 properties hold: the preorder number of w is less than or equal to v, and the preorder number of the last descendent of w is greater than or equal to that of v i.e. there exists a path from w to v.</li> </ol></p> </li> <li> <p>At the same time initialise each node to be a Union-Find set containing only itself.</p> </li> <li> <p>We then go through the nodes in reverse preorder preforming the following steps:     <ol> <li>Iterate through the node's back-edges, if it links to itself we set it as a self-loop. Otherwise, we add the source of the edge to a list.</li> <li>Create a worklist from our list and traverse back up to the loop header adding nodes as we go. If at any point we find a node that is not an ancestor of our loop header, then the loop is considered irreducible.</li> </ol></p> </li> <li> <p>Finally we union all nodes in the loop, setting their header as the loop header.</p> </li> <li> <p>To ensure we detect all the loops we can also insert new nodes in the case of a loop header being shared by a reducible and non-reducible back-edge.</p> </li> </ol>"},{"location":"fundamentals/structuring/loop-reduction/#conclusion","title":"Conclusion","text":"<p>Both the SGL and Havlak-Tarjan algorithms are excellent choices for a decompiler. It must be noted that the algorithms will produce different results, due to differing definitions on how to nest loops. Further modifications has also been suggested by Ramalingam<sup>8</sup> to ensure a near linear running time for both algorithms. </p> <ol> <li> <p>Tarjan, Robert. Testing flow graph reducibility. Proceedings of the Fifth Annual ACM Symposium on Theory of Computing. 1973.\u00a0\u21a9</p> </li> <li> <p>Sreedhar et al. Identifying loops using DJ graphs. ACM Transactions on Programming Languages and Systems (TOPLAS), Volume 18, Issue 6. November 1996.\u00a0\u21a9\u21a9</p> </li> <li> <p>Havlak, Paul. Nesting of Reducible and Irreducible Loops. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, July 1997.\u00a0\u21a9\u21a9</p> </li> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9\u21a9</p> </li> <li> <p>Allen, Frances E. Control flow analysis. Proceedings of a symposium on Compiler optimization. 1970.\u00a0\u21a9</p> </li> <li> <p>Tarjan, Robert. Depth-First Search and Linear Graph Algorithms. SIAM Journal on Computing. June 1972.\u00a0\u21a9</p> </li> <li> <p>Callahan, Tim. Dataflow &amp; Interval Analysis. 15-745: Optimizing Compilers for Modern Architectures. Carnegie Mellon University, March 2005.\u00a0\u21a9</p> </li> <li> <p>Ramalingam, Ganesan. Identifying Loops In Almost Linear Time. ACM Transactions on Programming Languages and Systems. March 1999.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/overview/","title":"Structuring Overview","text":""},{"location":"fundamentals/structuring/overview/#introduction","title":"Introduction","text":"<p>Academically introduced in Dr. Cifuentes' 1994 Dissertation<sup>1</sup>, decompilation control flow structuring is the process used to turn a control flow graph (CFG) into a structured high-level language.  Control flow structuring in decompilation is highly related to the general control flow structuring process found in compiler research. Although the goal of control flow structuring, referred to as structuring, is to output linear high-level code, the level of abstraction of that code is open research. Additionally, few works exist in structuring for targeting language output other than C. </p>"},{"location":"fundamentals/structuring/overview/#general-structuring-example","title":"General Structuring Example","text":"<p>Although often thought of in the context of assembly, control flow structuring requires a control flow graph and conditions <sup>3</sup>.  For example, the attributed control flow graph below can be used as input:</p> <pre><code>                            +-----+\n                            |  A  |\n                            +-----+\n                              |   |\n                           ~x |   +--+\n                              V      |\n                         +-----+     |\n                         |  B  |     | x\n                         +-----+     |\n                           |  +--+   |\n                        ~y |     | y |\n                           V     V   V\n                       +-----+  +-----+\n                       |  D  |  |  C  |\n                       +-----+  +-----+\n                            |    |\n                            V    V\n                            +-----+\n                            |  E  |\n                            +-----+\n</code></pre> <p>Using a schema-based structuring algorithm, the graph can be turned into the following C:</p> <pre><code>A();\nif(x)\n    goto label_c;\nB();\nif (y) {\nlabel_c:\n    C();\n}\nelse {\n    D();\n}\nE();\n</code></pre> <p>There are multiple ways of turning the graph into linear C code <sup>4</sup>. For instance, the first condition on <code>x</code> can be flipped, changing where the <code>goto</code> appears and how many <code>if</code> scopes exist in the program. </p>"},{"location":"fundamentals/structuring/overview/#types-of-structuring","title":"Types of Structuring","text":"<p>There are two dominant types of structuring algorithms<sup>8</sup>:</p> <ol> <li>Schema-based: Algorithms that construct code based on pre-known graph patterns that are omitted by compilers. These algorithms attempt to only make structured code when they are aware of a direct mapping to its source structure. </li> <li>Gotoless: Algorithms that prioritize removing all unstructured regions from code. These algorithms may use schema-based methods initially but are unique in their pattern-matching of structures that may not exist in its source. </li> </ol> <p>The biggest difference between these two is their reliance on known compiler patterns<sup>5</sup>.  In schema-based algorithms, the decompiler author creates a set of known compiler output patterns to recover a target language.  In gotoless algorithms, the decompiler author uses patterns outside of graph schemas, which may be compiler and language-independent. One such example is condition-based structuring, as used in the DREAM <sup>5</sup> decompiler. </p>"},{"location":"fundamentals/structuring/overview/#related-fields","title":"Related Fields","text":"<p>Structuring in decompilation was directly inspired by compiler works in structuring and general data-flow analysis<sup>3</sup>.  One of these earliest works was the 1970s paper \"Control flow analysis.\"<sup>2</sup>, which laid out the fundamental ideas for constructing control flow graphs. Additionally, many of the ideas for eliminating gotos, which were often a byproduct of schema-based structuring, were inspired by work in restructuring source code<sup>6</sup> <sup>7</sup>.</p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Allen, Frances E. \"Control flow analysis.\" ACM Sigplan Notices 5.7 (1970): 1-19.\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe. \u201c30 Years of Decompilation and the Unsolved Structuring Problem: Part 1.\u201d Mahaloz.Re, 2 Jan. 2024, https://mahaloz.re/dec-history-pt1. Accessed 11 Apr. 2024.\u00a0\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9</p> </li> <li> <p>Williams, M. Howard, and G. Chen. \"Restructuring pascal programs containing goto statements.\" The Computer Journal 28.2 (1985): 134-137.\u00a0\u21a9</p> </li> <li> <p>Erosa, Ana M., and Laurie J. Hendren. \"Taming control flow: A structured approach to eliminating goto statements.\" Proceedings of 1994 IEEE International Conference on Computer Languages (ICCL'94). IEEE, 1994.\u00a0\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/schema-based/schema-based/","title":"Schema-based Structuring","text":""},{"location":"fundamentals/structuring/schema-based/schema-based/#introduction","title":"Introduction","text":"<p>Schema-based structuring is a type of structuring that depends on a set of known compiler graph patterns<sup>1</sup>. With this in mind, a decompiler must know all of the compiler graph patterns to generate code that is structured<sup>3</sup> (contains no gotos). An example of this type of structuring can be found in the overview section.  Schema-based structuring techniques are the most popular techniques among decompilers<sup>1</sup><sup>2</sup><sup>4</sup><sup>5</sup><sup>6</sup>.</p>"},{"location":"fundamentals/structuring/schema-based/schema-based/#example-graph-patterns","title":"Example Graph Patterns","text":"<p>Example graph patterns, from Cifuentes 1994 dissertation<sup>1</sup>, can be seen below:</p> <p></p>"},{"location":"fundamentals/structuring/schema-based/schema-based/#approaches","title":"Approaches","text":"<p>After the foundational dissertation from Cifuentes, the Phoenix<sup>2</sup> work improved on structuring by adding more condition patterns.  These patterns allowed for more correct structures (like loop reductions). </p> <p>Follow-ups to this work all used gotoless<sup>3</sup><sup>4</sup> structuring methods until the SAILR<sup>5</sup> work in 2024. The SAILR work improved on the gotoless algorithms by introducing a new type of schema that \"reverts compiler optimizations.\"</p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>\u010eurfina, Luk\u00e1\u0161, et al. \"Design of a retargetable decompiler for a static platform-independent malware analysis.\" Information Security and Assurance: International Conference, ISA 2011, Brno, Czech Republic, August 15-17, 2011. Proceedings. Springer Berlin Heidelberg, 2011.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/","title":"Switch Structuring","text":""},{"location":"fundamentals/structuring/schema-based/switch-structuring/#introduction","title":"Introduction","text":"<p>One important control flow structure in C is the Switch statement. In schema-based structuring algorithms the Switch statement is handled much like other structures, but requires some special attention because of the many ways a compiler can output a Switch<sup>3</sup>. Phoenix<sup>1</sup> documented much of the simpler cases, found in the examples below. </p> <p>In the simplest form, a Switch uses an indirect jump of a calculated distance from the base address of a jump table to the specific case statement.  Below you can find an example of a Switch statement in C, its corresponding assembly, and its Control Flow Graph (CFG).</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-in-c","title":"Switch in C","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n  int x = rand() % 5;\n  switch (x) {\n    case 0:\n      printf(\"hello\\n\");\n      break;\n    case 1:\n      printf(\"hello2\\n\");\n      break;\n    case 2:\n      printf(\"hello3\\n\");\n      break;\n    case 3:\n      printf(\"hello4\\n\");\n      break;\n    case 4:\n      printf(\"hello5\\n\");\n      break;\n    default:\n      return 0;\n  }\n  return x;\n}\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-in-assembly","title":"Switch in Assembly","text":"<pre><code>0x401169:       endbr64\n0x40116d:       push    rbp\n0x40116e:       mov     rbp, rsp\n0x401171:       sub     rsp, 0x10\n0x401175:       call    0x401070 ; rand()\n0x40117a:       movsxd  rdx, eax\n0x40117d:       imul    rdx, rdx, 0x66666667\n0x401184:       shr     rdx, 0x20\n0x401188:       sar     edx, 1\n0x40118a:       mov     ecx, eax\n0x40118c:       sar     ecx, 0x1f\n0x40118f:       sub     edx, ecx\n0x401191:       mov     dword ptr [rbp - 4], edx\n0x401194:       mov     ecx, dword ptr [rbp - 4]\n0x401197:       mov     edx, ecx\n0x401199:       shl     edx, 2\n0x40119c:       add     edx, ecx\n0x40119e:       sub     eax, edx\n0x4011a0:       mov     dword ptr [rbp - 4], eax\n0x4011a3:       cmp     dword ptr [rbp - 4], 4\n0x4011a7:       ja      0x401222\n0x4011a9:       mov     eax, dword ptr [rbp - 4]\n0x4011ac:       lea     rdx, [rax*4]\n0x4011b4:       lea     rax, [rip + 0xe6d]\n0x4011bb:       mov     eax, dword ptr [rdx + rax]\n0x4011be:       cdqe\n0x4011c0:       lea     rdx, [rip + 0xe61]\n0x4011c7:       add     rax, rdx\n0x4011ca:       notrack jmp     rax\n0x401222:       mov     eax, 0\n0x401227:       jmp     0x40122c\n0x401200:       lea     rax, [rip + 0xe11] ; case 0\n0x401207:       mov     rdi, rax\n0x40120a:       call    0x401060\n0x4011cd:       lea     rax, [rip + 0xe30] ; case 1\n0x4011d4:       mov     rdi, rax\n0x4011d7:       call    0x401060\n0x4011ef:       lea     rax, [rip + 0xe1b] ; case 2\n0x4011f6:       mov     rdi, rax\n0x4011f9:       call    0x401060\n0x401211:       lea     rax, [rip + 0xe07] ; case 3\n0x401218:       mov     rdi, rax\n0x40121b:       call    0x401060\n0x4011de:       lea     rax, [rip + 0xe25] ; case 4\n0x4011e5:       mov     rdi, rax\n0x4011e8:       call    0x401060\n0x40122c:       leave\n0x40122d:       ret\n0x40120f:       jmp     0x401229\n0x4011dc:       jmp     0x401229\n0x4011fe:       jmp     0x401229\n0x401220:       jmp     0x401229\n0x4011ed:       jmp     0x401229\n0x401229:       mov     eax, dword ptr [rbp - 4]\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-cfg","title":"Switch CFG","text":""},{"location":"fundamentals/structuring/schema-based/switch-structuring/#compiler-representations","title":"Compiler Representations","text":"<p>After compilation, a Switch can take on many forms, largely due to compiler optimizations<sup>3</sup>.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#jump-table","title":"Jump Table","text":"<p>The switch statement above loads the base offset of the jump table from memory, and then computes an indirect JMP to that address offset by the % calculation.  The offset is multiplied by 4 to account for the size of each address in memory (rdx = rax * 4).  This computation is what allows us constant time when performing a simple switch statement.  The indirect branch is particularly important, because it is rare to see a non-hardcoded unconditional JMP instruction, therefore anytime we see such a situation we can mark it as a potential switch.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#multiple-jump-tables","title":"Multiple Jump Tables","text":"<p>In the situation where we have large distance between sets of numbers in our switch, the compiler may create multiple jump tables as a simple way to retain constant time in our switch statement. </p> <p>Below, C code can be found that will emit multiple jump tables when compiled. </p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#multi-jump-table-switch-when-compiled","title":"Multi Jump Table Switch (when compiled)","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n  int x = rand() % 5;\n  switch (x) {\n  case 0:\n    printf(\"hello\\n\");\n    break;\n  case 1:\n    printf(\"hello2\\n\");\n    break;\n  case 2:\n    printf(\"hello3\\n\");\n    break;\n  case 1340:\n    printf(\"hello4\\n\");\n    break;\n  case 1341:\n    printf(\"hello5\\n\");\n    break;\n  case 1342:\n    printf(\"hello6\\n\");\n    break;\n  // and so on...\n  default:\n    return 0;\n  }\n  return x;\n}\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#balanced-tree","title":"Balanced Tree","text":"<p>Binary Search Trees (BSTs) and Ternary Search Trees (TSTs) are used by LLVM and GCC respectively<sup>4</sup> when it is not possible to implement a jump table(s)<sup>2</sup>.  In our first figure we showed a C program containing simple and consecutive case numbers (0, 1, 2, 3, 4) this means it is easy to calculate an offset by looking up the index associated with it. If the case constants are not close or a constant distance apart then the compiler prefers a ST over large amounts of unused memory. </p> <p>An example below shows C code that would result in an ST when compiled.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#search-tree-switch","title":"Search Tree Switch","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n  int x = rand() % 5;\n  switch (x) {\n    case 0:\n      printf(\"hello\\n\");\n      break;\n    case 10:\n      printf(\"hello2\\n\");\n      break;\n    case 200:\n      printf(\"hello3\\n\");\n      break;\n    case 301:\n      printf(\"hello4\\n\");\n      break;\n    case 457:\n      printf(\"hello5\\n\");\n      break;\n    default:\n      return 0;\n  }\n  return x;\n}\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#cascascaded-conditionals","title":"Cascascaded Conditionals","text":"<p>In a few cases the compiler may deem the best representation of the switch statement to be a set of if, else-if, and else's, in this case the techniques outlined in the phoenix paper<sup>1</sup> would decompile them back to cascaded conditionals, which can result in a rather messy output from the decompiler. SAILR<sup>3</sup> introduces some techniques for identifying/reverting this, and producing a more accurate output.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#structuring","title":"Structuring","text":"<p>We must now switch up our discussion from the different ways in which a compiler may represent a switch statement, to the ways in which we can structure them. Much of this section has been written from the influence of implementations found in the Reko Decompiler.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#standard-switch-structuring","title":"Standard Switch Structuring","text":"<p>When structuring we visit each of the nodes in post-order and attempt to match against a set of schemas<sup>1</sup><sup>5</sup>. 2 of these schemas outlined in the Phoenix paper<sup>1</sup> represent the 2 types of switch statement structures we can get: Switch and IncSwitch (Incomplete-Switch). If no such statements are found in a particular traversal we may identify switch candidates (covered in the next section).</p> <p>For us to structure a switch the following must hold:</p> <ol> <li>The switch head is the only entry.</li> <li>There must be one node that is exited to, or all switch cases have no successors.</li> </ol> <p>If these conditions are not met we queue the switch and hope other structuring occurs before we procede with its reduction.</p> <p>The algorithm for structuring a switch is as follows:</p> <ol> <li>Identify the switch.<ol> <li>Indirect jump to a jump table. </li> </ol> </li> <li>Identify the cases.<ol> <li>Simply find all the successors of the switch head.</li> <li>The front end of the decompiler identifies jump tables relevent to the function and adds the correct successors of the graph. The entries that are added to this are done by determining the bounds that are possible to be accessed, this is obvious due to the JA instruction which defines an upper bound and the base address being loaded into the RAX register. Any gaps in the jump table should hold the same address as the JA instruction<sup>6</sup>.  </li> </ol> </li> <li>Produce a switch statement.</li> <li>Relink graph.</li> </ol>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#non-standard-switch-structuring","title":"Non-Standard Switch Structuring","text":"<p>If the switch statement is not represented by a jump table It will likely be structured as cascading if/else statements. In this case we can perform a rewrite to make it more legible after we have completed structuring. To achieve this 2 steps need to be performed:</p> <ol> <li> <p>Identify the first node in a cascade.</p> </li> <li> <p>Collect all subsequent nodes in a cascade.</p> </li> </ol> <p>Identifying the first node is done by checking for if statements that meet a set of pattern requirements in their condition. The condition for a valid statement can be a simple <code>==</code>/<code>!=</code>; a sequence of <code>!=</code>'s separated by <code>&amp;&amp;</code>'s or a sequence of <code>==</code>'s separated by <code>&amp;&amp;</code>'s. We then check consecutive if structures to see if they meet this pattern, and if we can collect a sufficient number of statements we can condense it down into a switch.</p> <p>This however obviously wouldn't work in the case of STs, which uses &gt; and &lt; comparators.  When STs occur few decompilers seem correctly identify this, of those that do there is 2 choices: simplify, but maintain if statements (Hex-Rays) or convert to a switch statement (angr). The addition to support this involves simply continuing to traverse for == or != operators to find the leaf nodes of the STs.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-candidates","title":"Switch Candidates","text":"<p>This is a term used in the Phoenix paper<sup>1</sup> to describe a schema that hasn't been matched, but could be potentially a switch. They occur when a switch has extra edges into or out of it. If there are extra incoming edges to nodes other than the switch head we can simply replace the edge with a jump and for extra outgoing edges we have 2 potential choices:</p> <ol> <li>If there is an edge from one of the case nodes to the node that immediately post dominates the switch head, we choose the immediate post dominator as our successor to the switch.     <ol> <li>Immediately post dominating means the closest node n where if we reversed all edges on the graph you would have to go through n to get to the switch head.</li> </ol></li> <li>Else we choose the successor to be the node with the highest number of edges from case nodes, that is not a case node itself.</li> </ol>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#code-examples","title":"Code Examples","text":"<p>Open-source decompilers document a few different implementations of Switch structuring.  Below, you can find links to code sections in open-source decompilers as well as their likely implemented algorithm.</p> <ul> <li>angr decompiler: Phoenix and SAILR</li> <li>Reko: Phoenix with modifications.</li> </ul> <ol> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Eilam, Eldad. \"Reversing: Secrets of Reverse Engineering\". Wiley. 2005.\u00a0\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Liska, Martin: Switch lowering improvements \u2013 slideslive. https://slideslive.com/38902416/switch-lowering-improvements. 2017.\u00a0\u21a9</p> </li> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Yang, Zack: How C/C++ Compiler Generate Assembly Code For Switch Statement. https://www.zackyang.blog/article/assembly-code-generation-for-switch-statement. 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/","title":"Constraint Simplification","text":""},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#introduction","title":"Introduction","text":"<p>Lattice-based type recovery is a decompilation technique that infers data types in an intermediate representation (IR). Two major systems use this approach: TIE <sup>2</sup> and Retypd <sup>1</sup>. Retypd extends TIE by supporting both recursive and polymorphic types.</p> <p></p> <p>In a type lattice, an edge points from a supertype to its subtype\u2014e.g., <code>int32</code> is a subtype of <code>num32</code>. TIE determines upper and lower lattice bounds by applying constraints and then chooses an appropriate type. Retypd instead encodes constraints in a sketch <sup>1</sup>: a tree whose nodes are types and whose outgoing edges are labeled with the capabilities of those types.</p> <p>This article outlines Retypd\u2019s process for constraint generation and simplification (see Figure 1 in <sup>1</sup>). Retypd is open-source and underpins decompilers such as angr. The implementation diverges slightly from the original publication; those differences are summarized in the type-recovery outline</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#prerequisites","title":"Prerequisites","text":"<p>This section reviews the key type-theory terms used in lattice-based constraint simplification<sup>1</sup>.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#subtypes","title":"Subtypes","text":"<p>The notation <code>A &lt;: B</code> means \u201cA is a subtype of B.\u201d Many object-oriented languages adopt the same idea\u2014for example, if <code>Dog</code> implements <code>Animal</code>, then <code>Dog &lt;: Animal</code>.</p> <p>Alternate form: some texts write the relation as <code>A \u2286 B</code>.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#labels","title":"Labels","text":"<p>A label describes a capability of a type and is written <code>type.label</code>. For a pointer of type <code>X</code>, Retypd models three capabilities:</p> <ol> <li><code>X.load</code></li> <li><code>X.store</code></li> <li><code>X.\u03c3N@k</code></li> </ol> <p><code>X.load</code> reads through the pointer (<code>A = *X</code>). <code>X.store</code> writes through the pointer (<code>*X = A</code>). <code>X.\u03c3N@k</code> accesses N bits at offset k; for instance, <code>t-&gt;b</code> in the code below is represented as <code>t.\u03c316@64</code>.</p> <pre><code>struct Test {\n    int64_t a;\n    int16_t b;\n};\n\nstruct Test *t;\nt-&gt;b;\n</code></pre> <p>If the program assigns <code>t-&gt;b</code> to a variable, then <code>t</code> acquires the capability <code>t.load.\u03c316@64</code>.</p> <p>Functions introduce two additional labels:</p> Label Meaning <code>inL</code> Parameters at location L <code>outL</code> Return value at location L"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#variance","title":"Variance","text":"<p>Variance explains how a label\u2019s subtyping relationship follows (or reverses) the relationship of its base type:</p> Kind Labels Rule Covariant <code>out</code>, <code>load</code>, <code>\u03c3N@k</code> If <code>A &lt;: B</code>, then\u00a0<code>A.label &lt;: B.label</code>. Contravariant <code>in</code>, <code>store</code> If <code>A &lt;: B</code>, then\u00a0<code>B.label &lt;: A.label</code>."},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#deduction-rules","title":"Deduction Rules","text":"<p>Retypd represents inference rules in sequent form:</p>  \\frac{P_1 \\quad P_2 \\quad \\dots \\quad P_n}{C}  <p>Using <code>+</code> for covariance and <code>\u2212</code> for contravariance gives:</p> <p>Covariant</p>  \\frac{A &lt;: B \\quad \\text{Var}\\,B.\\text{label} \\quad \\langle \\text{label} \\rangle = +}      {A.\\text{label} &lt;: B.\\text{label}}  <p>Contravariant</p>  \\frac{A &lt;: B \\quad \\text{Var}\\,B.\\text{label} \\quad \\langle \\text{label} \\rangle = -}      {B.\\text{label} &lt;: A.\\text{label}}  <p>Retypd applies these rules implicitly during the later simplification pass.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#generating-constraints","title":"Generating Constraints","text":"<p>Constraint generation starts with the lifted IR plus an initial set of variables identified from function parameters and instruction patterns. Appendix A of <sup>1</sup> groups variable uses into six categories:</p> <ol> <li>Register loads and stores</li> <li>Addition and subtraction</li> <li>Memory loads and stores</li> <li>Procedure invocation</li> <li>Floating-point operations</li> <li>Bit manipulation</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#examples","title":"Examples","text":"<p>The table below shows how each IR pattern translates to a constraint.</p> Category / IR Input Constraint Produced <code>MOV R1, R2</code> <code>R2 &lt;: R1</code> <code>var = A + const</code> <code>A.+const &lt;: var</code> <code>var = A \u2212 const</code> <code>A.-const &lt;: var</code> <code>var = A + B</code> <code>Add(B, C, A)</code> <code>var = A \u2212 B</code> <code>Sub(B, C, A)</code> <code>A = *B</code> <code>B.load.\u03c3k@0 &lt;: A</code> <code>*A = B</code> <code>B &lt;: A.store.\u03c3k@0</code> <code>A = F(B)</code> <code>F.out &lt;: A</code>  and  <code>B &lt;: F.in</code> <p>Floating-point and bit-manipulation instructions may be handled explicitly or only when function signatures reveal the need <sup>1</sup>. Addition and subtraction propagate pointer-versus-integer information even when the constant operand is unknown, as explained in a later section.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#inferring-sketch-shape","title":"Inferring Sketch Shape","text":""},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#collecting-constraints","title":"Collecting Constraints","text":"<p>Retypd processes strongly connected components (SCCs) of the call graph from the leaves upward:</p> <ol> <li>Merge the constraints produced by every procedure in an SCC.</li> <li>Instantiate each call site: replace symbolic labels such as <code>F.inL</code> or <code>F.outL</code> with concrete lattice types, treating every call polymorphically.</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#building-the-quotient-graph","title":"Building the Quotient Graph","text":"<p>Algorithm E.1 <sup>1</sup> constructs a quotient graph <code>G</code> whose nodes are equivalence classes of derived type variables.</p> <pre><code>G \u2190 \u2205\nfor p.L1 \u2026 Ln in C.derivedTypeVars do\n    for i \u2190 1 \u2026 n do\n        s \u2190 FINDEQUIVREP(p.`1 \u2026 `i\u22121, G)\n        t \u2190 FINDEQUIVREP(p.`1 \u2026 `i,   G)\n        G.edges \u2190 G.edges \u222a (s, t, `i)  // label `i`\n    end for\nend for\n</code></pre> <p>For example, the variable <code>A.B.C</code> yields edges <code>A \u2192 A.B</code> (labeled <code>.B</code>) and <code>A.B \u2192 A.B.C</code> (labeled <code>.C</code>).</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#unification","title":"Unification","text":"<p>Each explicit subtype constraint <code>x &lt;: y</code> is enforced by unifying the corresponding nodes:</p> <pre><code>for x &lt;: y in C do\n    X \u2190 FINDEQUIVREP(x, G)\n    Y \u2190 FINDEQUIVREP(y, G)\n    UNIFY(X, Y, G)\nend for\n</code></pre> <p><code>UNIFY</code> merges equivalence classes and recurses on matching edges or the compatible pair <code>.load</code>/<code>.store</code>:</p> <pre><code>procedure UNIFY(X, Y, G)\n    if X \u2260 Y then\n        MAKEEQUIV(X, Y, G)\n        for (X', l) in G.outEdges(X) do\n            if (Y', l) in G.outEdges(Y) then\n                UNIFY(X', Y', G)\n            else if l = .load and (Y', .store) in G.outEdges(Y) then\n                UNIFY(X', Y', G)\n            end if\n        end for\n    end if\nend procedure\n</code></pre> <p></p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#propagating-add-sub","title":"Propagating <code>Add</code> / <code>Sub</code>","text":"<p>Retypd evaluates each <code>ADD</code> or <code>SUB</code> constraint repeatedly:</p> <pre><code>repeat\n    C_old \u2190 C\n    for c in C_old with c = ADD(_) or SUB(_) do\n        D \u2190 APPLYADDSUB(c, G, C)   // may generate new subtype edges\n        for \u03b4 in D with \u03b4 = X &lt;: Y do\n            UNIFY(X, Y, G)\n        end for\n    end for\nuntil C_old = C\n</code></pre> <p>Figure 13 of <sup>1</sup> shows how labels such as <code>.load</code> and <code>.store</code> disclose pointer information during this phase.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#initial-sketches","title":"Initial Sketches","text":"<p>Once propagation converges, each type variable receives an initial sketch:</p> <pre><code>for v in C.typeVars do\n    S \u2190 new Sketch\n    L(S) \u2190 ALLPATHSFROM(v, G)\n    for state w in S do\n        \u03bdS(w) \u2190 (\u27e8w\u27e9 = +) ? T : \u22a5   // T = lattice bottom; \u22a5 = lattice top\n    end for\n    B[v] \u2190 S\nend for\n</code></pre>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#creating-a-pushdown-system","title":"Creating a Pushdown System","text":"<p>The next step simplifies the constraint set with a pushdown system (PDS) derived from Figure 3 of <sup>1</sup>. <code>F.in0</code> serves as the start state and <code>F.out</code> as the final state.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#interesting-constraints","title":"Interesting Constraints","text":"<p>A constraint is interesting if it satisfies any of these conditions:</p> <ol> <li>It involves a capability label.</li> <li>It is recursively self-referential.</li> <li>It contains a type constant.</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#constraint-graph-construction","title":"Constraint Graph Construction","text":"<p>For every subtype edge (e.g., <code>A &lt;: B</code>, <code>A.load &lt;: D</code> and <code>C &lt;: B.store</code>) the PDS adds an unlabeled edge:</p> <p></p> <p>Next, two families of labeled edges are inserted:</p> Direction Edge label Description Subtype \u2192 prefix <code>pop</code> Remove one label from the end (forget). Supertype \u2192 prefix <code>push</code> Add one label to the end (recall). <p>Edges are added recursively to cover all prefixes, maintaining correct variance:</p> <p></p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#saturation","title":"Saturation","text":"<p>Caucal\u2019s algorithm <sup>3</sup> adds shortcut edges so consecutive <code>push</code>/<code>pop</code> pairs cancel, achieving closure in O(n^3) time. Additional adjustments:</p> <ol> <li>Insert edges implied by <code>A.load &lt;: A.store</code>.</li> <li>Remove self-loops.</li> <li>Ensure a <code>pop</code> edge is unreachable after a single <code>push</code> with the same label.</li> </ol> <p></p> <p>In the example above, the following path: <pre><code>A.load.\u2296\n     \u2192 A.store.\u2296 (implied node)\npush \u2192 A.\u2295 \n     \u2192 B.\u2295\npop  \u2192 B.store.\u2296\n</code></pre></p> <p>This path contains a <code>push</code> directly followed by a <code>pop</code>. Where both use the same label: <code>store</code>. Therefore, we can apply the shortcut:</p> <pre><code>A.load.\u2296 \u2192 B.store.\u2296\n</code></pre>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#breaking-sccs","title":"Breaking SCCs","text":"<p>Recursive type information becomes visible by iteratively removing strongly connected components:</p> <ol> <li>Temporarily strip the current set of interesting nodes.</li> <li>Identify remaining SCCs.</li> <li>For each SCC, select the node with the shortest label prefix, add it to the interesting set, and remove it from the graph.</li> <li>Repeat until no SCCs persist.</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#code-examples","title":"Code Examples","text":"<p>The Retypd implementation provides several solvers for constraints viewable here.</p> <p>Although the Retypd implementation and angr\u2019s Typehoon analysis follow the same principles, both deviate slightly from the original paper:</p> <ul> <li>Retypd source tree</li> <li>angr Typehoon</li> </ul> <ol> <li> <p>Noonan, Matt; Loginov, Alexey; and Cok, David. \u201cPolymorphic Type Inference for Machine Code.\u201d PLDI 2016.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Lee, JongHyup; Avgerinos, Thanassis; and Brumley, David. \u201cTIE: Principled Reverse Engineering of Types in Binary Programs.\u201d USENIX Security 2011.\u00a0\u21a9</p> </li> <li> <p>Caucal, Didier. \u201cOn the Regular Structure of Prefix Rewriting.\u201d Theoretical Computer Science 106 (1): 61\u201386, 1992.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/type-recovery/overview/","title":"Type Recovery Overview","text":""},{"location":"fundamentals/type-recovery/overview/#introduction","title":"Introduction","text":"<p>Type recovery is the process of identifying high-level variables and their types from a binary<sup>1</sup>, usually in the form of a CFG. This wiki groups variable identification with type recovery since they are often intertwined in decompilation<sup>2</sup>. </p> <p>In most decompilers, the typing system requires a well-formed CFG, usually lifted, on which to perform analysis.  Analysis phases can be thought to work in two refining stages:</p> <ol> <li>Variable Identification: discover initial locations and their boundaries</li> <li>Type Constraining: utilizing the variables' uses in the code, make constraints for size and choose a type.</li> </ol> <p>This process is often iterative between constraint building and variable identification.  Variable identification also includes retyping/resizing as more constraints are gathered. </p> <p></p>"},{"location":"fundamentals/type-recovery/overview/#typing-example","title":"Typing Example","text":"<p>A simple C program is shown below: <pre><code>int main(int argc, char** argv) {\n    char* str = argv[1];\n    puts(str);\n}\n</code></pre></p> <p>After compiling and disassembling: <pre><code>gcc example.c -o example &amp;&amp; objdump -D -M intel example | grep \"&lt;main&gt;:\" -A 12\n</code></pre></p> <p>We are left with the following: <pre><code>0000000000001149 &lt;main&gt;:\n    1149:   f3 0f 1e fa             endbr64\n    114d:   55                      push   rbp\n    114e:   48 89 e5                mov    rbp,rsp\n    1151:   48 83 ec 20             sub    rsp,0x20\n    1155:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi\n    1158:   48 89 75 e0             mov    QWORD PTR [rbp-0x20],rsi\n    115c:   48 8b 45 e0             mov    rax,QWORD PTR [rbp-0x20]\n    1160:   48 8b 40 08             mov    rax,QWORD PTR [rax+0x8]\n    1164:   48 89 45 f8             mov    QWORD PTR [rbp-0x8],rax\n    1168:   48 8b 45 f8             mov    rax,QWORD PTR [rbp-0x8]\n    116c:   48 89 c7                mov    rdi,rax\n    116f:   e8 dc fe ff ff          call   1050 &lt;puts@plt&gt;\n</code></pre></p> <p>A naive variable recovery algorithm might do the following: <pre><code>int main(int a1, long long a2) {\n    int v1; // rbp-0x14\n    long long v2; // rbp-0x20\n    long long v3; // rax\n    long long v4; // rbp-0x8\n    v1 = a1;\n    v2 = a2;\n    v3 = *(&amp;v2 + 1);\n    v4 = v3\n    puts((char *) v4);\n}\n</code></pre></p> <p>However, since <code>puts</code> is known to take a <code>char *</code> as the first argument this would allow <code>v4</code> to be constrained to be a <code>char *</code>. Back-propagating this type constraint to the earlier variables, we get the following: <pre><code>int main(int a1, char** a2) {\n    int v1; // rbp-0x14\n    char** v2; // rbp-0x20\n    char * v3; // rax\n    char * v4; // rbp-0x8\n    v1 = a1;\n    v2 = a2;\n    v3 = v2[1];\n    v4 = v3\n    puts(v4);\n}\n</code></pre></p>"},{"location":"fundamentals/type-recovery/overview/#variable-identification","title":"Variable Identification","text":"<p>Variable identification seeks to map memory locations and registers to high-level variables in the targeted language output (usually C). Early decompilers often mapped variables to locations based on their simple accesses <sup>2</sup>. Later work has followed up on this by expanding the set of uses supported for a variable location identification <sup>1</sup>.  Identified variables often have many candidates for size (because of their potential type). These candidates have often been reduced to a single type based on their type sinks<sup>3</sup>, uses that have explicit types like in a call argument. </p>"},{"location":"fundamentals/type-recovery/overview/#type-constraining","title":"Type Constraining","text":"<p>Type constraining is directly linked to variable identification since the use of the variable is affected by its size (arrays vs primitive types). Previous works in this area have looked at multiple ways to gather and reduce types for variables. Some of these include: def-use analysis<sup>1</sup><sup>5</sup><sup>6</sup>, library awareness<sup>3</sup><sup>5</sup><sup>6</sup>, emulation<sup>4</sup>, lattice-solving<sup>7</sup><sup>8</sup>, and machine learning<sup>9</sup><sup>10</sup>.</p> <p>Of these works, typing involving lattice-based methods<sup>7</sup><sup>8</sup>, also known as push-down typing, is the most popular among modern open-source decompilers. </p> <ol> <li> <p>Balakrishnan, Gogul, and Thomas Reps. \"Divine: Discovering variables in executables.\" International Workshop on Verification, Model Checking, and Abstract Interpretation. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Mycroft, Alan. \"Type-based decompilation (or program reconstruction via type reconstruction).\" European Symposium on Programming. Berlin, Heidelberg: Springer Berlin Heidelberg, 1999.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lin, Zhiqiang, Xiangyu Zhang, and Dongyan Xu. \"Automatic reverse engineering of data structures from binary execution.\" Proceedings of the 11<sup>th</sup> Annual Information Security Symposium. 2010.\u00a0\u21a9\u21a9</p> </li> <li> <p>Slowinska, Asia, Traian Stancescu, and Herbert Bos. \"Howard: A Dynamic Excavator for Reverse Engineering Data Structures.\" NDSS. 2011.\u00a0\u21a9</p> </li> <li> <p>Haller, Istvan, Asia Slowinska, and Herbert Bos. \"Mempick: High-level data structure detection in c/c++ binaries.\" 2013 20<sup>th</sup> Working Conference on Reverse Engineering (WCRE). IEEE, 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Jin, Wesley, et al. \"Recovering C++ objects from binaries using inter-procedural data-flow analysis.\" Proceedings of ACM SIGPLAN on Program Protection and Reverse Engineering Workshop 2014. 2014.\u00a0\u21a9\u21a9</p> </li> <li> <p>Noonan, Matt, Alexey Loginov, and David Cok. \"Polymorphic type inference for machine code.\" Proceedings of the 37<sup>th</sup> ACM SIGPLAN Conference on Programming Language Design and Implementation. 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011)\u00a0\u21a9\u21a9</p> </li> <li> <p>Zhang, Zhuo, et al. \"Osprey: Recovery of variable and data structure via probabilistic analysis for stripped binary.\" 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Chen, Qibin, et al. \"Augmenting decompiler output with learned variable names and types.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/type-recovery/var-detection/","title":"Variable Detection","text":""},{"location":"fundamentals/type-recovery/var-detection/#introduction","title":"Introduction","text":"<p>Variable detection encompasses a set of techniques used to annotate memory regions and registers with variable type classifications. Common classifications include global, local, heap, and register variables. Accurate variable classification is foundational to subsequent analyses for decompilation.</p>"},{"location":"fundamentals/type-recovery/var-detection/#naive-variable-identification","title":"Na\u00efve Variable Identification","text":"<p>Early decompilation techniques<sup>1</sup> primarily relied on recognizing common idioms in assembly code:</p> Type Access Pattern Local <code>[rbp-4]</code> Global <code>[0xdeadbeef]</code> or <code>[rip+0xdeadbeef]</code> <p>Local variables are typically identified by their access as an offset from the frame pointer. Global variables, in contrast, are accessed via absolute addressing\u2014either through a fixed address or an offset relative to the instruction pointer.</p> <p>This analysis can be extended by incorporating knowledge of calling conventions, which define:</p> <ul> <li>How arguments are passed (via stack or registers)  </li> <li>Which registers must be preserved (caller- vs. callee-saved)  </li> <li>How return values are provided  </li> </ul> <p>The regularity imposed by calling conventions allows the use of pattern matching to infer function arguments, return values, and therefore variables.</p> <p>Although this approach is relatively simple, it achieves surprisingly high accuracy\u2014detecting approximately 83% of local variables<sup>2</sup>. Its main limitations arise when variables are accessed indirectly, promoted to registers, or dynamically allocated on the heap. Nevertheless, many decompilers use na\u00efve identification as the foundation for variable and type analysis because of its efficiency, simplicity, and robustness.</p>"},{"location":"fundamentals/type-recovery/var-detection/#divine","title":"DIVINE","text":"<p>DIVINE<sup>2</sup> introduced a more advanced approach by combining Value-Set Analysis (VSA) with Aggregate Structure Identification (ASI). VSA models memory to determine potential addresses and values of memory locations, while ASI detects higher-level data structures (e.g., arrays, records, objects). Results from ASI can be fed back into VSA for further refinement through additional iterations.</p>"},{"location":"fundamentals/type-recovery/var-detection/#value-set-analysis-vsa","title":"Value-Set Analysis (VSA)","text":"<p>VSA enables the identification of heap-allocated variables and generally outperforms na\u00efve techniques. It partitions memory into three regions: global memory, the activation record (stack), and the heap.</p>"},{"location":"fundamentals/type-recovery/var-detection/#memory-representation","title":"Memory Representation","text":"<p>VSA computes an over-approximation of memory regions by maintaining a set of possible addresses or values for each location. Memory regions are typically represented as:</p> Region Type Occurrence Procedure One per procedure Global One per program Heap One per heap allocation <p>This separation allows addresses to be disambiguated across procedures and heap allocations, which is generally unnecessary for globals.  </p> <p>A memory address can be modeled as:</p> <pre><code>struct Addr {\n    struct MemoryRegion region;\n    size_t offset;\n};\n</code></pre> <p>Here, <code>offset</code> represents the displacement within the region (e.g., relative to the frame pointer for procedure regions).</p> <p>While this representation captures direct addresses, it struggles with indirect memory accesses. Offsets are therefore more accurately represented as sets of possible values, often encoded as strided intervals <code>s[l, u]</code>, where <code>s</code> is the stride and <code>[l, u]</code> the lower and upper bounds:</p> <p>Example: <code>4[8, 22] \u2192 {8, 12, 16, 20}</code></p> <p>This model is especially effective for representing array element offsets.</p> <p>To handle indirect addressing, we must track the possible values of registers and memory at each program point. These are represented as abstract locations (a-locs). Na\u00efve variable identification provides an initial set of a-locs: one per global, one per heap region, and one per register.</p> <pre><code>struct Stride {\n    size_t stride;\n    size_t lower;\n    size_t upper;\n};\n\nstruct AbstractLocation {\n    size_t offset;\n    size_t size;\n    struct Stride value_set;\n};\n\nstruct Store {\n    struct MemoryRegion region;\n    map[offset] =&gt; AbstractLocation;\n};\n</code></pre>"},{"location":"fundamentals/type-recovery/var-detection/#vsa-algorithm","title":"VSA Algorithm","text":"<p>The VSA algorithm determines the set of possible addresses and values for each abstract location at every program point. Key steps:</p> <ol> <li> <p>Build a Control Flow Graph (CFG):    Each instruction becomes a node, with edges representing control flow.</p> </li> <li> <p>Initialize the Abstract Store:    Start with known global variables and initial stack/register state.</p> </li> <li> <p>Apply Transfer Functions:    For instructions such as <code>mov</code>, <code>lea</code>, <code>cmp</code>, <code>push</code>, <code>pop</code>, and conditional jumps, update the abstract store accordingly.</p> </li> <li> <p>Handle Cycles with Widening:    Repeated analysis of loops is prevented through widening, which generalizes patterns.    Example: <code>{1, 3, 5, 7, \u2026}</code> becomes <code>2[1, \u221e)</code>.</p> </li> <li> <p>Model Interprocedural Behavior:    Add nodes for function calls and returns, propagating parameter and return value information.    Stack arguments are retrieved from caller memory regions; register arguments from register value sets.    Upon function exit, registers and stack values are restored.</p> </li> <li> <p>Address Indirect Jumps/Calls:    Where possible, resolve targets via value sets; otherwise, conservatively ignore them.</p> </li> </ol>"},{"location":"fundamentals/type-recovery/var-detection/#affine-relations","title":"Affine Relations","text":"<p>Loops introduce imprecision, as value sets could otherwise grow unboundedly. Affine relation analysis<sup>5</sup> improves precision by expressing relationships between variables:</p>  a_0 + \\sum_{i=1}^n a_i r_i = 0  <p>This allows inference of bounds for one register based on others.</p> <p>Example: Array Initialization Loop</p> <pre><code>xor ecx, ecx\nmov edx, arr\nL1:\n    lea eax, [ecx*8]\n    add eax, edx\n    mov [eax], ecx\n    inc ecx\n    cmp ecx, 10\n    jl L1\n</code></pre> <p>Here: <code>eax = ecx*8 + edx</code></p> <p>Since <code>ecx &lt; 10</code>, we deduce: <code>eax = 8[0, 9] + &amp;arr[0] = {0, 8, \u2026, 72} + &amp;arr[0]</code></p> <p>This increases precision when modeling memory accesses.</p>"},{"location":"fundamentals/type-recovery/var-detection/#call-strings","title":"Call Strings","text":"<p>Call strings<sup>7</sup> provide context-sensitive interprocedural analysis by recording call paths. Each unique call string corresponds to a separate abstract store, improving precision. However, this approach is computationally more expensive, as each function must be re-analyzed per distinct call path.</p> <p>Example of valid call strings: <code>{{1,2,4}, {1,2,3}, {1,3,4}}</code> This excludes invalid paths such as <code>1\u21922\u21923\u21924</code>.</p> <p>Call strings are typically bounded to prevent explosion in recursive or deep call chains.</p>"},{"location":"fundamentals/type-recovery/var-detection/#aggregate-structure-identification-asi","title":"Aggregate Structure Identification (ASI)","text":"<p>The output of VSA includes:</p> <ul> <li>Value sets for registers, globals, stack, and heap</li> <li>Detected indirect memory accesses</li> <li>Memory state changes across program points</li> </ul> <p>ASI uses this information to recover variables, arrays, and structures.</p> <p>For stack analysis, size can be inferred from stack-pointer adjustments (e.g., <code>sub esp, 80</code> \u2192 80-byte frame). Memory accesses are used to split the region into smaller atoms.</p> <p>Example:</p> <pre><code>mov ebp, esp\nsub esp, 80\nxor ecx, ecx\nmov edx, ebp\nL1:\n    lea eax, [ecx*8]\n    add eax, edx\n    mov [eax], ecx\n    inc ecx\n    cmp ecx, 10\n    jl L1\nmov esp, ebp\n</code></pre> <p>Here, the 80-byte region is divided into 10 elements of 8 bytes each, suggesting an array. Structure-like accesses appear as fixed offsets from a pointer:</p> <pre><code>mov [eax], 1\nmov [eax+4], 2\nmov [eax+16], 3\n</code></pre> <p>ASI results are fed back into VSA, refining abstract locations and improving subsequent iterations. Iterations continue until results converge, after which information is passed to the type analysis phase.</p>"},{"location":"fundamentals/type-recovery/var-detection/#improvements","title":"Improvements","text":"<p>The main drawback of VSA (and affine relation analysis) is runtime cost<sup>2</sup><sup>6</sup>, especially when multiple VSA\u2013ASI iterations are required. Handling self-modifying code also remains challenging.</p> <p>Improvements include:</p> <ul> <li>DVSA (TIE system)<sup>4</sup>: Uses SSA-form IR and linear combinations over strided intervals, improving precision and simplifying value-set computation.</li> <li>SecondWrite<sup>3</sup>: Suggests runtime optimizations for DIVINE to make it more scalable.</li> <li>Ghidra[^8]: Performs VSA intraprocedurally and ignores calls, floating point arithmetic and heap regions.</li> </ul>"},{"location":"fundamentals/type-recovery/var-detection/#code-examples","title":"Code Examples","text":"<p>Ghidra</p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Balakrishnan, Gogul, and Thomas Reps. \"Divine: Discovering variables in executables.\" International Workshop on Verification, Model Checking, and Abstract Interpretation. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>ElWazeer, Khaled et al. \"Scalable Variable and Data Type Detection in a Binary Rewriter.\" PLDI '13: Proceedings of the 34<sup>th</sup> ACM SIGPLAN Conference on Programming Language Design and Implementation. 2013.\u00a0\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011)\u00a0\u21a9</p> </li> <li> <p>M\u00a8uller-Olm, Markus and Helmut Seidl. \"Precise Interprocedural Analysis through Linear Algebra.\" POPL '04. 2004.\u00a0\u21a9</p> </li> <li> <p>Balakrishnan, Gogul, and Thomas Reps. Analyzing memory accesses in x86 executables. Comp Construct. 2004.\u00a0\u21a9</p> </li> <li> <p>M. Sharir and A. Pnueli. \"Two approaches to interprocedural data flow analysis\". In Program Flow Analysis: Theory and Applications, chapter 7, pages 189\u2013234. Prentice-Hall, 1981.\u00a0\u21a9</p> </li> </ol>"},{"location":"misc/blogs/","title":"Community Blogs","text":"<p>A collection of decompilation blogs from the community.</p>"},{"location":"misc/blogs/#legend","title":"Legend","text":"<ul> <li>\ud83d\udc80: inactive (2 years without activity)</li> <li>\ud83d\udcbe: created by the developer or maintainer of a standalone decompiler</li> <li>\ud83d\udd0d: has posts on fundamental topics</li> <li>\u2699\ufe0f: has posts on applied research topics</li> <li>\ud83c\udf0d: utilizes decompilation for some means </li> </ul>"},{"location":"misc/blogs/#alphabetical-order","title":"Alphabetical Order","text":"<ul> <li>angr blog (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> <li>Intranautic (\ud83d\udd0d)</li> <li>Kronotai (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> <li>mahaloz (\ud83d\udcbe, \ud83d\udd0d, \ud83c\udf0d)</li> <li>msm (\ud83d\udd0d, \ud83c\udf0d)</li> <li>fcd (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> <li>REC Blog (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> </ul>"},{"location":"misc/talks/","title":"Talks","text":"<p>Talks that are about decompilation techniques or applications. All talks listed should be either at a conference or university, where talks are reviewed.  Exceptions are made for especially impactful talks.</p>"},{"location":"misc/talks/#reverse-chronological-order","title":"Reverse Chronological Order","text":"<ul> <li>\"30 Years, From Compilation Student to Decompilation Pioneer.\" Christina Cifuentes. LABScon24.</li> <li>\"BTD: Unleashing the Power of Decompilation for x86 Deep Neural Network Executables.\" Zhibo Liu. Blackhat 2023.</li> <li>\u201cModern Approaches in Human-Centric Decompilation.\u201d Zion Leonahenahe Basque. Ohio State University 2023.</li> <li>\u201cBridging the gap in the static and dynamic analysis of binaries through decompiler tomfoolery!\u201d Zion Leonahenahe Basque. CactusCon 2023.</li> <li>\"30 Years into Scientific Binary Decompilation.\" Dr. Ruoyu (Fish) Wang. NDSS BAR 2022.</li> <li>\"Beyond the C: Retargetable Decompilation using Neural Machine Translation.\" Iman Hosseini. NDSS BAR 2022.</li> <li>\"How to Build A Decompiler\" Tsviatko Yovtchev. Devoxx belgium. 2019</li> <li>\"Recovering Meaningful Variable Names in Decompiled Code.\" Dr. Bogdan Vasilescu. CMU 2019.</li> <li>\"How not to write a decompiler\" Giovanni Dante Grazioli. r2con 2018</li> <li>\"Repsych: Psychological Warfare in Reverse Engineering.\" Chris Domas. DEF CON 2015.</li> </ul>"}]}