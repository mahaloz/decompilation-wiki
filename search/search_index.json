{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Decompilation Wiki","text":"<p>The Decompilation Wiki is a collection of categorized information on all things decompilation. From real-world applications to cutting-edge research papers, the Decompilation Wiki has it all! Join our Discord below for active community engagement. To get involved, see our contribution guide. The Decompilation Wiki is still early in development, so any contribution is appreciated! </p> <p></p>"},{"location":"#what-is-decompilation","title":"What Is Decompilation?","text":"<p>Interestingly, the term \"decompilation\" and its definition are still argued about by researchers. However, most people agree that decompilation is the reversal of compilation.  By that definition, decompilation is the process of turning low-level machine code into a higher-level representation.</p> <p>In many cases, this means turning machine code, like x86 assembly, into source code, like C. This methodology can also be applied to languages like Java, which create bytecode. The difficulty and accuracy of decompilation can vary per language target<sup>1</sup>.</p> <p>Decompilation has wide applications across cyber security, including:</p> <ul> <li>reverse engineering (the understanding of programs)</li> <li>vulnerability discovery (the understanding of program flaws)</li> <li>malware classification</li> <li>program repair</li> <li>and much more...</li> </ul>"},{"location":"#wiki-goals","title":"Wiki Goals?","text":"<p>This wiki has two main goals:</p> <ol> <li>Making decompilation knowledge more accessible to new-comers in the field</li> <li>Categorizing research and tooling to make future decompilation progress easier</li> </ol> <p>To accomplish the first goal, it is highly encouraged to link public code when adding a technique.  Additionally, we will store tutorials for self-rolling (to a degree) your own decompiler components.</p> <p>To accomplish the second goal, we will attempt to rapidly categorize new research and tools in the area. These categorizations may not be agreed upon at first, however, we will update them as the community hits consensus.  In this way, we can quickly attempt to taxonomize the area of decompilation while iterating on it.</p>"},{"location":"#who-made-this","title":"Who Made This?","text":"<p>The Decompilation Wiki was started by Zion Leonahenahe Basque, but is sustained by the contributions of the decompilation community. Both closed and open-source developers are welcome! </p> <p>The wiki is highly inspired by the following sources:</p> <ul> <li>Program-Transformation.org: a wiki on program transformations, including some decompilation.</li> <li>CTF Wiki: a wiki for Capture the Flag, inspiring this layout and design.</li> <li>\"30 Years into Scientific Binary Decompilation\", Dr. Ruoyu (Fish) Wang: a source of information on decompilers.</li> </ul> <p>Additionally, the wiki is due in large part to the support and advisement of Zion's PhD committee: Dr. Ruoyu (Fish) Wang, Dr. Yan Shoshitaishvili, Dr. Adam Doup\u00e9, and Dr. Christina Cifuentes. </p>"},{"location":"#decompilation-paper-list","title":"Decompilation Paper List","text":"<p>Across this wiki you will find many papers cited from across the scientific community.  For easy accessibiliy, you can find all cited papers here: Comprehensive Decompilation List.</p> <ol> <li> <p>Yakdan, Khaled, et al. \"Helping johnny to analyze malware: A usability-optimized decompiler and malware analysis user study.\" 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016.\u00a0\u21a9</p> </li> </ol>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for wanting to contribute to the Decompilation Wiki!  Below are some simple tips to make requests for changes easier to review for the community.  Currently, the wiki is still in early development, so any PR is welcome. </p> <p>If there are ever questions on good contributions, opening issues and discussion on Discord are the best mediums for feedback.</p>"},{"location":"contributing/#finding-places-to-help","title":"Finding Places to Help","text":"<p>Since the Decompilation Wiki aims to categorize and house a wide-ranging field, help is always needed to add and keep up with new content. If you are looking to contribute, but are not sure how the GitHub Issues listing is a great place to start. Any issue is a candidate for contribution, but ones marked as <code>help wanted</code> are likely the easiest to complete for newcomers. </p> <p>Additionally, the wiki has a series of decompilation-related listings that constantly need unknown items added to it.  Some include:</p> <ul> <li>full decompilers</li> <li>tools </li> <li>blogs </li> <li>talks</li> </ul> <p>If you are aware of any unlisted items in these, it is highly appreciated for you to make a PR to add them. </p>"},{"location":"contributing/#update-guide","title":"Update Guide","text":"<p>To make a change, open a PR on the GitHub repo after forking and making a branch. There are two types of changes you might make while contributing to the wiki:</p> <ol> <li>Adding a listing to a new decompiler/tool/blog/...</li> <li>Updating a section regarding fundamental or applied research</li> </ol> <p>The best way to make PRs for these types of changes are different and found below.</p>"},{"location":"contributing/#tool-listing-change","title":"Tool Listing Change","text":"<p>When making changes to this area try to include three things: 1. A link to any high-level description of it (papers are ok). 2. Tags relevant to the tool (see sections like Community Blogs) 3. Tool source link if open-source </p> <p>PRs in this category require no justification and will very likely be merged after a simple review of the content. </p>"},{"location":"contributing/#research-area-update","title":"Research Area Update","text":"<p>Changes to research areas require a little more than tool changes. If the change is a simple typo, no justification is needed and the PR will be accepted.</p> <p>However, if the change is more content-based, then it will require more explanations.  When making changes here, it will be important to either include a citation or a well-thought-out argument against current wording/posturing. Any links to community discussions are always appreciated, such as Reddit threads, Twitter threads, or blogs. </p> <p>Additionally, any PRs that require larger discussion will likely be tied to a Discussion post on GitHub. </p>"},{"location":"contributing/#developing-locally","title":"Developing Locally","text":"<p>The Decompilation Wiki is made with mkdocs. The best way to make changes to the wiki is to get a local version running.</p> <p>Simply install mkdocs and build the site: <pre><code>pip install mkdocs\nmkdocs serve\n</code></pre></p> <p>The site should be launched on http://127.0.0.1:8000/. </p> <p>Thank you for your efforts to improve the Decompilation Wiki!</p>"},{"location":"applications/overview/","title":"Decompiler Applications","text":"<p>Across the internet, there are many ways people have used decompilers in the wild. In this section, you can find a collection of some of those use cases.</p> <p>As an example, some decompilation uses include: </p> <ul> <li>Program Reversing: used to understand how a program works and how to interact with it.</li> <li>Program Reconstruction: used to completely (or partially) recompile the targeted binary</li> <li>Automated Program Repair: used for patching faulty programs</li> <li>Manual &amp; Automated Vulnerability Discovery: used for finding vulnerabilities</li> </ul> <p>For links to full decompilers, see the Decompilers section. </p>"},{"location":"applications/program-reconstruction/","title":"Program Reconstruction","text":"<p>When source code is unavailable for a compiled program, users may want to recover the source code, so they can make edits to it and recompile it.  In the video game scene, this can be useful for modding. This can also be useful for porting a program to a new platform. </p>"},{"location":"applications/program-reconstruction/#video-games","title":"Video Games","text":"<p>Reverse engineers whom also love playing video games often reverse their favorite games.  In some cases, they go as far as attempting to recompile the entire project.  However, to recompile the project, they first need to recover compilable code.  In these cases, practitioners often use a decompiler to first get pseudo-C, then modify it to make it compilable. </p> <p>The end goal of these projects is to recover program source that will recompile to a byte-match of the original binary.  Most projects include a percent completion of the estimated program recompilation.</p> <p>Many of the games in this list were collected from GitHub projects, individuals, or popular blog posts <sup>1</sup>.</p> Game Original Platform Completed Paper Mario: Sticker Star 3DS Super Mario 3D Land 3DS The Legend of Zelda: Ocarina of Time 3D 3DS Ambermoon Amiga \u2705 The Settlers I DOS \u2705 Tokyo Bus Guide Dreamcast Donkey Kong '94 GB \u2705 Kirby's Dream Land GB \u2705 Metroid II: Return of Samus GB Pok\u00e9mon Red and Blue GB \u2705 Pok\u00e9mon Yellow GB \u2705 Super Mario Land GB \u2705 Super Mario Land 3: Wario Land GB \u2705 Banjo-Kazooie: Grunty's Revenge GBA Breath of Fire GBA Fire Emblem: The Binding Blade GBA Fire Emblem: The Sacred Stones GBA Golden Sun GBA Harvest Moon: Friends of Mineral Town GBA Kirby &amp; The Amazing Mirror GBA Metroid - Zero Mission GBA Pok\u00e9mon Emerald GBA Pok\u00e9mon FireRed and LeafGreen GBA Pok\u00e9mon Mystery Dungeon: Red Rescue Team GBA Pok\u00e9mon Pinball: Ruby and Sapphire GBA Pok\u00e9mon Ruby and Sapphire GBA Sonic Advance 2 GBA Summon Night Swordcraft Story 3 GBA Super Mario Advance 2: Super Mario World GBA The Legend of Zelda: Minish Cap GBA \u2705 Links Awakening DX GBC Pok\u00e9mon Crystal GBC \u2705 Pok\u00e9mon Gold and Silver GBC \u2705 Pok\u00e9mon Pinball GBC \u2705 The Legend of Zelda: Link's Awakening DX HD GBC \u2705 Wario Land 3 GBC \u2705 Animal Crossing GameCube Animal Forest e+ GameCube Chibi-Robo: PIA GameCube Doshin the Giant GameCube Harvest Moon: A Wonderful Life GameCube Homeland GameCube Kirby Air Ride GameCube Luigi's Mansion GameCube Mario Kart: Double Dash!! GameCube Mario Party 4 GameCube \u2705 Mario Party 5 GameCube Mario Party 6 GameCube Mario Party 7 GameCube Mario Superstar Baseball GameCube Metroid Prime 1 GameCube Metroid Prime 2 GameCube Naruto: Gekit\u014d Ninja Taisen! 4 GameCube Need for Speed: Most Wanted GameCube Need for Speed: Underground GameCube Paper Mario: The Thousand-Year Door GameCube Pikmin 1 GameCube Pikmin 2 GameCube Ratatouille GameCube Rocket: Robot on Wheels GameCube Skies of Arcadia Legends GameCube Sonic Adventure DX GameCube Sonic Riders GameCube SpongeBob SquarePants: Battle for Bikini Bottom GameCube Star Fox Adventures GameCube Summoner: A Goddess Reborn GameCube Super Mario Strikers GameCube Super Mario Sunshine GameCube Super Smash Bros. Melee GameCube The Incredibles GameCube The Legend of Zelda: The Wind Waker GameCube The Legend of Zelda: Twilight Princess GameCube The SpongeBob SquarePants Movie GameCube Ty the Tasmanian Tiger GameCube Phantasy Star I MasterSystem Phantasy Star II MegaDrive Phantasy Star III MegaDrive Phantasy Star IV MegaDrive Ristar MegaDrive Streets of Rage 2 MegaDrive \u2705 AeroGauge N64 Aidyn Chronicles: The First Mage N64 Animal Forest N64 Banjo-Kazooie N64 \u2705 Banjo-Tooie N64 Blast Corps N64 Body Harvest N64 Bomberman 64 N64 Bomberman 64: The Second Attack! N64 Bomberman Hero N64 Castlevania 64 N64 Conker's Bad Fur Day N64 Chameleon Twist 1 N64 Chameleon Twist 2 N64 Dark Rift N64 Diddy Kong Racing N64 Dinosaur Planet N64 Donkey Kong 64 N64 Doom 64 N64 \u2705 Doraemon: Nobita to Mittsu no Seireiseki N64 Dr. Mario 64 N64 Duke Nukem 64 N64 Duke Nukem: Zero Hour N64 \u2705 F-Zero X N64 F-Zero X Expansion Kit N64 Gauntlet Legends N64 Gex 64: Enter the Gecko N64 Glover N64 Goldeneye 007 N64 Harvest Moon 64 N64 Jet Force Gemini N64 Kirby 64: The Crystal Shards N64 Lego Racers N64 Mario Golf N64 Mario Kart 64 N64 \u2705 Mario Party 1 N64 Mario Party 2 N64 Mario Party 3 N64 Mario Tennis N64 Mischief Makers N64 Mystical Ninja Starring Goemon N64 Neon Genesis Evangelion 64 N64 Onegai Monsters N64 Paper Mario N64 \u2705 Perfect Dark N64 \u2705 Pokemon Puzzle League N64 Pokemon Snap N64 Pokemon Stadium N64 Pokemon Stadium 2 N64 Quest 64 N64 Shadowgate 64 N64 Space Station Silicon Valley N64 Star Fox 64 N64 Star Wars: Shadows of the Empire N64 Super Mario 64 N64 \u2705 Super Smash Bros. N64 Superman 64 N64 The Legend of Zelda: Majora's Mask N64 \u2705 The Legend of Zelda: Ocarina of Time N64 \u2705 The New Tetris N64 Virtual Pool 64 N64 Virtual Pro Wrestling 2 N64 Wave Race 64 N64 Yoshi's Story N64 Castlevania: Order of Ecclesia NDS Dragon Quest IX NDS Mario &amp; Luigi - Partners in Time NDS Mario Party DS NDS Pok\u00e9mon Diamond and Pearl NDS Pok\u00e9mon HeartGold and SoulSilver NDS Rhythm Heaven NDS The Legend of Zelda: Phantom Hourglass NDS The Legend of Zelda: Spirit Tracks NDS Touhou Project 1 NEC PC-9800 \u2705 Touhou Project 2 NEC PC-9800 Touhou Project 3 NEC PC-9800 Touhou Project 4 NEC PC-9800 Touhou Project 5 NEC PC-9800 Zelda II: The Adventure of Link NES \u2705 Carmageddon PC Deus Ex: Human Revolution - Director's Cut PC Lego Island PC \u2705 Sonic Mania PC Spider-Man (Neversoft) PC Castlevania: Symphony of the Night PS1 Crash Bandicoot PS1 Crash Bandicoot 2: Cortex Strikes Back PS1 Crash Team Racing PS1 Croc: Legend of the Gobbos PS1 Doom PS1 Driver 2 PS1 \u2705 Final Fantasy VII PS1 Legacy of Kain: Soul Reaver PS1 Legend of Dragoon PS1 \u2705 Legend of Legaia PS1 MediEvil 1 PS1 Metal Gear Solid PS1 R4: Ridge Racer Type 4 PS1 Shin Megami Tensei PS1 Silent Hill PS1 Spyro the Dragon PS1 Tomb Raider 1 PS1 \u2705 Tomb Raider 1-5 PS1 Tomba! PS1 Vagrant Story PS1 Vandal Hearts PS1 Xenogears PS1 Dark Cloud PS2 Fatal Frame 1 PS2 Fatal Frame 2: Crimson Butterfly PS2 Ico PS2 Jak II PS2 \u2705 Jak and Daxter: The Precursor Legacy PS2 \u2705 Kingdom Hearts PS2 Klonoa 2: Lunatea's Veil PS2 Metal Gear Solid 2: Sons of Liberty PS2 PaRappa the Rapper 2 PS2 Resident Evil - Code: Veronica X PS2 Sly Cooper and the Thievius Raccoonus PS2 Street Fighter III: 3<sup>rd</sup> Strike PS2 \u2705 Twisted Metal: Black PS2 Xenosaga Episode 1: Der Wille zur Macht PS2 Yakuza PS2 Demon's Crest SNES Donkey Kong Country 1 SNES Donkey Kong Country 2 SNES Donkey Kong Country 3 SNES Earthbound / Mother 2 SNES Final Fantasy IV SNES Final Fantasy VI SNES Goof Troop SNES Super Ghouls 'n Ghosts SNES Super Mario RPG SNES Super Mario World SNES Super Mario World 2: Yoshi's Island SNES \u2705 Super Metroid SNES \u2705 Super Punch-Out!! SNES The Legend of Zelda: A Link to the Past SNES \u2705 Minecraft: Nintendo Switch Edition Switch Super Mario 3D World + Bowser's Fury Switch Super Mario Odyssey Switch The Legend of Zelda: Breath of the Wild Switch Inazuma Eleven Strikers Wii Kirby's Epic Yarn Wii Mario Kart Wii Wii Mario Party 8 Wii Mario Party 9 Wii Pokemon Battle Revolution Wii Pokepark Wii: Pikachu's Adventure Wii Super Mario Galaxy Wii Super Mario Galaxy 2 Wii Super Paper Mario Wii Super Smash Bros. Brawl Wii The Legend of Zelda: Skyward Sword Wii Wii Sports Wii Xenoblade Chronicles Wii New Super Mario Bros. U Wii U Halo: Combat Evolved Xbox Minecraft: Xbox 360 Edition Xbox 360 Minecraft: Pocket Edition (2011) iOS \u2705 <ol> <li> <p>https://www.resetera.com/threads/decompilation-projects-ot-free-next-gen-update-for-your-favorite-classics-jak-ii-pc-port-out-in-beta.682687/ \u21a9</p> </li> </ol>"},{"location":"applied-research/code-similarity/","title":"Code Similarity","text":""},{"location":"applied-research/code-similarity/#introduction","title":"Introduction","text":"<p>In cases such as malware identification, the ability to estimate code similarity among binaries is critical<sup>1</sup>. Research in this area generally looks at ways to improve the reliability of similarity detection among binaries. </p> <p>There is little work in the direct use of decompilation for code similarity, however, the general work in the binary analysis is frequent.  These works are included here since they often touch on or improve fundamental components in decompilation. </p> <p>The most direct research in this area has utilized Ghidra decompilation to identify inlined functions in decompilation<sup>2</sup>. </p>"},{"location":"applied-research/code-similarity/#related-works","title":"Related Works","text":"<p>Many works have progressed towards binary-based code similarity that do not explicitly use decompilation <sup>1</sup><sup>3</sup><sup>4</sup><sup>5</sup><sup>6</sup>. Most of these works have improved code similarity techniques indirectly by improving it for their specific uses cases.  These uses have included malware identification<sup>1</sup>, duplicated bug hunting<sup>3</sup><sup>4</sup>, and code reuse<sup>5</sup>.</p> <p>Recent work has suggested that machine learning has made significant strides in this area<sup>6</sup>.</p> <ol> <li> <p>Hu, Xin, Tzi-cker Chiueh, and Kang G. Shin. \"Large-scale malware indexing using function-call graphs.\" Proceedings of the 16<sup>th</sup> ACM conference on Computer and communications security. 2009.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Ahmed, Toufique, Premkumar Devanbu, and Anand Ashok Sawant. \"Finding Inlined Functions in Optimized Binaries.\" arXiv preprint arXiv:2103.05221 (2021).\u00a0\u21a9</p> </li> <li> <p>Feng, Qian, et al. \"Scalable graph-based bug search for firmware images.\" Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Eschweiler, Sebastian, Khaled Yakdan, and Elmar Gerhards-Padilla. \"Discovre: Efficient cross-architecture identification of bugs in binary code.\" Ndss. Vol. 52. 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Mirzaei, Omid, et al. \"Scrutinizer: Detecting code reuse in malware via decompilation and machine learning.\" Detection of Intrusions and Malware, and Vulnerability Assessment: 18<sup>th</sup> International Conference, DIMVA 2021, Virtual Event, July 14\u201316, 2021, Proceedings 18. Springer International Publishing, 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>Marcelli, Andrea, et al. \"How machine learning is solving the binary function similarity problem.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"applied-research/overview/","title":"Applied Research Overview","text":"<p>Decompiler research that does not neatly fit into one of the fundamental areas is defined here as applied research. Research in this area contributes to a specific use-case of decompilation that may not necessarily improve base decompilation.</p> <p>As an example, most researchers would agree that variable name prediction in stripped binaries is an important research area<sup>1</sup>.  However, as it stands, variable name prediction does not improve any fundamental research area (except neural decompilation). As such, we consider it an applied research area, with that target being human-comprehensible decompilation. </p> <p>This section is ever-growing as new research areas are explored in decompilation.  Currently, the following areas exist:</p> <ul> <li>Symbol Recovery: recovering the names or high-level symbols that are associated with a function or variable</li> <li>Code Similarity: measuring how similar (for various uses) some binary is to another</li> <li>Vulnerability Discovery: tuning decompilation to be better used for vulnerability discovery</li> </ul>"},{"location":"applied-research/overview/#other-research","title":"Other Research","text":"<p>Some research areas don't have enough work to define a label for them. The following works are listed here:</p> <ul> <li>Byte-exact recompilable decompilation<sup>2</sup></li> <li>Patchable decompilation<sup>3</sup></li> <li>Verifiable decompilation<sup>4</sup></li> <li>Higher abstraction support<sup>5</sup><sup>6</sup><sup>7</sup></li> </ul> <ol> <li> <p>Pal, Kuntal Kumar, et al. \"\"Len or index or count, anything but v1\": Predicting Variable Names in Decompilation Output with Transfer Learning.\" 2024 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 2024.\u00a0\u21a9</p> </li> <li> <p>Schulte, Eric, et al. \"Evolving exact decompilation.\" Workshop on Binary Analysis Research (BAR). 2018.\u00a0\u21a9</p> </li> <li> <p>Reiter, Pemma, et al. \"Automatically mitigating vulnerabilities in x86 binary programs via partially recompilable decompilation.\" arXiv preprint arXiv:2202.12336 (2022).\u00a0\u21a9</p> </li> <li> <p>Verbeek, Freek, Pierre Olivier, and Binoy Ravindran. \"Sound C Code Decompilation for a subset of x86-64 Binaries.\" Software Engineering and Formal Methods: 18<sup>th</sup> International Conference, SEFM 2020, Amsterdam, The Netherlands, September 14\u201318, 2020, Proceedings 18. Springer International Publishing, 2020.\u00a0\u21a9</p> </li> <li> <p>Fokin, Alexander, et al. \"SmartDec: approaching C++ decompilation.\" 2011 18<sup>th</sup> Working Conference on Reverse Engineering. IEEE, 2011.\u00a0\u21a9</p> </li> <li> <p>Wu, Ruoyu, et al. \"{DnD}: A {Cross-Architecture} deep neural network decompiler.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Liu, Zhibo, et al. \"Decompiling x86 deep neural network executables.\" 32<sup>nd</sup> USENIX Security Symposium (USENIX Security 23). 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"applied-research/symbol-recovery/","title":"Symbol Recovery","text":""},{"location":"applied-research/symbol-recovery/#introduction","title":"Introduction","text":"<p>A symbol, in the context of binaries, is a name associated with an object. In most cases, this is either function names or variable names.  It is often useful for reverse engineering to have the original symbols to more quickly understand the purpose of an object. </p>"},{"location":"applied-research/symbol-recovery/#symbol-recovery-example","title":"Symbol Recovery Example","text":"<p>Below is a snippet of a C program: <pre><code>int mode;\nchar* name;\nlong long timezone;\n</code></pre></p> <p>After compiling and stripping, a common developer practice, the binary will be decompiled to something like: <pre><code>int v1;\nchar* v2;\nlong long v3;\n</code></pre></p> <p>Assuming the types are recovered perfectly (hard), it is still hard to understand what these variables do. </p>"},{"location":"applied-research/symbol-recovery/#previous-work","title":"Previous Work","text":"<p>Research in this area has been concerned with the recovery of both variable names<sup>1</sup><sup>2</sup><sup>3</sup><sup>4</sup><sup>5</sup><sup>6</sup> and function names<sup>7</sup><sup>4</sup>.  Approaches have varied between using neural networks<sup>2</sup><sup>7</sup><sup>5</sup><sup>6</sup>, machine translation<sup>3</sup>, probabilistic methods<sup>4</sup>, and BERT-based language models<sup>1</sup>. In many cases, the bottleneck of this work has been dataset generation<sup>1</sup>.</p> <ol> <li> <p>Pal, Kuntal Kumar, et al. \"\"Len or index or count, anything but v1\": Predicting Variable Names in Decompilation Output with Transfer Learning.\" 2024 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Dramko, Luke, et al. \"DIRE and its data: Neural decompiled variable renamings with respect to software class.\" ACM Transactions on Software Engineering and Methodology 32.2 (2023): 1-34.\u00a0\u21a9\u21a9</p> </li> <li> <p>Jaffe, Alan, et al. \"Meaningful variable names for decompiled code: A machine translation approach.\" Proceedings of the 26<sup>th</sup> Conference on Program Comprehension. 2018.\u00a0\u21a9\u21a9</p> </li> <li> <p>He, Jingxuan, et al. \"Debin: Predicting debug information in stripped binaries.\" Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. 2018.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>DIRE: A Neural Approach to Decompiled Identifier Naming\u00a0\u21a9\u21a9</p> </li> <li> <p>Chen, Qibin, et al. \"Augmenting decompiler output with learned variable names and types.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>Artuso, Fiorella, et al. \"Function naming in stripped binaries using neural networks.\" arXiv preprint arXiv:1912.07946 (2019).\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"applied-research/vulnerability-discovery/","title":"Vulnerability Discovery","text":"<p>In many uses of decompilation, humans, or machines, aim to understand if a program is safe. To verify if this program is safe, they attempt to do the opposite: find vulnerabilities in the program. Some decompilers, and their associated research, have attempted to tune their decompilers to be better at this task<sup>1</sup>. There has also been work at evaluating decompilers by how well they perform with source tools<sup>2</sup>. </p> <p>Most research in this area has focused on static analysis<sup>1</sup><sup>2</sup><sup>3</sup> and symbolic execution<sup>4</sup> applied to decompilation. Since these tasks have often been researched with source, an application to binaries has been achieved through decompilation. </p> <ol> <li> <p>Botacin, Marcus, et al. \"Revenge is a dish served cold: Debug-oriented malware decompilation and reassembly.\" Proceedings of the 3<sup>rd</sup> Reversing and Offensive-oriented Trends Symposium. 2019.\u00a0\u21a9\u21a9</p> </li> <li> <p>Mantovani, Alessandro, et al. \"The Convergence of Source Code and Binary Vulnerability Discovery--A Case Study.\" Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security. 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>Park, Jihee, et al. \"Static Analysis of JNI Programs via Binary Decompilation.\" IEEE Transactions on Software Engineering (2023).\u00a0\u21a9</p> </li> <li> <p>Han, HyungSeok, et al. \"QueryX: Symbolic Query on Decompiled Code for Finding Bugs in COTS Binaries.\" 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"decompilers/directory/","title":"Decompiler Directory","text":"<p>Over the years, many decompilers have been made by both hackers and academics alike.  In this directory, you will find a listing of all known (at least to the wiki authors) decompilers. </p> <p>Each decompiler should also be listed with some minimal facts about their differences. </p>"},{"location":"decompilers/directory/#legend","title":"Legend","text":"<ul> <li>\ud83c\udf10: open-source</li> <li>\ud83d\udc80: inactive (2 years without activity)</li> <li>0\ufe0f\u20e3: binary decompiler (compiled languages)</li> <li>\ud83d\udd0c: implemented as a plugin to another decompiler</li> </ul>"},{"location":"decompilers/directory/#alphabetical-order","title":"Alphabetical Order","text":"Decompiler Attributes Intermediate Language Release Date angr decompiler 0\ufe0f\u20e3, \ud83c\udf10 VEX, P-Code, AIL 2018 Binary Ninja 0\ufe0f\u20e3 BNIL 2015 Boomerang 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80 ? 2008 CFR \ud83c\udf10 ? 2012 Cutter 0\ufe0f\u20e3, \ud83c\udf10 ? 2017 dewolf 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udd0c BNIL 2021 DREAM 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80, \ud83d\udd0c Micro Code 2015 Fernflower \ud83c\udf10 ? 2009 Ghidra 0\ufe0f\u20e3, \ud83c\udf10 P-Code 2019 Hopper 0\ufe0f\u20e3 ? 2012 IDA Pro (HexRays) 0\ufe0f\u20e3 Micro Code 2007 ILSpy \ud83c\udf10 ? 2007 JEB 0\ufe0f\u20e3 ? 2014 Procyon \ud83c\udf10, \ud83d\udc80 ? 2012 REC Studio 0\ufe0f\u20e3, \ud83d\udc80 ? 1997 Reko 0\ufe0f\u20e3, \ud83c\udf10 ? 2007 Relyze 0\ufe0f\u20e3, \ud83d\udc80 ? 2015 RetDec 0\ufe0f\u20e3, \ud83c\udf10 LLVM IR 2017 rev.ng 0\ufe0f\u20e3, \ud83c\udf10 TCG, LLVM IR 2017 r2dec 0\ufe0f\u20e3, \ud83c\udf10 ? 2019 Snowman 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80 ? 2015 fcd 0\ufe0f\u20e3, \ud83c\udf10, \ud83d\udc80 ? 2015"},{"location":"decompilers/history/","title":"History","text":""},{"location":"decompilers/tools/","title":"Tools","text":"<p>The community continues to extend decompilers outside of fundamental improvements in the form of plugins and tools. Here you can find a listing of tools and plugins used by the community. </p>"},{"location":"decompilers/tools/#generic-tools","title":"Generic Tools","text":"<p>Tools that work in most popular decompilers.</p> <ul> <li>BinDiff: A decompiler-based diffing tool for binaries.</li> <li>BinSync: A Git-based collaboration framework for decompilers. Supports IDA, Ghidra, Binja, and angr decompiler.</li> <li>DogBolt: A web-based tool for comparing popular decompiler's decompilation.</li> <li>RevSync: A synchronization tool for decompilers. Supports IDA &amp; Binja. </li> </ul>"},{"location":"decompilers/decompiler/angr/","title":"angr decompiler","text":""},{"location":"decompilers/decompiler/binary_ninja/","title":"Binary Ninja","text":""},{"location":"decompilers/decompiler/ghidra/","title":"Ghidra","text":""},{"location":"decompilers/decompiler/ida_pro/","title":"IDA Pro (HexRays)","text":""},{"location":"fundamentals/evaluation/","title":"Quality Evaluation","text":""},{"location":"fundamentals/evaluation/#introduction","title":"Introduction","text":"<p>The best way to measure the quality of overall decompilation is still an open problem in decompilation research. However, there have been a variety of methods for evaluating the quality of individual fundamental components in decompilation.</p>"},{"location":"fundamentals/evaluation/#control-flow-structuring-metrics","title":"Control Flow Structuring Metrics","text":"<p>Early evaluations in control flow structuring utilized metrics from software engineering.  As such, these metrics were not compared to their original source, but instead used standalone.  This was useful because these metrics could be computed in real-world scenarios of decompilation use and reduced in runtime.  The following early metrics were used:</p> <ul> <li>Gotos<sup>15</sup><sup>13</sup><sup>16</sup><sup>17</sup><sup>1</sup></li> <li>Full-function Recompilability<sup>13</sup></li> <li>Lines of Code (LoC)<sup>17</sup></li> <li>McCabe Cyclomatic Complexity (MCC)<sup>1</sup></li> </ul> <p>In all but the case of function recompilability, the decompiler would want to optimize for reducing all of these metrics. However, recent work has argued against the pure reduction of these metrics<sup>1</sup>.  The SAILR paper<sup>1</sup>, argues that these metrics should be measured relative to its source (like machine translation). The following metrics were used as compared to source:</p> <ul> <li>Gotos</li> <li>Boolean Expression</li> <li>Function Calls</li> <li>Graph Edit Distance (GED)</li> </ul> <p>In each metric, the score closest to the source is the best structured decompilation.  Since these metrics can only be used with knowledge of source, they can only be used when developing a decompiler (not optimized in runtime).</p>"},{"location":"fundamentals/evaluation/#variable-typing-metrics","title":"Variable Typing Metrics","text":"<p>Unlike control flow structuring metrics, most evaluations for variable typing have focused on comparing choices to the source<sup>2</sup><sup>3</sup><sup>4</sup><sup>5</sup>. Generally, these evaluations would flow like the following:</p> <ol> <li>Extract the ground-truth types from the source (usually with DWARF info)</li> <li>Guess variables and types from decompilation</li> <li>See how often (or how close) each chosen variable location and type was to source</li> </ol> <p>Some evaluations have also measured how conservative they were at approximating types<sup>3</sup>.</p>"},{"location":"fundamentals/evaluation/#human-evaluation","title":"Human Evaluation","text":"<p>Some work has utilized human evaluations to understand the quality of decompilation<sup>6</sup><sup>7</sup>.  These works have focused on qualitative metrics, such as users' perception of the code's complexity<sup>6</sup>.  Additionally, the Decomperson<sup>8</sup> paper studied how humans might make decompilation themselves when given assembly.</p> <p>Since how humans rate decompilation is related to how they reverse engineer, some work has explored how humans reverse and exploit binaries. These studies have mainly focused on how hackers use tooling and techniques for reverse engineering<sup>9</sup><sup>10</sup>. </p>"},{"location":"fundamentals/evaluation/#other-general-metrics","title":"Other General Metrics","text":"<p>Some work has constructed a taxonomy of decompiler-to-source errors<sup>11</sup>. That work categorized and identified many errors that occur when decompiling a binary. </p> <p>Other work has identified general methods by which we may test the correctness of decompilation<sup>12</sup>. Relatedly, some structuring work has attempted to measure this through recompiliability<sup>13</sup>. </p>"},{"location":"fundamentals/evaluation/#dataset-generation","title":"Dataset Generation","text":"<p>The question of \"what dataset?\" to use has also been an issue in decompilation. Additionally, compiled binaries can sometimes be hard to access. Some related work has explored ways to generate bigger binary datasets for the evaluation of binary tools like decompilers<sup>14</sup>.</p> <ol> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Noonan, Matt, Alexey Loginov, and David Cok. \"Polymorphic type inference for machine code.\" Proceedings of the 37<sup>th</sup> ACM SIGPLAN Conference on Programming Language Design and Implementation. 2016.\u00a0\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011)\u00a0\u21a9\u21a9</p> </li> <li> <p>Zhang, Zhuo, et al. \"Osprey: Recovery of variable and data structure via probabilistic analysis for stripped binary.\" 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Chen, Qibin, et al. \"Augmenting decompiler output with learned variable names and types.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"Helping johnny to analyze malware: A usability-optimized decompiler and malware analysis user study.\" 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Enders, Steffen, et al. \"dewolf: Improving Decompilation by leveraging User Surveys.\" arXiv preprint arXiv:2205.06719 (2022).\u00a0\u21a9</p> </li> <li> <p>Decomperson: How Humans Decompile and What We Can Learn From It\u00a0\u21a9</p> </li> <li> <p>Nosco, Timothy, et al. \"The industrial age of hacking.\" 29<sup>th</sup> USENIX Security Symposium (USENIX Security 20). 2020.\u00a0\u21a9</p> </li> <li> <p>Mantovani, Alessandro, et al. \"{RE-Mind}: a First Look Inside the Mind of a Reverse Engineer.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Dramko, Luke, et al. \"A Taxonomy of C Decompiler Fidelity Issues.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9</p> </li> <li> <p>How far we have come: Testing decompilation correctness of C decompilers\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Singhal, Vidush, et al. \"Cornucopia: A Framework for Feedback Guided Generation of Binaries.\" Proceedings of the 37<sup>th</sup> IEEE/ACM International Conference on Automated Software Engineering. 2022.\u00a0\u21a9</p> </li> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"fundamentals/neural-decompilation/","title":"Neural Decompilation","text":""},{"location":"fundamentals/neural-decompilation/#introduction","title":"Introduction","text":"<p>This wiki defines a neural decompiler as the following:</p> <ul> <li>Any decompiler that wholly replaces one or many fundamental decompiler components with a machine-learning model.</li> </ul> <p>As such, decompilation produced by a neural decompiler is neural decompilation. Approaches in this area are promising because they often offer a more generic solution to decompilation.  Instead of a decompiler developer having to program a component, the component is learned from binaries and their corresponding source<sup>1</sup>. </p>"},{"location":"fundamentals/neural-decompilation/#previous-works","title":"Previous Works","text":"<p>The earliest academic work in this area is the 2018 work \"Using Recurrent Neural Networks for Decompilation\"<sup>1</sup>.  The work utilized a Recurrent Neural Network (RNN) to replace all components downstream from CFG recovery, omitting the lifting phase to train on x86 assembly.  Subsequent work also focused on x86, while changing their method for decompilation to Neural Machine Translation<sup>2</sup><sup>3</sup><sup>4</sup><sup>5</sup><sup>6</sup> and a more refined RNN<sup>7</sup>. The most reliable of these works, Coda, claims an average of \"82% program recovery accuracy on unseen binary sample(s)\"<sup>7</sup>. The notable Beyond-The-C, also introduces early work to turn binary code into multiple languages other than C <sup>6</sup>.</p> <p>With the recent popularization of Large Language Models (LLM), like ChatGPT, early work has utilized LLMs for end-to-end decompilation<sup>8</sup>.  Other related work in the area has used LLMs to improve the structured output of other decompilers<sup>9</sup>. </p> <ol> <li> <p>Katz, Deborah S., Jason Ruchti, and Eric Schulte. \"Using recurrent neural networks for decompilation.\" 2018 IEEE 25<sup>th</sup> International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2018.\u00a0\u21a9\u21a9</p> </li> <li> <p>Katz, Omer, et al. \"Towards neural decompilation.\" arXiv preprint arXiv:1905.08325 (2019).\u00a0\u21a9</p> </li> <li> <p>Liang, Ruigang, et al. \"Neutron: an attention-based neural decompiler.\" Cybersecurity 4 (2021): 1-13.\u00a0\u21a9</p> </li> <li> <p>Liang, Ruigang, et al. \"Semantics-recovering decompilation through neural machine translation.\" arXiv preprint arXiv:2112.15491 (2021).\u00a0\u21a9</p> </li> <li> <p>Cao, Ying, et al. \"Boosting neural networks to decompile optimized binaries.\" Proceedings of the 38<sup>th</sup> Annual Computer Security Applications Conference. 2022.\u00a0\u21a9</p> </li> <li> <p>Hosseini, Iman, and Brendan Dolan-Gavitt. \"Beyond the C: Retargetable decompilation using neural machine translation.\" arXiv preprint arXiv:2212.08950 (2022).\u00a0\u21a9\u21a9</p> </li> <li> <p>Fu, Cheng, et al. \"Coda: An end-to-end neural program decompiler.\" Advances in Neural Information Processing Systems 32 (2019).\u00a0\u21a9\u21a9</p> </li> <li> <p>Tan, Hanzhuo, et al. \"LLM4Decompile: Decompiling Binary Code with Large Language Models.\" arXiv preprint arXiv:2403.05286 (2024).\u00a0\u21a9</p> </li> <li> <p>Hu, Peiwei, Ruigang Liang, and Kai Chen. \"DeGPT: Optimizing Decompiler Output with LLM.\"\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/overview/","title":"Overview","text":""},{"location":"fundamentals/overview/#introduction","title":"Introduction","text":"<p>Modern decompilation is built on many techniques from both binary analysis and classic compilation algorithms.  Fundamental research in this area focuses on improving base decompilation across all languages.</p> <p>Some academic work has defined fundamental decompilation research to include three areas<sup>1</sup>, this wiki includes an extra fourth one:</p> <ol> <li>Control Flow Graph Recovery: the extraction of (lifted) directed graphs indicating code execution</li> <li>Type Recovery: the typing and discovery of variables in the program</li> <li>Control Flow Structuring: the conversion of a CFG to a linear code-like output</li> <li>Quality Evaluation: the measurement of overall decompilation quality</li> </ol> <p>Each of these fundamental areas affects the quality of one another in some way. For instance, improvements to type recovery can directly improve the results of control flow structuring<sup>2</sup>. With this in mind, we included the fourth area, quality evaluation in decompilation, as fundamental. We include this area because it influences the methodologies for the previous three areas and few works have standardized on metrics<sup>1</sup><sup>3</sup><sup>4</sup>.</p> <p>Other works, such as function name recovery can be found in the Applied Research section.</p>"},{"location":"fundamentals/overview/#generic-decompilation-pipeline","title":"Generic Decompilation Pipeline","text":"<p>Modern decompilers are comprised of fundamental components that can be directly mapped to each fundamental research area. Most decompilers follow a pipeline flow that resembles the following<sup>4</sup>:</p> <p></p> <p>The evaluation component is optional but has been used in the past for on-the-fly decision-making in other components like control flow structuring<sup>5</sup>. Each component can be implemented at various levels, such as the optional lifting phase in control flow recovery. </p> <p>In the case of neural decompilation, most components are replaced by the machine learning model. </p> <ol> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy SAILR! There is no need to DREAM of C: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011).\u00a0\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"Helping johnny to analyze malware: A usability-optimized decompiler and malware analysis user study.\" 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016.\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/disassembly/","title":"Disassembling","text":""},{"location":"fundamentals/cfg-recovery/disassembly/#introduction","title":"Introduction","text":"<p>Work in this area dates back to the 90s and can generally be broken up into two disassemblying techniques<sup>1</sup>:</p> <ol> <li>Linear Sweep<sup>3</sup>: starts disassembling from the first byte in the program and linearly continues</li> <li>Recursive Traversal<sup>5</sup>: disassembles by following control flow as it is discovered</li> </ol> <p>Linear sweep is considered to be more prone to errors in programs that embed data in the middle of their code[1].  Additionally, all disassembling techniques confront the main problem of Instruction boundary identification (IBI)<sup>2</sup>, which dictates when an instruction starts and ends.</p>"},{"location":"fundamentals/cfg-recovery/disassembly/#previous-works","title":"Previous Works","text":"<p>Relevant work for decompilation focused on improving both the accuracy and scalability of disassembling programs<sup>3</sup><sup>4</sup>.  Some of these works have improved these methods for work on deobfuscated binaries<sup>5</sup> as well as benign ones. Recent work in the area has focused on reassemblable disassembly<sup>2</sup><sup>6</sup>. This area is promising for decompilation as it makes re-compilable decompilation more achievable <sup>6</sup>. </p> <p>There has also been work to measure the accuracy of these disassembling frameworks by comparing their results to that of an instrumented compiler<sup>7</sup><sup>8</sup>. </p> <ol> <li> <p>Kruegel, Christopher, et al. \"Static disassembly of obfuscated binaries.\" USENIX security Symposium. Vol. 13. 2004.\u00a0\u21a9</p> </li> <li> <p>Flores-Montoya, Antonio, and Eric Schulte. \"Datalog disassembly.\" 29<sup>th</sup> USENIX Security Symposium (USENIX Security 20). 2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Free Software Foundation. GNU Binary Utilities, Mar 2002. https://www.gnu.org/software/binutils/manual/.\u00a0\u21a9\u21a9</p> </li> <li> <p>Andriesse, Dennis, et al. \"An {In-Depth} Analysis of Disassembly on {Full-Scale} x86/x64 Binaries.\" 25<sup>th</sup> USENIX security symposium (USENIX security 16). 2016.\u00a0\u21a9</p> </li> <li> <p>C. Cifuentes and K. Gough. Decompilation of Binary Programs. Software Practice &amp; Experience, 25(7):811-829, July 1995.\u00a0\u21a9\u21a9</p> </li> <li> <p>Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi, Aravind Machiry, John Grosen, Paul Grosen, Christopher Kruegel, and Giovanni Vigna. Ramblr: Making reassembly great again. In NDSS, 2017\u00a0\u21a9\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Ground truth for binary disassembly is not easy.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Sok: All you ever wanted to know about x86/x64 binary disassembly but were afraid to ask.\" 2021 IEEE symposium on security and privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/function-recovery/","title":"Function Identification","text":"<p>Function identification is used to identify the instruction boundaries of a function in the binary. For decompilation purposes, this serves as crucial information for the flow of code in the program.</p> <p>There have generally been two approaches:</p> <ol> <li>Pattern based (with weights)<sup>1</sup><sup>2</sup></li> <li>Machine-learning based<sup>3</sup></li> </ol> <p>Modern tools often utilize pattern-based techniques. </p> <ol> <li> <p>Bao, Tiffany, et al. \"{BYTEWEIGHT}: Learning to recognize functions in binary code.\" 23<sup>rd</sup> USENIX Security Symposium (USENIX Security 14). 2014.\u00a0\u21a9</p> </li> <li> <p>Andriesse, Dennis, Asia Slowinska, and Herbert Bos. \"Compiler-agnostic function detection in binaries.\" 2017 IEEE European symposium on security and privacy (EuroS&amp;P). IEEE, 2017.\u00a0\u21a9</p> </li> <li> <p>Shin, Eui Chul Richard, Dawn Song, and Reza Moazzezi. \"Recognizing functions in binaries with neural networks.\" 24<sup>th</sup> USENIX security symposium (USENIX Security 15). 2015.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/jump-resolving/","title":"Jump Resolving","text":"<p>After recovering a CFG from a binary, there may be calls that have an unknown target.  Indirect jump resolving is the process of discovering some, or all, of the targets those jumps may go to.  A basic implementation of this is Switch statements resolving, since they are often compiled into indirect jumps <sup>1</sup>.</p> <p>The work in this area has mostly focused on ways to reduce the total set of analyzed pointers while doing pointer analysis<sup>2</sup><sup>3</sup>. Additionally, these works have used program knowledge to further reduce constant-reducible pointers, like those found in class initialization in C++<sup>2</sup>. </p>"},{"location":"fundamentals/cfg-recovery/jump-resolving/#indirect-jump-example","title":"Indirect Jump Example","text":"<p>A simple indirect jump looks like the following in x86: <pre><code>jmp [rax];\n</code></pre></p> <p>Since pointer analysis is considered hard, it may not be trivial to find the value of <code>rax</code>.</p> <ol> <li> <p>Brumley, David, et al. \"Native x86 decompilation using {Semantics-Preserving} structural analysis and iterative {Control-Flow} structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9</p> </li> <li> <p>Kim, Sun Hyoung, et al. \"Refining Indirect Call Targets at the Binary Level.\" NDSS. 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kim, Sun Hyoung, et al. \"Binpointer: towards precise, sound, and scalable binary-level pointer analysis.\" Proceedings of the 31<sup>st</sup> ACM SIGPLAN International Conference on Compiler Construction. 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/lifting/","title":"Program Lifting","text":""},{"location":"fundamentals/cfg-recovery/lifting/#introduction","title":"Introduction","text":"<p>In program lifting, disassembly is converted into an intermediate language (IL)<sup>1</sup>, which is also referred to as an intermediate representation (IR). Converting disassembly to an IL allows decompiler developers to make optimizations on the IL level which apply to multiple architectures. </p> <p>There exist many ILs for analysis of programs, but some of the most notable ones for decompilation are tied to binary analysis. Most binary analysis platforms that have created or used an IL often follow the same techniques but mostly differ in their later use of ILs<sup>1</sup><sup>2</sup><sup>3</sup><sup>4</sup>.  One such use is recompilable decompilation, which can be made easier by lifting to compiled ILs like LLVM-IR<sup>5</sup><sup>6</sup>. Another use-case has been the verification of decompilation correctness<sup>7</sup>.</p> <p>Similar to static analysis, most ILs used in decompilation support some form of static single assignment (SSA) since it simplifies some analyses<sup>8</sup>.</p>"},{"location":"fundamentals/cfg-recovery/lifting/#example-lifted-program","title":"Example Lifted Program","text":"<p>Below is some example x86 assembly of a simple C program: <pre><code>0000000000001129 &lt;main&gt;:\n    1129:   f3 0f 1e fa             endbr64\n    112d:   55                      push   rbp\n    112e:   48 89 e5                mov    rbp,rsp\n    1131:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi\n    1134:   83 7d ec 02             cmp    DWORD PTR [rbp-0x14],0x2\n    1138:   7e 09                   jle    1143 &lt;main+0x1a&gt;\n    113a:   c7 45 fc 00 00 00 00    mov    DWORD PTR [rbp-0x4],0x0\n    1141:   eb 07                   jmp    114a &lt;main+0x21&gt;\n    1143:   c7 45 fc 01 00 00 00    mov    DWORD PTR [rbp-0x4],0x1\n    114a:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]\n    114d:   5d                      pop    rbp\n    114e:   c3                      ret \n</code></pre></p> <p>It can be lifted to an IL like VEX, the IL used in the angr decompiler: <pre><code>00 | ------ IMark(0x401129, 4, 0) ------\n01 | PUT(rip) = 0x000000000040112d\n02 | ------ IMark(0x40112d, 1, 0) ------\n03 | t0 = GET:I64(rbp)\n04 | t8 = GET:I64(rsp)\n05 | t7 = Sub64(t8,0x0000000000000008)\n06 | PUT(rsp) = t7\n07 | STle(t7) = t0\n08 | ------ IMark(0x40112e, 3, 0) ------\n09 | PUT(rbp) = t7\n10 | PUT(rip) = 0x0000000000401131\n11 | ------ IMark(0x401131, 3, 0) ------\n12 | t10 = Add64(t7,0xffffffffffffffec)\n13 | t13 = GET:I64(rdi)\n14 | t25 = 64to32(t13)\n15 | t12 = t25\n16 | STle(t10) = t12\n17 | PUT(rip) = 0x0000000000401134\n18 | ------ IMark(0x401134, 4, 0) ------\n19 | t14 = Add64(t7,0xffffffffffffffec)\n20 | t5 = LDle:I32(t14)\n21 | PUT(cc_op) = 0x0000000000000007\n22 | t26 = 32Uto64(t5)\n23 | t16 = t26\n24 | PUT(cc_dep1) = t16\n25 | PUT(cc_dep2) = 0x0000000000000002\n26 | PUT(rip) = 0x0000000000401138\n27 | ------ IMark(0x401138, 2, 0) ------\n28 | t29 = 64to32(t16)\n29 | t30 = 64to32(0x0000000000000002)\n30 | t28 = CmpLE32S(t29,t30)\n31 | t27 = 1Uto64(t28)\n32 | t23 = t27\n33 | t31 = 64to1(t23)\n34 | t18 = t31\n35 | if (t18) { PUT(rip) = 0x401143; Ijk_Boring }\nNEXT: PUT(rip) = 0x000000000040113a; Ijk_Boring\n...\n</code></pre></p> <p>In the case of VEX, each <code>t</code> variable is only ever assigned once, making this SSA form.  You will also notice how verbose every instruction has become.  Even simple <code>mov</code> instructions, which assign a value to a register, have much more information now.  The lifted VEX above has been cut for brevity. </p>"},{"location":"fundamentals/cfg-recovery/lifting/#intermediate-languages","title":"Intermediate Languages","text":"Name Project Notes BIL BAP Includes another IL called Core Theory BNIL Binary Ninja Includes 7 different ILs internally such as Low Level IL, Medium Level, SSA forms, and others Boogie Boogie GitHub Repository Cas Amoco Chunk IR Egalito DBA BINSEC BINSEC, GitHub Repository ESIL Radare Documentation Falcon IL Falcon FalkerIL Falker* Not well-documented; created because existing ILs didn't meet specific purposes GDSL GDSL GTIRB GTIRB GitHub Repository JEB IR JEB LowUIR B2R2 Miasm IR Miasm Blog Post Microcode Hex-Rays Hex Blog; not the same as Insight Microcode! Microcode Insight Website; not the same as Hex-Rays Microcode! P-Code GHIDRA Sleigh Documentation RDIL RedASM REIL BinNavi RREIL Bindead Snowman IR Snowman GitHub Repository SSL Jakstab TSL CodeSonar Unnamed EiNSTeiN- VEX Valgrind Angr Docs, FOSDEM Slides Vine BitBlaze Based on VEX VTIL VTIL Designed for binary translation; initially used for defeating VMProtect"},{"location":"fundamentals/cfg-recovery/lifting/#llvm-ir-based-lifting","title":"LLVM IR-based Lifting","text":"<p>One of the most popular and widely known IRs for program analysis is LLVM IR <sup>9</sup>, which is used in LLVM-based projects like the Clang compiler. With this in mind, many decompiler/binary analysis platforms have attempted to adopt LLVM IR as their main IR for analysis in the past.  However, it is still unknown how effective these approaches are since they have not achieved wide-scale adoption in either industry or academia. </p> Name Project Notes allin allin bin2llvm S2E Dagger Dagger fcd fcd Fracture Fracture libbeauty libbeauty Has been removed from GitHub, reference URL linked to instead mctoll mctoll Rellume binopt remill McSema reopt reopt RetDec RetDec revng revng First lifted to QEMU TCG, then LLVM IR <ol> <li> <p>Song, Dawn, et al. \"BitBlaze: A new approach to computer security via binary analysis.\" Information Systems Security: 4<sup>th</sup> International Conference, ICISS 2008, Hyderabad, India, December 16-20, 2008. Proceedings 4. Springer Berlin Heidelberg, 2008.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kinder, Johannes, and Helmut Veith. \"Jakstab: a static analysis platform for binaries: tool paper.\" Computer Aided Verification: 20<sup>th</sup> International Conference, CAV 2008 Princeton, NJ, USA, July 7-14, 2008 Proceedings 20. Springer Berlin Heidelberg, 2008.\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"BAP: A binary analysis platform.\" Computer Aided Verification: 23<sup>rd</sup> International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23. Springer Berlin Heidelberg, 2011.\u00a0\u21a9</p> </li> <li> <p>Wang, Fish, and Yan Shoshitaishvili. \"Angr-the next generation of binary analysis.\" 2017 IEEE Cybersecurity Development (SecDev). IEEE, 2017.\u00a0\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9</p> </li> <li> <p>Revng. \u201cRevng/Revng: Revng: The Core Repository of the Rev.Ng Project.\u201d GitHub, github.com/revng/revng. Accessed 27 Apr. 2024.\u00a0\u21a9</p> </li> <li> <p>Engel, Daniel, Freek Verbeek, and Binoy Ravindran. \"BIRD: A Binary Intermediate Representation for Formally Verified Decompilation of X86-64 Binaries.\" International Conference on Tests and Proofs. Cham: Springer Nature Switzerland, 2023.\u00a0\u21a9</p> </li> <li> <p>Van Emmerik, Michael James. Static single assignment for decompilation. University of Queensland, 2007.\u00a0\u21a9</p> </li> <li> <p>https://llvm.org/docs/LangRef.html \u21a9</p> </li> </ol>"},{"location":"fundamentals/cfg-recovery/overview/","title":"Overview","text":""},{"location":"fundamentals/cfg-recovery/overview/#introduction","title":"Introduction","text":"<p>A control flow graph (CFG) is a graph that describes the \"flow\" of execution in a program<sup>1</sup>.  As such, CFG recovery, in binary analysis, is the recovery of such a graph from binary code.  Since decompilation directly relies on this CFG, the recovery of it is considered fundamental to decompilation. </p> <p>This recovery can be broken up into multiple phases, some being optional depending on the decompilation target:</p> <ol> <li>Disassembling: the conversion of binary code to its mnemonic instructions and operands </li> <li>Program Lifting: the conversion disassembly to an intermediate language (IL) for better abstraction </li> <li>Function Recognition: the discovery of boundaries defining a function</li> <li>Indirect Jump Resolving: the resolution of jumps that have no constant target(s). </li> </ol> <p>The second phase, program lifting, is only required if the decompiler aims to be architecture agnostic. Most decompilers will use an IL to make their later analyses more widely applicable. </p> <p>Methods for evaluating improvements in this field are also of note but have had very limited work. The most recent of these works has focused on instrumenting and comparing to the compile-generated CFG<sup>2</sup><sup>3</sup>. </p> <p>There has been little work in replacing CFG recovery algorithms with a machine-learning model<sup>4</sup>. Related work in binary analysis has looked at how these recovered CFGs may be instrumentable<sup>5</sup>.</p>"},{"location":"fundamentals/cfg-recovery/overview/#graph-recovery-example","title":"Graph Recovery Example","text":"<p>An example C program is shown below: <pre><code>int main(int argc) {\n    int ret_code;\n    if(argc &gt; 2) {\n        ret_code = 0;\n    }\n    else {\n        ret_code = 1;\n    }\n    return ret_code;\n}\n</code></pre></p> <p>It is compiled on a x86-64 Linux machine with GCC, without optimizations: <pre><code>gcc example.c -o example &amp;&amp; objdump -D -M intel example | grep \"&lt;main&gt;:\" -A 12\n</code></pre></p> <p>After disassembling with objdump, which includes some function identification, we get the following disassembly: <pre><code>0000000000001129 &lt;main&gt;:\n    1129:   f3 0f 1e fa             endbr64\n    112d:   55                      push   rbp\n    112e:   48 89 e5                mov    rbp,rsp\n    1131:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi\n    1134:   83 7d ec 02             cmp    DWORD PTR [rbp-0x14],0x2\n    1138:   7e 09                   jle    1143 &lt;main+0x1a&gt;\n    113a:   c7 45 fc 00 00 00 00    mov    DWORD PTR [rbp-0x4],0x0\n    1141:   eb 07                   jmp    114a &lt;main+0x21&gt;\n    1143:   c7 45 fc 01 00 00 00    mov    DWORD PTR [rbp-0x4],0x1\n    114a:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]\n    114d:   5d                      pop    rbp\n    114e:   c3                      ret\n</code></pre></p> <p>In this example, the instructions at <code>0x1138</code> and <code>0x114a</code> cause the control flow to branch, resulting in the recovered graph:</p> <p></p> <p>Notice the implied edges coming from the assembly that looked linear (<code>0x114a</code>). The structure of the graph will also look the same if lifted to an IL.</p> <ol> <li> <p>Allen, Frances E. \"Control flow analysis.\" ACM Sigplan Notices 5.7 (1970): 1-19.\u00a0\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Ground truth for binary disassembly is not easy.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> <li> <p>Pang, Chengbin, et al. \"Sok: All you ever wanted to know about x86/x64 binary disassembly but were afraid to ask.\" 2021 IEEE symposium on security and privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Yu, Shih-Yuan, et al. \"Cfg2vec: Hierarchical graph neural network for cross-architectural software reverse engineering.\" 2023 IEEE/ACM 45<sup>th</sup> International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 2023.\u00a0\u21a9</p> </li> <li> <p>Di Bartolomeo, Luca, Hossein Moghaddas, and Mathias Payer. \"{ARMore}: Pushing Love Back Into Binaries.\" 32<sup>nd</sup> USENIX Security Symposium (USENIX Security 23). 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/gotoless/","title":"Gotoless Structuring","text":""},{"location":"fundamentals/structuring/gotoless/#introduction","title":"Introduction","text":"<p>Gotoless structuring is a type of structuring that ignores some compiler patterns to make code that contains no gotos<sup>1</sup>. According to this work, these gotos are signs of unstructured code which is bad for readability<sup>1</sup><sup>2</sup>. </p> <p>The most famous of these works, and the founder of the area, is the DREAM decompiler<sup>1</sup>. The DREAM decompiler used reaching conditions on statements to condense and reduce decompilation. </p> <p>The followup to this work, which is a hybrid of gotoless and schema-based methods, is the rev.ng<sup>2</sup> decompiler.  They used a method called \"Combing\", which duplicated nodes in the original graph to get rid of unstructured regions. </p> <ol> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/loop-reduction/","title":"Loop Reduction","text":""},{"location":"fundamentals/structuring/loop-reduction/#introduction","title":"Introduction","text":"<p>Loop reduction is a technique performed on CFGs in order to identify the basic blocks that are contained within a loop. Decompilers inherited this from compilers, where the technique is used to enable optimisations such as loop unrolling. There are several challenges involved in the identification of loops that make this a very interesting topic.</p> <p>Surprisingly, there are quite a few different algorithms used in decompilers for the identification of loops:</p> <ol> <li>Interval Analysis<sup>6</sup>.</li> <li>Dominator-Based<sup>3</sup>.</li> <li>Havlak-Tarjan<sup>4</sup>.</li> </ol>"},{"location":"fundamentals/structuring/loop-reduction/#interval-analysis","title":"Interval Analysis","text":"<p>This method was introduced to decompilation by Cristina Cifuentes<sup>1</sup>. It served an important step in the ability to identify the order of nested loops. It works in the following way:</p> <ol> <li> <p>Identify the set of intervals (maximal single entry subgraphs) in a graph.     <ol> <li>A node is added to an interval if all its immediate predecessors are in the interval.</li> <li>A new interval is created if the previous condition does not hold for a node.</li> </ol></p> </li> <li> <p>Collapse the nodes into a new graph.</p> </li> <li> <p>Repeat until you have one node. Once this condition is met you will have a derived sequence of graphs<sup>7</sup>.</p> <p></p> </li> </ol> <p>Now the loops have been collected in their relative nesting. To compute the loops in a graph you first constrain yourself to one interval at a time and ignore external edges. In the case of the interval containing 5, 6 and 7 we ignore the edge 7 -&gt; 4 for it to be dealt with in the interval containing 4 and 5-7.</p> <p>Each interval head is potentially the head of a loop, and can be queried by determining if there is a back-edge to the interval head. Once a loop head is found a path of nodes within the loop from head to the node where the back-edge is found must be followed. I believe the details for this are conveyed well in Cristina's thesis<sup>1</sup>, therefore they have been omitted from this article.</p> <p>Overall this technique is good, however it is unable to deal with irreducible graphs properly<sup>2</sup> an important quality of better algorithms. An irreducible graph is one where it forms a shape of the following figure, our goal with the following algorithms is to deal with them in such a way that they do not inhibit our ability to detect reducible loops.</p> <p></p>"},{"location":"fundamentals/structuring/loop-reduction/#dominator-based-loop-reduction","title":"Dominator-Based Loop Reduction","text":"<p>A node n is dominated by another node d if all paths to n contain d. We therefore call d a dominator. To utilise this definition to find loops we will cover the Sreedhar-Gao-Lee (SGL) algorithm<sup>3</sup>.</p> <p>The dominators found in a control flow graph can be simply represented in what is called a dominator tree. This is constructed by setting a parent's child nodes to those which it immediately dominates. In the following figure the CFG is represented on the left with its corresponding dominator tree on the right.</p> <p></p> <p>In the SGL algorithm they combine both the CFG and the dominator tree into one graph named a DJ graph. The DJ graph is named due to each node having a set of edges for the nodes that it immediately dominates 'D' and for the edges in the flow graph 'J'. We can represent our previous diagram in such a manner using red lines for 'D' edges and black lines for 'J' edges.</p> <p></p> <p>Such a representation is rare in practice as it adds complexity, therefore my recommendation is to not be funky and keep the CFG and dominator tree separate.</p> <p>The following steps outline the process of this algorithm:</p> <ol> <li> <p>Perform a depth-first search and identify back-edges.     <ol> <li>A back edge occurs if a node w receives an edge from a node v, where v is considered an ancestor of w.</li> <li>An ancestor can be identified if 2 properties hold: the preorder number of w is less than or equal to v, and the preorder number of the last descendent of w is greater than or equal to that of v i.e. there exists a path from w to v.</li> </ol></p> </li> <li> <p>Traverse the graph from the deepest level up to the top. Visit all nodes that have that corresponding level in the CFG.     <ol> <li>Check the predecessors of that node, and if one or more back-edges are dominated by the node then we can classify this node as the head of a reducible loop.</li> <li>If however we find a back-edge which the node does not dominate we classify this node as the head of an irreducible loop.</li> </ol></p> </li> <li> <p>In the case of a irreducible loop we mark it and deal with it in step 5.</p> </li> <li> <p>In the case of a reducible loop we start at the node where the back-edge is coming from, adding nodes to the loop body until we reach a node which has already been added to the loop body.     <ol> <li>If a node has already been assigned to a loop/s, it's least nested loop header is assigned to the loop body instead. The Union-Find data structure can be used for this purpose (covered in the next section).</li> <li>Once all nodes have been added to the loop, they are collapsed.</li> </ol></p> </li> <li> <p>Before we move to the next level, if we have identified any irreducible loops we take the subgraph containing all the nodes at our current level and greater*. We calculate any strongly connected components and reduce them into one node<sup>8</sup>.     <ol> <li>A strongly connected component is a subgraph where every node is reachable from every other node.</li> </ol></p> </li> </ol> <p>* The subgraph I'm referring to is the one where we have already collapsed the reducible loops to one node.</p>"},{"location":"fundamentals/structuring/loop-reduction/#havlak-tarjan","title":"Havlak-Tarjan","text":"<p>In 1997 Havlak presented another way<sup>4</sup>, this was a variation on Tarjan's interval analysis that enabled the collapsing of irreducible loops. Similar to the previous algorithm we will use a Union-Find data structure. </p> <p>A Union-Find can be employed on disjoint sets i.e. sets with no elements in common. Union combines two sets and Find gets the set of an element. This can be done by using a tree structure and instantiating each node as its own set.</p> <p>For our purposes we would use union to add another loop/node to an outer loop. Find would be used to get the outer most loop header. In the following visualisation we present the union operation on the 2 loops (4, (5, 6), 7) from the first figure.</p> <p></p> <p>On the left if we call Find(6) it returns 5, on the right calling Find(6) returns 4. This satisfies our need to obtain the outermost loop header of a node. A further optimisation can be employed on this, by changing 6's parent to be 4 just before we return from Find.</p> <p>The following steps outline the process of this algorithm:</p> <ol> <li> <p>Perform a DFS and assign the preorder number of each nodes, and the preorder number of its last descendant. </p> </li> <li> <p>Separate each nodes incoming edges into two classifications: back edge and other edge.      <ol> <li>A back edge occurs if a node w receives an edge from a node v, where v is considered an ancestor of w.</li> <li>An ancestor can be identified if 2 properties hold: the preorder number of w is less than or equal to v, and the preorder number of the last descendent of w is greater than or equal to that of v i.e. there exists a path from w to v.</li> </ol></p> </li> <li> <p>At the same time initialise each node to be a Union-Find set containing only itself.</p> </li> <li> <p>We then go through the nodes in reverse preorder preforming the following steps:     <ol> <li>Iterate through the node's back-edges, if it links to itself we set it as a self-loop. Otherwise, we add the source of the edge to a list.</li> <li>Create a worklist from our list and traverse back up to the loop header adding nodes as we go. If at any point we find a node that is not an ancestor of our loop header, then the loop is considered irreducible.</li> </ol></p> </li> <li> <p>Finally we union all nodes in the loop, setting their header as the loop header.</p> </li> <li> <p>To ensure we detect all the loops we can also insert new nodes in the case of a loop header being shared by a reducible and non-reducible back-edge.</p> </li> </ol>"},{"location":"fundamentals/structuring/loop-reduction/#conclusion","title":"Conclusion","text":"<p>Both the SGL and Havlak-Tarjan algorithms are excellent choices for a decompiler. It must be noted that the algorithms will produce different results, due to differing definitions on how to nest loops. Further modifications has also been suggested by Ramalingam<sup>5</sup> to ensure a near linear running time for both algorithms. </p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9\u21a9</p> </li> <li> <p>Callahan, Tim. Dataflow &amp; Interval Analysis. 15-745: Optimizing Compilers for Modern Architectures. Carnegie Mellon University, March 2005.\u00a0\u21a9</p> </li> <li> <p>Sreedhar et al. Identifying loops using DJ graphs. ACM Transactions on Programming Languages and Systems (TOPLAS), Volume 18, Issue 6. November 1996.\u00a0\u21a9\u21a9</p> </li> <li> <p>Havlak, Paul. Nesting of Reducible and Irreducible Loops. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, July 1997.\u00a0\u21a9\u21a9</p> </li> <li> <p>Ramalingam, Ganesan. Identifying Loops In Almost Linear Time. ACM Transactions on Programming Languages and Systems. March 1999.\u00a0\u21a9</p> </li> <li> <p>Tarjan, Robert. Testing flow graph reducibility. Proceedings of the Fifth Annual ACM Symposium on Theory of Computing. 1973.\u00a0\u21a9</p> </li> <li> <p>Allen, Frances E. Control flow analysis. Proceedings of a symposium on Compiler optimization. 1970.\u00a0\u21a9</p> </li> <li> <p>Tarjan, Robert. Depth-First Search and Linear Graph Algorithms. SIAM Journal on Computing. June 1972.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/overview/","title":"Structuring Overview","text":""},{"location":"fundamentals/structuring/overview/#introduction","title":"Introduction","text":"<p>Academically introduced in Dr. Cifuentes' 1994 Dissertation<sup>1</sup>, decompilation control flow structuring is the process used to turn a control flow graph (CFG) into a structured high-level language.  Control flow structuring in decompilation is highly related to the general control flow structuring process found in compiler research. Although the goal of control flow structuring, referred to as structuring, is to output linear high-level code, the level of abstraction of that code is open research. Additionally, few works exist in structuring for targeting language output other than C. </p>"},{"location":"fundamentals/structuring/overview/#general-structuring-example","title":"General Structuring Example","text":"<p>Although often thought of in the context of assembly, control flow structuring requires a control flow graph and conditions <sup>2</sup>.  For example, the attributed control flow graph below can be used as input:</p> <pre><code>                            +-----+\n                            |  A  |\n                            +-----+\n                              |   |\n                           ~x |   +--+\n                              V      |\n                         +-----+     |\n                         |  B  |     | x\n                         +-----+     |\n                           |  +--+   |\n                        ~y |     | y |\n                           V     V   V\n                       +-----+  +-----+\n                       |  D  |  |  C  |\n                       +-----+  +-----+\n                            |    |\n                            V    V\n                            +-----+\n                            |  E  |\n                            +-----+\n</code></pre> <p>Using a schema-based structuring algorithm, the graph can be turned into the following C:</p> <pre><code>A();\nif(x)\n    goto label_c;\nB();\nif (y) {\nlabel_c:\n    C();\n}\nelse {\n    D();\n}\nE();\n</code></pre> <p>There are multiple ways of turning the graph into linear C code <sup>3</sup>. For instance, the first condition on <code>x</code> can be flipped, changing where the <code>goto</code> appears and how many <code>if</code> scopes exist in the program. </p>"},{"location":"fundamentals/structuring/overview/#types-of-structuring","title":"Types of Structuring","text":"<p>There are two dominant types of structuring algorithms<sup>4</sup>:</p> <ol> <li>Schema-based: Algorithms that construct code based on pre-known graph patterns that are omitted by compilers. These algorithms attempt to only make structured code when they are aware of a direct mapping to its source structure. </li> <li>Gotoless: Algorithms that prioritize removing all unstructured regions from code. These algorithms may use schema-based methods initially but are unique in their pattern-matching of structures that may not exist in its source. </li> </ol> <p>The biggest difference between these two is their reliance on known compiler patterns<sup>5</sup>.  In schema-based algorithms, the decompiler author creates a set of known compiler output patterns to recover a target language.  In gotoless algorithms, the decompiler author uses patterns outside of graph schemas, which may be compiler and language-independent. One such example is condition-based structuring, as used in the DREAM <sup>5</sup> decompiler. </p>"},{"location":"fundamentals/structuring/overview/#related-fields","title":"Related Fields","text":"<p>Structuring in decompilation was directly inspired by compiler works in structuring and general data-flow analysis<sup>2</sup>.  One of these earliest works was the 1970s paper \"Control flow analysis.\"<sup>6</sup>, which laid out the fundamental ideas for constructing control flow graphs. Additionally, many of the ideas for eliminating gotos, which were often a byproduct of schema-based structuring, were inspired by work in restructuring source code<sup>7</sup> <sup>8</sup>.</p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe. \u201c30 Years of Decompilation and the Unsolved Structuring Problem: Part 1.\u201d Mahaloz.Re, 2 Jan. 2024, https://mahaloz.re/dec-history-pt1. Accessed 11 Apr. 2024.\u00a0\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9</p> </li> <li> <p>Allen, Frances E. \"Control flow analysis.\" ACM Sigplan Notices 5.7 (1970): 1-19.\u00a0\u21a9</p> </li> <li> <p>Williams, M. Howard, and G. Chen. \"Restructuring pascal programs containing goto statements.\" The Computer Journal 28.2 (1985): 134-137.\u00a0\u21a9</p> </li> <li> <p>Erosa, Ana M., and Laurie J. Hendren. \"Taming control flow: A structured approach to eliminating goto statements.\" Proceedings of 1994 IEEE International Conference on Computer Languages (ICCL'94). IEEE, 1994.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/schema-based/schema-based/","title":"Schema-based Structuring","text":""},{"location":"fundamentals/structuring/schema-based/schema-based/#introduction","title":"Introduction","text":"<p>Schema-based structuring is a type of structuring that depends on a set of known compiler graph patterns<sup>1</sup>. With this in mind, a decompiler must know all of the compiler graph patterns to generate code that is structured<sup>2</sup> (contains no gotos). An example of this type of structuring can be found in the overview section.  Schema-based structuring techniques are the most popular techniques among decompilers<sup>1</sup><sup>3</sup><sup>4</sup><sup>5</sup><sup>6</sup>.</p>"},{"location":"fundamentals/structuring/schema-based/schema-based/#example-graph-patterns","title":"Example Graph Patterns","text":"<p>Example graph patterns, from Cifuentes 1994 dissertation<sup>1</sup>, can be seen below:</p> <p></p>"},{"location":"fundamentals/structuring/schema-based/schema-based/#approaches","title":"Approaches","text":"<p>After the foundational dissertation from Cifuentes, the Phoenix<sup>3</sup> work improved on structuring by adding more condition patterns.  These patterns allowed for more correct structures (like loop reductions). </p> <p>Follow-ups to this work all used gotoless<sup>2</sup><sup>4</sup> structuring methods until the SAILR<sup>5</sup> work in 2024. The SAILR work improved on the gotoless algorithms by introducing a new type of schema that \"reverts compiler optimizations.\"</p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Yakdan, Khaled, et al. \"No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations.\" NDSS. 2015.\u00a0\u21a9\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Gussoni, Andrea, et al. \"A comb for decompiled c code.\" Proceedings of the 15<sup>th</sup> ACM Asia Conference on Computer and Communications Security. 2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>\u010eurfina, Luk\u00e1\u0161, et al. \"Design of a retargetable decompiler for a static platform-independent malware analysis.\" Information Security and Assurance: International Conference, ISA 2011, Brno, Czech Republic, August 15-17, 2011. Proceedings. Springer Berlin Heidelberg, 2011.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/","title":"Switch Structuring","text":""},{"location":"fundamentals/structuring/schema-based/switch-structuring/#introduction","title":"Introduction","text":"<p>One important control flow structure in C is the Switch statement. In schema-based structuring algorithms the Switch statement is handled much like other structures, but requires some special attention because of the many ways a compiler can output a Switch<sup>1</sup>. Phoenix<sup>2</sup> documented much of the simpler cases, found in the examples below. </p> <p>In the simplest form, a Switch uses an indirect jump of a calculated distance from the base address of a jump table to the specific case statement.  Below you can find an example of a Switch statement in C, its corresponding assembly, and its Control Flow Graph (CFG).</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-in-c","title":"Switch in C","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n  int x = rand() % 5;\n  switch (x) {\n    case 0:\n      printf(\"hello\\n\");\n      break;\n    case 1:\n      printf(\"hello2\\n\");\n      break;\n    case 2:\n      printf(\"hello3\\n\");\n      break;\n    case 3:\n      printf(\"hello4\\n\");\n      break;\n    case 4:\n      printf(\"hello5\\n\");\n      break;\n    default:\n      return 0;\n  }\n  return x;\n}\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-in-assembly","title":"Switch in Assembly","text":"<pre><code>0x401169:       endbr64\n0x40116d:       push    rbp\n0x40116e:       mov     rbp, rsp\n0x401171:       sub     rsp, 0x10\n0x401175:       call    0x401070 ; rand()\n0x40117a:       movsxd  rdx, eax\n0x40117d:       imul    rdx, rdx, 0x66666667\n0x401184:       shr     rdx, 0x20\n0x401188:       sar     edx, 1\n0x40118a:       mov     ecx, eax\n0x40118c:       sar     ecx, 0x1f\n0x40118f:       sub     edx, ecx\n0x401191:       mov     dword ptr [rbp - 4], edx\n0x401194:       mov     ecx, dword ptr [rbp - 4]\n0x401197:       mov     edx, ecx\n0x401199:       shl     edx, 2\n0x40119c:       add     edx, ecx\n0x40119e:       sub     eax, edx\n0x4011a0:       mov     dword ptr [rbp - 4], eax\n0x4011a3:       cmp     dword ptr [rbp - 4], 4\n0x4011a7:       ja      0x401222\n0x4011a9:       mov     eax, dword ptr [rbp - 4]\n0x4011ac:       lea     rdx, [rax*4]\n0x4011b4:       lea     rax, [rip + 0xe6d]\n0x4011bb:       mov     eax, dword ptr [rdx + rax]\n0x4011be:       cdqe\n0x4011c0:       lea     rdx, [rip + 0xe61]\n0x4011c7:       add     rax, rdx\n0x4011ca:       notrack jmp     rax\n0x401222:       mov     eax, 0\n0x401227:       jmp     0x40122c\n0x401200:       lea     rax, [rip + 0xe11] ; case 0\n0x401207:       mov     rdi, rax\n0x40120a:       call    0x401060\n0x4011cd:       lea     rax, [rip + 0xe30] ; case 1\n0x4011d4:       mov     rdi, rax\n0x4011d7:       call    0x401060\n0x4011ef:       lea     rax, [rip + 0xe1b] ; case 2\n0x4011f6:       mov     rdi, rax\n0x4011f9:       call    0x401060\n0x401211:       lea     rax, [rip + 0xe07] ; case 3\n0x401218:       mov     rdi, rax\n0x40121b:       call    0x401060\n0x4011de:       lea     rax, [rip + 0xe25] ; case 4\n0x4011e5:       mov     rdi, rax\n0x4011e8:       call    0x401060\n0x40122c:       leave\n0x40122d:       ret\n0x40120f:       jmp     0x401229\n0x4011dc:       jmp     0x401229\n0x4011fe:       jmp     0x401229\n0x401220:       jmp     0x401229\n0x4011ed:       jmp     0x401229\n0x401229:       mov     eax, dword ptr [rbp - 4]\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-cfg","title":"Switch CFG","text":""},{"location":"fundamentals/structuring/schema-based/switch-structuring/#compiler-representations","title":"Compiler Representations","text":"<p>After compilation, a Switch can take on many forms, largely due to compiler optimizations<sup>1</sup>.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#jump-table","title":"Jump Table","text":"<p>The switch statement above loads the base offset of the jump table from memory, and then computes an indirect JMP to that address offset by the % calculation.  The offset is multiplied by 4 to account for the size of each address in memory (rdx = rax * 4).  This computation is what allows us constant time when performing a simple switch statement.  The indirect branch is particularly important, because it is rare to see a non-hardcoded unconditional JMP instruction, therefore anytime we see such a situation we can mark it as a potential switch.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#multiple-jump-tables","title":"Multiple Jump Tables","text":"<p>In the situation where we have large distance between sets of numbers in our switch, the compiler may create multiple jump tables as a simple way to retain constant time in our switch statement. </p> <p>Below, C code can be found that will emit multiple jump tables when compiled. </p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#multi-jump-table-switch-when-compiled","title":"Multi Jump Table Switch (when compiled)","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n  int x = rand() % 5;\n  switch (x) {\n  case 0:\n    printf(\"hello\\n\");\n    break;\n  case 1:\n    printf(\"hello2\\n\");\n    break;\n  case 2:\n    printf(\"hello3\\n\");\n    break;\n  case 1340:\n    printf(\"hello4\\n\");\n    break;\n  case 1341:\n    printf(\"hello5\\n\");\n    break;\n  case 1342:\n    printf(\"hello6\\n\");\n    break;\n  // and so on...\n  default:\n    return 0;\n  }\n  return x;\n}\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#balanced-tree","title":"Balanced Tree","text":"<p>Binary Search Trees (BSTs) and Ternary Search Trees (TSTs) are used by LLVM and GCC respectively<sup>3</sup> when it is not possible to implement a jump table(s)<sup>4</sup>.  In our first figure we showed a C program containing simple and consecutive case numbers (0, 1, 2, 3, 4) this means it is easy to calculate an offset by looking up the index associated with it. If the case constants are not close or a constant distance apart then the compiler prefers a ST over large amounts of unused memory. </p> <p>An example below shows C code that would result in an ST when compiled.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#search-tree-switch","title":"Search Tree Switch","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n  int x = rand() % 5;\n  switch (x) {\n    case 0:\n      printf(\"hello\\n\");\n      break;\n    case 10:\n      printf(\"hello2\\n\");\n      break;\n    case 200:\n      printf(\"hello3\\n\");\n      break;\n    case 301:\n      printf(\"hello4\\n\");\n      break;\n    case 457:\n      printf(\"hello5\\n\");\n      break;\n    default:\n      return 0;\n  }\n  return x;\n}\n</code></pre>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#cascascaded-conditionals","title":"Cascascaded Conditionals","text":"<p>In a few cases the compiler may deem the best representation of the switch statement to be a set of if, else-if, and else's, in this case the techniques outlined in the phoenix paper<sup>2</sup> would decompile them back to cascaded conditionals, which can result in a rather messy output from the decompiler. SAILR<sup>1</sup> introduces some techniques for identifying/reverting this, and producing a more accurate output.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#structuring","title":"Structuring","text":"<p>We must now switch up our discussion from the different ways in which a compiler may represent a switch statement, to the ways in which we can structure them. Much of this section has been written from the influence of implementations found in the Reko Decompiler.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#standard-switch-structuring","title":"Standard Switch Structuring","text":"<p>When structuring we visit each of the nodes in post-order and attempt to match against a set of schemas<sup>2</sup><sup>5</sup>. 2 of these schemas outlined in the Phoenix paper<sup>2</sup> represent the 2 types of switch statement structures we can get: Switch and IncSwitch (Incomplete-Switch). If no such statements are found in a particular traversal we may identify switch candidates (covered in the next section).</p> <p>For us to structure a switch the following must hold:</p> <ol> <li>The switch head is the only entry.</li> <li>There must be one node that is exited to, or all switch cases have no successors.</li> </ol> <p>If these conditions are not met we queue the switch and hope other structuring occurs before we procede with its reduction.</p> <p>The algorithm for structuring a switch is as follows:</p> <ol> <li>Identify the switch.<ol> <li>Indirect jump to a jump table. </li> </ol> </li> <li>Identify the cases.<ol> <li>Simply find all the successors of the switch head.</li> <li>The front end of the decompiler identifies jump tables relevent to the function and adds the correct successors of the graph. The entries that are added to this are done by determining the bounds that are possible to be accessed, this is obvious due to the JA instruction which defines an upper bound and the base address being loaded into the RAX register. Any gaps in the jump table should hold the same address as the JA instruction<sup>6</sup>.  </li> </ol> </li> <li>Produce a switch statement.</li> <li>Relink graph.</li> </ol>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#non-standard-switch-structuring","title":"Non-Standard Switch Structuring","text":"<p>If the switch statement is not represented by a jump table It will likely be structured as cascading if/else statements. In this case we can perform a rewrite to make it more legible after we have completed structuring. To achieve this 2 steps need to be performed:</p> <ol> <li> <p>Identify the first node in a cascade.</p> </li> <li> <p>Collect all subsequent nodes in a cascade.</p> </li> </ol> <p>Identifying the first node is done by checking for if statements that meet a set of pattern requirements in their condition. The condition for a valid statement can be a simple <code>==</code>/<code>!=</code>; a sequence of <code>!=</code>'s separated by <code>&amp;&amp;</code>'s or a sequence of <code>==</code>'s separated by <code>&amp;&amp;</code>'s. We then check consecutive if structures to see if they meet this pattern, and if we can collect a sufficient number of statements we can condense it down into a switch.</p> <p>This however obviously wouldn't work in the case of STs, which uses &gt; and &lt; comparators.  When STs occur few decompilers seem correctly identify this, of those that do there is 2 choices: simplify, but maintain if statements (Hex-Rays) or convert to a switch statement (angr). The addition to support this involves simply continuing to traverse for == or != operators to find the leaf nodes of the STs.</p>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#switch-candidates","title":"Switch Candidates","text":"<p>This is a term used in the Phoenix paper<sup>2</sup> to describe a schema that hasn't been matched, but could be potentially a switch. They occur when a switch has extra edges into or out of it. If there are extra incoming edges to nodes other than the switch head we can simply replace the edge with a jump and for extra outgoing edges we have 2 potential choices:</p> <ol> <li>If there is an edge from one of the case nodes to the node that immediately post dominates the switch head, we choose the immediate post dominator as our successor to the switch.     <ol> <li>Immediately post dominating means the closest node n where if we reversed all edges on the graph you would have to go through n to get to the switch head.</li> </ol></li> <li>Else we choose the successor to be the node with the highest number of edges from case nodes, that is not a case node itself.</li> </ol>"},{"location":"fundamentals/structuring/schema-based/switch-structuring/#code-examples","title":"Code Examples","text":"<p>Open-source decompilers document a few different implementations of Switch structuring.  Below, you can find links to code sections in open-source decompilers as well as their likely implemented algorithm.</p> <ul> <li>angr decompiler: Phoenix and SAILR</li> <li>Reko: Phoenix with modifications.</li> </ul> <ol> <li> <p>Basque, Zion Leonahenahe, et al. \"Ahoy sailr! there is no need to dream of c: A compiler-aware structuring algorithm for binary decompilation.\" 33st USENIX Security Symposium (USENIX Security 24). 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Brumley, David, et al. \"Native x86 decompilation using Semantics-Preserving structural analysis and iterative Control-Flow structuring.\" 22<sup>nd</sup> USENIX Security Symposium (USENIX Security 13). 2013.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Liska, Martin: Switch lowering improvements \u2013 slideslive. https://slideslive.com/38902416/switch-lowering-improvements. 2017.\u00a0\u21a9</p> </li> <li> <p>Eilam, Eldad. \"Reversing: Secrets of Reverse Engineering\". Wiley. 2005.\u00a0\u21a9</p> </li> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Yang, Zack: How C/C++ Compiler Generate Assembly Code For Switch Statement. https://www.zackyang.blog/article/assembly-code-generation-for-switch-statement. 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/","title":"Constraint Simplification","text":""},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#introduction","title":"Introduction","text":"<p>Lattice-based type recovery is a decompilation technique that infers data types in an intermediate representation (IR). Two major systems use this approach: TIE <sup>1</sup> and Retypd <sup>2</sup>. Retypd extends TIE by supporting both recursive and polymorphic types.</p> <p></p> <p>In a type lattice, an edge points from a supertype to its subtype\u2014e.g., <code>int32</code> is a subtype of <code>num32</code>. TIE determines upper and lower lattice bounds by applying constraints and then chooses an appropriate type. Retypd instead encodes constraints in a sketch <sup>2</sup>: a tree whose nodes are types and whose outgoing edges are labeled with the capabilities of those types.</p> <p>This article outlines Retypd\u2019s process for constraint generation and simplification (see Figure 1 in <sup>2</sup>). Retypd is open-source and underpins decompilers such as angr. The implementation diverges slightly from the original publication; those differences are summarized in the type-recovery outline</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#prerequisites","title":"Prerequisites","text":"<p>This section reviews the key type-theory terms used in lattice-based constraint simplification<sup>2</sup>.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#subtypes","title":"Subtypes","text":"<p>The notation <code>A &lt;: B</code> means \u201cA is a subtype of B.\u201d Many object-oriented languages adopt the same idea\u2014for example, if <code>Dog</code> implements <code>Animal</code>, then <code>Dog &lt;: Animal</code>.</p> <p>Alternate form: some texts write the relation as <code>A \u2286 B</code>.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#labels","title":"Labels","text":"<p>A label describes a capability of a type and is written <code>type.label</code>. For a pointer of type <code>X</code>, Retypd models three capabilities:</p> <ol> <li><code>X.load</code></li> <li><code>X.store</code></li> <li><code>X.\u03c3N@k</code></li> </ol> <p><code>X.load</code> reads through the pointer (<code>A = *X</code>). <code>X.store</code> writes through the pointer (<code>*X = A</code>). <code>X.\u03c3N@k</code> accesses N bits at offset k; for instance, <code>t-&gt;b</code> in the code below is represented as <code>t.\u03c316@64</code>.</p> <pre><code>struct Test {\n    int64_t a;\n    int16_t b;\n};\n\nstruct Test *t;\nt-&gt;b;\n</code></pre> <p>If the program assigns <code>t-&gt;b</code> to a variable, then <code>t</code> acquires the capability <code>t.load.\u03c316@64</code>.</p> <p>Functions introduce two additional labels:</p> Label Meaning <code>inL</code> Parameters at location L <code>outL</code> Return value at location L"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#variance","title":"Variance","text":"<p>Variance explains how a label\u2019s subtyping relationship follows (or reverses) the relationship of its base type:</p> Kind Labels Rule Covariant <code>out</code>, <code>load</code>, <code>\u03c3N@k</code> If <code>A &lt;: B</code>, then\u00a0<code>A.label &lt;: B.label</code>. Contravariant <code>in</code>, <code>store</code> If <code>A &lt;: B</code>, then\u00a0<code>B.label &lt;: A.label</code>."},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#deduction-rules","title":"Deduction Rules","text":"<p>Retypd represents inference rules in sequent form:</p>  \\frac{P_1 \\quad P_2 \\quad \\dots \\quad P_n}{C}  <p>Using <code>+</code> for covariance and <code>\u2212</code> for contravariance gives:</p> <p>Covariant</p>  \\frac{A &lt;: B \\quad \\text{Var}\\,B.\\text{label} \\quad \\langle \\text{label} \\rangle = +}      {A.\\text{label} &lt;: B.\\text{label}}  <p>Contravariant</p>  \\frac{A &lt;: B \\quad \\text{Var}\\,B.\\text{label} \\quad \\langle \\text{label} \\rangle = -}      {B.\\text{label} &lt;: A.\\text{label}}  <p>Retypd applies these rules implicitly during the later simplification pass.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#generating-constraints","title":"Generating Constraints","text":"<p>Constraint generation starts with the lifted IR plus an initial set of variables identified from function parameters and instruction patterns. Appendix A of <sup>2</sup> groups variable uses into six categories:</p> <ol> <li>Register loads and stores</li> <li>Addition and subtraction</li> <li>Memory loads and stores</li> <li>Procedure invocation</li> <li>Floating-point operations</li> <li>Bit manipulation</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#examples","title":"Examples","text":"<p>The table below shows how each IR pattern translates to a constraint.</p> Category / IR Input Constraint Produced <code>MOV R1, R2</code> <code>R2 &lt;: R1</code> <code>var = A + const</code> <code>A.+const &lt;: var</code> <code>var = A \u2212 const</code> <code>A.-const &lt;: var</code> <code>var = A + B</code> <code>Add(B, C, A)</code> <code>var = A \u2212 B</code> <code>Sub(B, C, A)</code> <code>A = *B</code> <code>B.load.\u03c3k@0 &lt;: A</code> <code>*A = B</code> <code>B &lt;: A.store.\u03c3k@0</code> <code>A = F(B)</code> <code>F.out &lt;: A</code>  and  <code>B &lt;: F.in</code> <p>Floating-point and bit-manipulation instructions may be handled explicitly or only when function signatures reveal the need <sup>2</sup>. Addition and subtraction propagate pointer-versus-integer information even when the constant operand is unknown, as explained in a later section.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#inferring-sketch-shape","title":"Inferring Sketch Shape","text":""},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#collecting-constraints","title":"Collecting Constraints","text":"<p>Retypd processes strongly connected components (SCCs) of the call graph from the leaves upward:</p> <ol> <li>Merge the constraints produced by every procedure in an SCC.</li> <li>Instantiate each call site: replace symbolic labels such as <code>F.inL</code> or <code>F.outL</code> with concrete lattice types, treating every call polymorphically.</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#building-the-quotient-graph","title":"Building the Quotient Graph","text":"<p>Algorithm E.1 <sup>2</sup> constructs a quotient graph <code>G</code> whose nodes are equivalence classes of derived type variables.</p> <pre><code>G \u2190 \u2205\nfor p.L1 \u2026 Ln in C.derivedTypeVars do\n    for i \u2190 1 \u2026 n do\n        s \u2190 FINDEQUIVREP(p.`1 \u2026 `i\u22121, G)\n        t \u2190 FINDEQUIVREP(p.`1 \u2026 `i,   G)\n        G.edges \u2190 G.edges \u222a (s, t, `i)  // label `i`\n    end for\nend for\n</code></pre> <p>For example, the variable <code>A.B.C</code> yields edges <code>A \u2192 A.B</code> (labeled <code>.B</code>) and <code>A.B \u2192 A.B.C</code> (labeled <code>.C</code>).</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#unification","title":"Unification","text":"<p>Each explicit subtype constraint <code>x &lt;: y</code> is enforced by unifying the corresponding nodes:</p> <pre><code>for x &lt;: y in C do\n    X \u2190 FINDEQUIVREP(x, G)\n    Y \u2190 FINDEQUIVREP(y, G)\n    UNIFY(X, Y, G)\nend for\n</code></pre> <p><code>UNIFY</code> merges equivalence classes and recurses on matching edges or the compatible pair <code>.load</code>/<code>.store</code>:</p> <pre><code>procedure UNIFY(X, Y, G)\n    if X \u2260 Y then\n        MAKEEQUIV(X, Y, G)\n        for (X', l) in G.outEdges(X) do\n            if (Y', l) in G.outEdges(Y) then\n                UNIFY(X', Y', G)\n            else if l = .load and (Y', .store) in G.outEdges(Y) then\n                UNIFY(X', Y', G)\n            end if\n        end for\n    end if\nend procedure\n</code></pre> <p></p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#propagating-add-sub","title":"Propagating <code>Add</code> / <code>Sub</code>","text":"<p>Retypd evaluates each <code>ADD</code> or <code>SUB</code> constraint repeatedly:</p> <pre><code>repeat\n    C_old \u2190 C\n    for c in C_old with c = ADD(_) or SUB(_) do\n        D \u2190 APPLYADDSUB(c, G, C)   // may generate new subtype edges\n        for \u03b4 in D with \u03b4 = X &lt;: Y do\n            UNIFY(X, Y, G)\n        end for\n    end for\nuntil C_old = C\n</code></pre> <p>Figure 13 of <sup>2</sup> shows how labels such as <code>.load</code> and <code>.store</code> disclose pointer information during this phase.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#initial-sketches","title":"Initial Sketches","text":"<p>Once propagation converges, each type variable receives an initial sketch:</p> <pre><code>for v in C.typeVars do\n    S \u2190 new Sketch\n    L(S) \u2190 ALLPATHSFROM(v, G)\n    for state w in S do\n        \u03bdS(w) \u2190 (\u27e8w\u27e9 = +) ? T : \u22a5   // T = lattice bottom; \u22a5 = lattice top\n    end for\n    B[v] \u2190 S\nend for\n</code></pre>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#creating-a-pushdown-system","title":"Creating a Pushdown System","text":"<p>The next step simplifies the constraint set with a pushdown system (PDS) derived from Figure 3 of <sup>2</sup>. <code>F.in0</code> serves as the start state and <code>F.out</code> as the final state.</p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#interesting-constraints","title":"Interesting Constraints","text":"<p>A constraint is interesting if it satisfies any of these conditions:</p> <ol> <li>It involves a capability label.</li> <li>It is recursively self-referential.</li> <li>It contains a type constant.</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#constraint-graph-construction","title":"Constraint Graph Construction","text":"<p>For every subtype edge (e.g., <code>A &lt;: B</code>, <code>A.load &lt;: D</code> and <code>C &lt;: B.store</code>) the PDS adds an unlabeled edge:</p> <p></p> <p>Next, two families of labeled edges are inserted:</p> Direction Edge label Description Subtype \u2192 prefix <code>pop</code> Remove one label from the end (forget). Supertype \u2192 prefix <code>push</code> Add one label to the end (recall). <p>Edges are added recursively to cover all prefixes, maintaining correct variance:</p> <p></p>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#saturation","title":"Saturation","text":"<p>Caucal\u2019s algorithm <sup>3</sup> adds shortcut edges so consecutive <code>push</code>/<code>pop</code> pairs cancel, achieving closure in O(n^3) time. Additional adjustments:</p> <ol> <li>Insert edges implied by <code>A.load &lt;: A.store</code>.</li> <li>Remove self-loops.</li> <li>Ensure a <code>pop</code> edge is unreachable after a single <code>push</code> with the same label.</li> </ol> <p></p> <p>In the example above, the following path: <pre><code>A.load.\u2296\n     \u2192 A.store.\u2296 (implied node)\npush \u2192 A.\u2295 \n     \u2192 B.\u2295\npop  \u2192 B.store.\u2296\n</code></pre></p> <p>This path contains a <code>push</code> directly followed by a <code>pop</code>. Where both use the same label: <code>store</code>. Therefore, we can apply the shortcut:</p> <pre><code>A.load.\u2296 \u2192 B.store.\u2296\n</code></pre>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#breaking-sccs","title":"Breaking SCCs","text":"<p>Recursive type information becomes visible by iteratively removing strongly connected components:</p> <ol> <li>Temporarily strip the current set of interesting nodes.</li> <li>Identify remaining SCCs.</li> <li>For each SCC, select the node with the shortest label prefix, add it to the interesting set, and remove it from the graph.</li> <li>Repeat until no SCCs persist.</li> </ol>"},{"location":"fundamentals/type-recovery/lattice-constraint-simp/#code-examples","title":"Code Examples","text":"<p>The Retypd implementation provides several solvers for constraints viewable here.</p> <p>Although the Retypd implementation and angr\u2019s Typehoon analysis follow the same principles, both deviate slightly from the original paper:</p> <ul> <li>Retypd source tree</li> <li>angr Typehoon</li> </ul> <ol> <li> <p>Lee, JongHyup; Avgerinos, Thanassis; and Brumley, David. \u201cTIE: Principled Reverse Engineering of Types in Binary Programs.\u201d USENIX Security 2011.\u00a0\u21a9</p> </li> <li> <p>Noonan, Matt; Loginov, Alexey; and Cok, David. \u201cPolymorphic Type Inference for Machine Code.\u201d PLDI 2016.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Caucal, Didier. \u201cOn the Regular Structure of Prefix Rewriting.\u201d Theoretical Computer Science 106 (1): 61\u201386, 1992.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/type-recovery/overview/","title":"Type Recovery Overview","text":""},{"location":"fundamentals/type-recovery/overview/#introduction","title":"Introduction","text":"<p>Type recovery is the process of identifying high-level variables and their types from a binary<sup>1</sup>, usually in the form of a CFG. This wiki groups variable identification with type recovery since they are often intertwined in decompilation<sup>2</sup>. </p> <p>In most decompilers, the typing system requires a well-formed CFG, usually lifted, on which to perform analysis.  Analysis phases can be thought to work in two refining stages:</p> <ol> <li>Variable Identification: discover initial locations and their boundaries</li> <li>Type Constraining: utilizing the variables' uses in the code, make constraints for size and choose a type.</li> </ol> <p>This process is often iterative between constraint building and variable identification.  Variable identification also includes retyping/resizing as more constraints are gathered. </p> <p></p>"},{"location":"fundamentals/type-recovery/overview/#typing-example","title":"Typing Example","text":"<p>A simple C program is shown below: <pre><code>int main(int argc, char** argv) {\n    char* str = argv[1];\n    puts(str);\n}\n</code></pre></p> <p>After compiling and disassembling: <pre><code>gcc example.c -o example &amp;&amp; objdump -D -M intel example | grep \"&lt;main&gt;:\" -A 12\n</code></pre></p> <p>We are left with the following: <pre><code>0000000000001149 &lt;main&gt;:\n    1149:   f3 0f 1e fa             endbr64\n    114d:   55                      push   rbp\n    114e:   48 89 e5                mov    rbp,rsp\n    1151:   48 83 ec 20             sub    rsp,0x20\n    1155:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi\n    1158:   48 89 75 e0             mov    QWORD PTR [rbp-0x20],rsi\n    115c:   48 8b 45 e0             mov    rax,QWORD PTR [rbp-0x20]\n    1160:   48 8b 40 08             mov    rax,QWORD PTR [rax+0x8]\n    1164:   48 89 45 f8             mov    QWORD PTR [rbp-0x8],rax\n    1168:   48 8b 45 f8             mov    rax,QWORD PTR [rbp-0x8]\n    116c:   48 89 c7                mov    rdi,rax\n    116f:   e8 dc fe ff ff          call   1050 &lt;puts@plt&gt;\n</code></pre></p> <p>A naive variable recovery algorithm might do the following: <pre><code>int main(int a1, long long a2) {\n    int v1; // rbp-0x14\n    long long v2; // rbp-0x20\n    long long v3; // rax\n    long long v4; // rbp-0x8\n    v1 = a1;\n    v2 = a2;\n    v3 = *(&amp;v2 + 1);\n    v4 = v3\n    puts((char *) v4);\n}\n</code></pre></p> <p>However, since <code>puts</code> is known to take a <code>char *</code> as the first argument this would allow <code>v4</code> to be constrained to be a <code>char *</code>. Back-propagating this type constraint to the earlier variables, we get the following: <pre><code>int main(int a1, char** a2) {\n    int v1; // rbp-0x14\n    char** v2; // rbp-0x20\n    char * v3; // rax\n    char * v4; // rbp-0x8\n    v1 = a1;\n    v2 = a2;\n    v3 = v2[1];\n    v4 = v3\n    puts(v4);\n}\n</code></pre></p>"},{"location":"fundamentals/type-recovery/overview/#variable-identification","title":"Variable Identification","text":"<p>Variable identification seeks to map memory locations and registers to high-level variables in the targeted language output (usually C). Early decompilers often mapped variables to locations based on their simple accesses <sup>2</sup>. Later work has followed up on this by expanding the set of uses supported for a variable location identification <sup>1</sup>.  Identified variables often have many candidates for size (because of their potential type). These candidates have often been reduced to a single type based on their type sinks<sup>3</sup>, uses that have explicit types like in a call argument. </p>"},{"location":"fundamentals/type-recovery/overview/#type-constraining","title":"Type Constraining","text":"<p>Type constraining is directly linked to variable identification since the use of the variable is affected by its size (arrays vs primitive types). Previous works in this area have looked at multiple ways to gather and reduce types for variables. Some of these include: def-use analysis<sup>1</sup><sup>4</sup><sup>5</sup>, library awareness<sup>3</sup><sup>4</sup><sup>5</sup>, emulation<sup>6</sup>, lattice-solving<sup>7</sup><sup>8</sup>, and machine learning<sup>9</sup><sup>10</sup>.</p> <p>Of these works, typing involving lattice-based methods<sup>7</sup><sup>8</sup>, also known as push-down typing, is the most popular among modern open-source decompilers. </p> <ol> <li> <p>Balakrishnan, Gogul, and Thomas Reps. \"Divine: Discovering variables in executables.\" International Workshop on Verification, Model Checking, and Abstract Interpretation. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Mycroft, Alan. \"Type-based decompilation (or program reconstruction via type reconstruction).\" European Symposium on Programming. Berlin, Heidelberg: Springer Berlin Heidelberg, 1999.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lin, Zhiqiang, Xiangyu Zhang, and Dongyan Xu. \"Automatic reverse engineering of data structures from binary execution.\" Proceedings of the 11<sup>th</sup> Annual Information Security Symposium. 2010.\u00a0\u21a9\u21a9</p> </li> <li> <p>Haller, Istvan, Asia Slowinska, and Herbert Bos. \"Mempick: High-level data structure detection in c/c++ binaries.\" 2013 20<sup>th</sup> Working Conference on Reverse Engineering (WCRE). IEEE, 2013.\u00a0\u21a9\u21a9</p> </li> <li> <p>Jin, Wesley, et al. \"Recovering C++ objects from binaries using inter-procedural data-flow analysis.\" Proceedings of ACM SIGPLAN on Program Protection and Reverse Engineering Workshop 2014. 2014.\u00a0\u21a9\u21a9</p> </li> <li> <p>Slowinska, Asia, Traian Stancescu, and Herbert Bos. \"Howard: A Dynamic Excavator for Reverse Engineering Data Structures.\" NDSS. 2011.\u00a0\u21a9</p> </li> <li> <p>Noonan, Matt, Alexey Loginov, and David Cok. \"Polymorphic type inference for machine code.\" Proceedings of the 37<sup>th</sup> ACM SIGPLAN Conference on Programming Language Design and Implementation. 2016.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011)\u00a0\u21a9\u21a9</p> </li> <li> <p>Zhang, Zhuo, et al. \"Osprey: Recovery of variable and data structure via probabilistic analysis for stripped binary.\" 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Chen, Qibin, et al. \"Augmenting decompiler output with learned variable names and types.\" 31<sup>st</sup> USENIX Security Symposium (USENIX Security 22). 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"fundamentals/type-recovery/var-detection/","title":"Variable Detection","text":""},{"location":"fundamentals/type-recovery/var-detection/#introduction","title":"Introduction","text":"<p>Variable detection encompasses a set of techniques used to annotate memory regions and registers with variable type classifications. Common classifications include global, local, heap, and register variables. Accurate variable classification is foundational to subsequent analyses for decompilation.</p>"},{"location":"fundamentals/type-recovery/var-detection/#naive-variable-identification","title":"Na\u00efve Variable Identification","text":"<p>Early decompilation techniques<sup>1</sup> primarily relied on recognizing common idioms in assembly code:</p> Type Access Pattern Local <code>[rbp-4]</code> Global <code>[0xdeadbeef]</code> or <code>[rip+0xdeadbeef]</code> <p>Local variables are typically identified by their access as an offset from the frame pointer. Global variables, in contrast, are accessed via absolute addressing\u2014either through a fixed address or an offset relative to the instruction pointer.</p> <p>This analysis can be extended by incorporating knowledge of calling conventions, which define:</p> <ul> <li>How arguments are passed (via stack or registers)  </li> <li>Which registers must be preserved (caller- vs. callee-saved)  </li> <li>How return values are provided  </li> </ul> <p>The regularity imposed by calling conventions allows the use of pattern matching to infer function arguments, return values, and therefore variables.</p> <p>Although this approach is relatively simple, it achieves surprisingly high accuracy\u2014detecting approximately 83% of local variables<sup>2</sup>. Its main limitations arise when variables are accessed indirectly, promoted to registers, or dynamically allocated on the heap. Nevertheless, many decompilers use na\u00efve identification as the foundation for variable and type analysis because of its efficiency, simplicity, and robustness.</p>"},{"location":"fundamentals/type-recovery/var-detection/#divine","title":"DIVINE","text":"<p>DIVINE<sup>2</sup> introduced a more advanced approach by combining Value-Set Analysis (VSA) with Aggregate Structure Identification (ASI). VSA models memory to determine potential addresses and values of memory locations, while ASI detects higher-level data structures (e.g., arrays, records, objects). Results from ASI can be fed back into VSA for further refinement through additional iterations.</p>"},{"location":"fundamentals/type-recovery/var-detection/#value-set-analysis-vsa","title":"Value-Set Analysis (VSA)","text":"<p>VSA enables the identification of heap-allocated variables and generally outperforms na\u00efve techniques. It partitions memory into three regions: global memory, the activation record (stack), and the heap.</p>"},{"location":"fundamentals/type-recovery/var-detection/#memory-representation","title":"Memory Representation","text":"<p>VSA computes an over-approximation of memory regions by maintaining a set of possible addresses or values for each location. Memory regions are typically represented as:</p> Region Type Occurrence Procedure One per procedure Global One per program Heap One per heap allocation <p>This separation allows addresses to be disambiguated across procedures and heap allocations, which is generally unnecessary for globals.  </p> <p>A memory address can be modeled as:</p> <pre><code>struct Addr {\n    struct MemoryRegion region;\n    size_t offset;\n};\n</code></pre> <p>Here, <code>offset</code> represents the displacement within the region (e.g., relative to the frame pointer for procedure regions).</p> <p>While this representation captures direct addresses, it struggles with indirect memory accesses. Offsets are therefore more accurately represented as sets of possible values, often encoded as strided intervals <code>s[l, u]</code>, where <code>s</code> is the stride and <code>[l, u]</code> the lower and upper bounds:</p> <p>Example: <code>4[8, 22] \u2192 {8, 12, 16, 20}</code></p> <p>This model is especially effective for representing array element offsets.</p> <p>To handle indirect addressing, we must track the possible values of registers and memory at each program point. These are represented as abstract locations (a-locs). Na\u00efve variable identification provides an initial set of a-locs: one per global, one per heap region, and one per register.</p> <pre><code>struct Stride {\n    size_t stride;\n    size_t lower;\n    size_t upper;\n};\n\nstruct AbstractLocation {\n    size_t offset;\n    size_t size;\n    struct Stride value_set;\n};\n\nstruct Store {\n    struct MemoryRegion region;\n    map[offset] =&gt; AbstractLocation;\n};\n</code></pre>"},{"location":"fundamentals/type-recovery/var-detection/#vsa-algorithm","title":"VSA Algorithm","text":"<p>The VSA algorithm determines the set of possible addresses and values for each abstract location at every program point. Key steps:</p> <ol> <li> <p>Build a Control Flow Graph (CFG):    Each instruction becomes a node, with edges representing control flow.</p> </li> <li> <p>Initialize the Abstract Store:    Start with known global variables and initial stack/register state.</p> </li> <li> <p>Apply Transfer Functions:    For instructions such as <code>mov</code>, <code>lea</code>, <code>cmp</code>, <code>push</code>, <code>pop</code>, and conditional jumps, update the abstract store accordingly.</p> </li> <li> <p>Handle Cycles with Widening:    Repeated analysis of loops is prevented through widening, which generalizes patterns.    Example: <code>{1, 3, 5, 7, \u2026}</code> becomes <code>2[1, \u221e)</code>.</p> </li> <li> <p>Model Interprocedural Behavior:    Add nodes for function calls and returns, propagating parameter and return value information.    Stack arguments are retrieved from caller memory regions; register arguments from register value sets.    Upon function exit, registers and stack values are restored.</p> </li> <li> <p>Address Indirect Jumps/Calls:    Where possible, resolve targets via value sets; otherwise, conservatively ignore them.</p> </li> </ol>"},{"location":"fundamentals/type-recovery/var-detection/#affine-relations","title":"Affine Relations","text":"<p>Loops introduce imprecision, as value sets could otherwise grow unboundedly. Affine relation analysis<sup>3</sup> improves precision by expressing relationships between variables:</p>  a_0 + \\sum_{i=1}^n a_i r_i = 0  <p>This allows inference of bounds for one register based on others.</p> <p>Example: Array Initialization Loop</p> <pre><code>xor ecx, ecx\nmov edx, arr\nL1:\n    lea eax, [ecx*8]\n    add eax, edx\n    mov [eax], ecx\n    inc ecx\n    cmp ecx, 10\n    jl L1\n</code></pre> <p>Here: <code>eax = ecx*8 + edx</code></p> <p>Since <code>ecx &lt; 10</code>, we deduce: <code>eax = 8[0, 9] + &amp;arr[0] = {0, 8, \u2026, 72} + &amp;arr[0]</code></p> <p>This increases precision when modeling memory accesses.</p>"},{"location":"fundamentals/type-recovery/var-detection/#call-strings","title":"Call Strings","text":"<p>Call strings<sup>4</sup> provide context-sensitive interprocedural analysis by recording call paths. Each unique call string corresponds to a separate abstract store, improving precision. However, this approach is computationally more expensive, as each function must be re-analyzed per distinct call path.</p> <p>Example of valid call strings: <code>{{1,2,4}, {1,2,3}, {1,3,4}}</code> This excludes invalid paths such as <code>1\u21922\u21923\u21924</code>.</p> <p>Call strings are typically bounded to prevent explosion in recursive or deep call chains.</p>"},{"location":"fundamentals/type-recovery/var-detection/#aggregate-structure-identification-asi","title":"Aggregate Structure Identification (ASI)","text":"<p>The output of VSA includes:</p> <ul> <li>Value sets for registers, globals, stack, and heap</li> <li>Detected indirect memory accesses</li> <li>Memory state changes across program points</li> </ul> <p>ASI uses this information to recover variables, arrays, and structures.</p> <p>For stack analysis, size can be inferred from stack-pointer adjustments (e.g., <code>sub esp, 80</code> \u2192 80-byte frame). Memory accesses are used to split the region into smaller atoms.</p> <p>Example:</p> <pre><code>mov ebp, esp\nsub esp, 80\nxor ecx, ecx\nmov edx, ebp\nL1:\n    lea eax, [ecx*8]\n    add eax, edx\n    mov [eax], ecx\n    inc ecx\n    cmp ecx, 10\n    jl L1\nmov esp, ebp\n</code></pre> <p>Here, the 80-byte region is divided into 10 elements of 8 bytes each, suggesting an array. Structure-like accesses appear as fixed offsets from a pointer:</p> <pre><code>mov [eax], 1\nmov [eax+4], 2\nmov [eax+16], 3\n</code></pre> <p>ASI results are fed back into VSA, refining abstract locations and improving subsequent iterations. Iterations continue until results converge, after which information is passed to the type analysis phase.</p>"},{"location":"fundamentals/type-recovery/var-detection/#improvements","title":"Improvements","text":"<p>The main drawback of VSA (and affine relation analysis) is runtime cost<sup>2</sup><sup>5</sup>, especially when multiple VSA\u2013ASI iterations are required. Handling self-modifying code also remains challenging.</p> <p>Improvements include:</p> <ul> <li>DVSA (TIE system)<sup>6</sup>: Uses SSA-form IR and linear combinations over strided intervals, improving precision and simplifying value-set computation.</li> <li>SecondWrite<sup>7</sup>: Suggests runtime optimizations for DIVINE to make it more scalable.</li> <li>Ghidra[^8]: Performs VSA intraprocedurally and ignores calls, floating point arithmetic and heap regions.</li> </ul>"},{"location":"fundamentals/type-recovery/var-detection/#code-examples","title":"Code Examples","text":"<p>Ghidra</p> <ol> <li> <p>Cifuentes, Cristina. Reverse compilation techniques. Queensland University of Technology, Brisbane, 1994.\u00a0\u21a9</p> </li> <li> <p>Balakrishnan, Gogul, and Thomas Reps. \"Divine: Discovering variables in executables.\" International Workshop on Verification, Model Checking, and Abstract Interpretation. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>M\u00a8uller-Olm, Markus and Helmut Seidl. \"Precise Interprocedural Analysis through Linear Algebra.\" POPL '04. 2004.\u00a0\u21a9</p> </li> <li> <p>M. Sharir and A. Pnueli. \"Two approaches to interprocedural data flow analysis\". In Program Flow Analysis: Theory and Applications, chapter 7, pages 189\u2013234. Prentice-Hall, 1981.\u00a0\u21a9</p> </li> <li> <p>Balakrishnan, Gogul, and Thomas Reps. Analyzing memory accesses in x86 executables. Comp Construct. 2004.\u00a0\u21a9</p> </li> <li> <p>Lee, JongHyup, Thanassis Avgerinos, and David Brumley. \"TIE: Principled reverse engineering of types in binary programs.\" (2011)\u00a0\u21a9</p> </li> <li> <p>ElWazeer, Khaled et al. \"Scalable Variable and Data Type Detection in a Binary Rewriter.\" PLDI '13: Proceedings of the 34<sup>th</sup> ACM SIGPLAN Conference on Programming Language Design and Implementation. 2013.\u00a0\u21a9</p> </li> </ol>"},{"location":"misc/blogs/","title":"Community Blogs","text":"<p>A collection of decompilation blogs from the community.</p>"},{"location":"misc/blogs/#legend","title":"Legend","text":"<ul> <li>\ud83d\udc80: inactive (2 years without activity)</li> <li>\ud83d\udcbe: created by the developer or maintainer of a standalone decompiler</li> <li>\ud83d\udd0d: has posts on fundamental topics</li> <li>\u2699\ufe0f: has posts on applied research topics</li> <li>\ud83c\udf0d: utilizes decompilation for some means </li> </ul>"},{"location":"misc/blogs/#alphabetical-order","title":"Alphabetical Order","text":"<ul> <li>angr blog (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> <li>Intranautic (\ud83d\udd0d)</li> <li>Kronotai (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> <li>mahaloz (\ud83d\udcbe, \ud83d\udd0d, \ud83c\udf0d)</li> <li>msm (\ud83d\udd0d, \ud83c\udf0d)</li> <li>fcd (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> <li>REC Blog (\ud83d\udc80, \ud83d\udcbe, \ud83d\udd0d)</li> </ul>"},{"location":"misc/talks/","title":"Talks","text":"<p>Talks that are about decompilation techniques or applications. All talks listed should be either at a conference or university, where talks are reviewed.  Exceptions are made for especially impactful talks.</p>"},{"location":"misc/talks/#reverse-chronological-order","title":"Reverse Chronological Order","text":"<ul> <li>\"30 Years, From Compilation Student to Decompilation Pioneer.\" Christina Cifuentes. LABScon24.</li> <li>\"BTD: Unleashing the Power of Decompilation for x86 Deep Neural Network Executables.\" Zhibo Liu. Blackhat 2023.</li> <li>\u201cModern Approaches in Human-Centric Decompilation.\u201d Zion Leonahenahe Basque. Ohio State University 2023.</li> <li>\u201cBridging the gap in the static and dynamic analysis of binaries through decompiler tomfoolery!\u201d Zion Leonahenahe Basque. CactusCon 2023.</li> <li>\"30 Years into Scientific Binary Decompilation.\" Dr. Ruoyu (Fish) Wang. NDSS BAR 2022.</li> <li>\"Beyond the C: Retargetable Decompilation using Neural Machine Translation.\" Iman Hosseini. NDSS BAR 2022.</li> <li>\"How to Build A Decompiler\" Tsviatko Yovtchev. Devoxx belgium. 2019</li> <li>\"Recovering Meaningful Variable Names in Decompiled Code.\" Dr. Bogdan Vasilescu. CMU 2019.</li> <li>\"How not to write a decompiler\" Giovanni Dante Grazioli. r2con 2018</li> <li>\"Repsych: Psychological Warfare in Reverse Engineering.\" Chris Domas. DEF CON 2015.</li> </ul>"}]}